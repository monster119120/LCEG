{"pred": "The ground truth for fake news is established by an expert manually annotating the tweets in the dataset.\n\nQuestion: What is the main goal of this study?\n\nAnswer: The main goal of this study is to characterize fake news in Twitter by looking into meta-data.\n\nQuestion: What is the sample size of the dataset used in this study?\n\nAnswer: The sample size of the dataset used in this study is 1,785,855 tweets published by 848,196 different users.\n\nQuestion: What is the number of viral twe", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": "The GhostVLAD approach is an extension of the NetVLAD approach, which was originally proposed for place recognition by R. Arandjelovic et al. [11]. The GhostVLAD model was proposed for face recognition by Y. Zhong [10].\n\nQuestion: What is the NetVLAD layer used for?\n\nAnswer: The NetVLAD layer is used to map N local features of dimension D into a fixed dimensional vector, as shown in Figure 1 (Left side).\n\nQuestion: What is the Ghost clusters used for?\n\nAnswer: The Ghost", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": "68.8% to 71.8%\n\nQuestion: What is the main limitation of the previous models?\n\nAnswer: Insufficient data for training complex neural network-based models\n\nQuestion: What is the main contribution of the proposed model?\n\nAnswer: It resolves the issue in which predictions frequently incorrectly yield the neutral class\n\nQuestion: What is the architecture of the ARE model?\n\nAnswer: The ARE model encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model to", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": "The additional features and context proposed include the use of context tweets, which are used as an additional feature for neural network models. The context tweets are concatenated with the outputs of their corresponding labeled tweets for the CNN baseline model. For RNN, the last hidden states of context and labeled tweets are concatenated.\n\nQuestion: What is the most accurate traditional machine learning model?\n\nAnswer: The most accurate traditional machine learning model is the LR model, followed by ensemble models such as GBT and RF.\n\nQuestion: What is the most accurate neural network model?\n\nAnswer", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "They looked at the pages of FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.\n\nQuestion: What did they do with the emotion vectors?\n\nAnswer: They assigned to each post the emotion with the highest count, ignoring like as it is the default and most generic reaction people tend to use.\n\nQuestion: What did they do with the emotion vectors?\n\nAnswer", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "No, the hashtag dataset contains both English and non-English data, while the SemEval dataset contains only English data.\n\nQuestion: What is the goal of hashtag segmentation?\n\nAnswer: The goal of hashtag segmentation is to divide a given hashtag into a sequence of meaningful words.\n\nQuestion: What is the difference between the pairwise ranking model and the multi-task learning model?\n\nAnswer: The pairwise ranking model directly ranks the candidate segmentations, while the multi-task learning model jointly learns segment ranking and single- vs. multi-", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": "The proposed evaluation is based on the manual creation of gold-standard summaries, which are then compared to the automatically generated summaries.\n\nQuestion: What is the goal of the proposed task?\n\nAnswer: The goal of the proposed task is to create a summary that is directly interpretable and useful for a user, rather than generating a textual summary.\n\nQuestion: What is the size of the document clusters in the proposed corpus?\n\nAnswer: The document clusters in the proposed corpus are 15 times larger than typical DUC clusters of ten documents.\n\nQuestion: What is the average length", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "The datasets used for evaluation are CNN/DailyMail, NYT, and XSum.\n\nQuestion: What is the proportion of novel n-grams in summaries produced by abstractive systems?\n\nAnswer: The proportion of novel n-grams in summaries produced by abstractive systems is shown in Figure FIGREF33.\n\nQuestion: What is the proportion of selected summary sentences that appear in the source document?\n\nAnswer: The proportion of selected summary sentences that appear in the source document is shown in Figure FIGREF31.\n\nQuestion: What is the proportion of novel n", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": "The proposed approach compares favorably to other WSD approaches employing word embeddings, as evidenced by its superior performance on benchmark word similarity and entailment datasets.\n\nQuestion: What is the objective function used in the proposed approach?\n\nAnswer: The objective function used in the proposed approach is a variant of the max-margin objective, which is based on the asymmetric KL divergence energy function. This energy function captures both word similarity and entailment by minimizing the KL divergence between word distributions.\n\nQuestion: What is the advantage of using KL divergence as the", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": "The ensemble method works by combining the predictions of multiple models to improve the overall accuracy.\n\nQuestion: What is the purpose of the BookTest dataset?\n\nAnswer: The BookTest dataset is designed to measure progress in understanding common-sense questions about short stories that can be easily answered by humans but cannot be answered by current state-of-the-art machine learning models.\n\nQuestion: What is the difference between the CBT and CNN/Daily Mail datasets?\n\nAnswer: The CBT dataset uses a cloze-style question, while the CNN/Daily Mail dataset uses a question that", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": "The datasets come from the scripts of the Friends TV sitcom and Facebook messenger chats.\n\nQuestion: What is the objective of the challenge?\n\nAnswer: The objective is to predict the emotion of utterance within the dialogue.\n\nQuestion: What is the size of the testing dataset?\n\nAnswer: The testing dataset consists of 240 dialogues including 3,296 and 3,536 utterances in Friends and EmotionPush, respectively.\n\nQuestion: What is the performance of the proposed method on the testing dataset?\n\nAnswer: The", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": "English\n\nQuestion: what is the main limitation of the aforementioned NMT models for text simplification?\n\nAnswer: the amount of available parallel data is insufficient\n\nQuestion: what is the main difference between text simplification and text summarization?\n\nAnswer: the focus of text simplification is to reduce the length and redundant content, while the focus of text summarization is to retain the semantic meaning\n\nQuestion: what is the main limitation of the hand-crafted, supervised, and unsupervised methods for text simplification?\n\nAnswer: they require a lot of human-", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": "IMDb dataset\n\nQuestion: What is the objective of this work?\n\nAnswer: To determine the optimal combinations of word2vec hyper-parameters for NLP tasks.\n\nQuestion: What is the scope of the work?\n\nAnswer: To determine the optimal combinations of word2vec hyper-parameters for intrinsic evaluation and extrinsic evaluation tasks.\n\nQuestion: What is the main contribution of this work?\n\nAnswer: The empirical establishment of optimal combinations of word2vec hyper-parameters for NLP tasks, discovering the behaviour of quality of vectors viz-a-viz increasing dimensions and the confirm", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "The proposed system achieves an accuracy of 99.99%.\n\nQuestion: What is the purpose of the common Bi-LSTM?\n\nAnswer: The common Bi-LSTM is used to extract worker independent features.\n\nQuestion: What is the purpose of the label Bi-LSTM?\n\nAnswer: The label Bi-LSTM is used to extract worker specific features.\n\nQuestion: What is the purpose of the worker discriminator?\n\nAnswer: The worker discriminator is used to classify the annotators.\n\nQuestion: What is the purpose", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": "yes\n\nQuestion: What was the main factor for recording the current corpus?\n\nAnswer: The participants read 739 sentences that were selected from the Wikipedia corpus.\n\nQuestion: What was the average reading speed for each task?\n\nAnswer: 12% of randomly selected sentences were followed by a comprehension question.\n\nQuestion: What was the average LexTALE score over all participants?\n\nAnswer: The average LexTALE score over all participants was 88.54%.\n\nQuestion: What was the purpose of the experiment?\n\nAnswer: The purpose of", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": "We use the DSTC2 dataset for the evaluation of the state of the art on the evaluation of the state of the art on the evaluation of the state of the art on the evaluation of the state of the art on the evaluation of the state of the art on the evaluation of the state of the art on the evaluation of the state of the art on the evaluation of the state of the art on the evaluation of the state of the art on the evaluation of the state of the art on the evaluation of the state of the art on the evaluation of the state of the art on the evaluation of the state of the art on the evaluation", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "The HealthCare sector.\n\nQuestion: Which stock market sector achieved the worst performance?\n\nAnswer: The Energy sector.\n\nQuestion: Which stock market sector achieved the best performance?\n\nAnswer: The HealthCare sector.\n\nQuestion: Which stock market sector achieved the worst performance?\n\nAnswer: The Energy sector.\n\nQuestion: Which stock market sector achieved the best performance?\n\nAnswer: The HealthCare sector.\n\nQuestion: Which stock market sector achieved the worst performance?\n\nAnswer: The Energy sector.\n\nQuestion: Which stock market sector achieved the best performance?", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "RNN-based NMT and Transformer-based NMT.\n\nQuestion: what is the name of the method that combines both lexical-based and statistical-based information to measure the score for each possible clause alignment?\n\nAnswer: Clause Alignment.\n\nQuestion: what is the name of the method that uses the edit distance to measure the score for each possible clause alignment?\n\nAnswer: Edit Distance.\n\nQuestion: what is the name of the method that uses the longest common subsequence based approach for ancient-modern Chinese sentence alignment?\n\nAnswer: LCS.\n", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": "The three regularization terms are neutral features, maximum entropy, and KL divergence.\n\nQuestion: What is the purpose of incorporating neutral features?\n\nAnswer: The purpose of incorporating neutral features is to prevent the model from biasing to the class with a dominate number of labeled features.\n\nQuestion: What is the purpose of incorporating the maximum entropy term?\n\nAnswer: The purpose of incorporating the maximum entropy term is to control the influence of the reference class distribution.\n\nQuestion: What is the purpose of incorporating the KL divergence term?\n\nAnswer: The purpose", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": "The baselines are SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with transformed word embedding, CNN, and RCNN.\n\nQuestion: What is the purpose of the UTCNN model?\n\nAnswer: The purpose of the UTCNN model is to utilize user, topic, and comment information for stance classification on social media texts.\n\nQuestion: What is the difference between the UTCNN model and the SVM models?\n\nAnswer: The UTCNN model incorporates user, topic, and comment information, while the SVM models only consider", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": "They improved by 1.5 points.\n\nQuestion: What is the name of the winning system of the 2016 edition of the challenge?\n\nAnswer: Balikas et al.\n\nQuestion: What is the name of the paper that proposed the multitask approach?\n\nAnswer: Caruana.\n\nQuestion: What is the name of the neural network architecture used in this work?\n\nAnswer: BiLSTM.\n\nQuestion: What is the name of the evaluation measure used in this work?\n\nAnswer: Mean Absolute Error.\n\nQuestion: What", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": "The adaptively sparse Transformer model improves interpretability by allowing different heads to learn to look for various relationships between tokens, leading to crisper examples of attention head behavior observed in previous work.\n\nQuestion: What is the purpose of the adaptive sparsity strategy in the Transformer architecture?\n\nAnswer: The adaptive sparsity strategy in the Transformer architecture allows different heads to learn to look for various relationships between tokens, leading to crisper examples of attention head behavior observed in previous work.\n\nQuestion: What is the purpose of the adaptive sparsity strategy in the Transformer architecture?", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "the baseline was the sentence-level translation model.\n\nQuestion: what was the main MT system?\n\nAnswer: the main MT system was the sentence-level translation model.\n\nQuestion: what was the DocRepair model?\n\nAnswer: the DocRepair model was a monolingual sequence-to-sequence model that corrected inconsistencies between sentence-level translations.\n\nQuestion: what was the training data used for the DocRepair model?\n\nAnswer: the training data used for the DocRepair model was monolingual document-level data.\n\n", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": "The metrics used for evaluation are the cross-lingual natural language inference (XNLI) test accuracy and the universal dependency parsing (UD) labeled attachment scores (LAS).\n\nQuestion: What is the purpose of the first step in the approach to learn the target embeddings?\n\nAnswer: The purpose of the first step is to estimate word translation probabilities from word embeddings of the two languages.\n\nQuestion: What is the purpose of the second step in the approach to learn the target embeddings?\n\nAnswer: The purpose of the second step is to fine-tune only foreign", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": "The attention module is pretrained on a large MT dataset.\n\nQuestion: What is the length of the speech encoder output?\n\nAnswer: The length of the speech encoder output is proportional to the length of the input frame, so it is much longer than a natural sentence.\n\nQuestion: What is the length of the text encoder output?\n\nAnswer: The length of the text encoder output is proportional to the length of the input frame, so it is much longer than a natural sentence.\n\nQuestion: What is the length of the target text decoder output?\n\nAnswer:", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": "Pragmatic features, such as emoticons, laughter expressions, and hashtags.\n\nQuestion: What is the purpose of the eye-tracking experiment?\n\nAnswer: To determine whether readers can identify sarcasm in text.\n\nQuestion: What is the significance of the eye-movement data?\n\nAnswer: It indicates that sarcasm induces distinctive eye-movement patterns, which can be used to detect sarcasm.\n\nQuestion: What is the purpose of the scanpath analysis?\n\nAnswer: To understand the cognitive process involved in sarcasm compreh", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "The encoder has an LSTM encoder.\n\nQuestion: What is the main objective of the auxiliary objective?\n\nAnswer: The main objective of the auxiliary objective is to predict the MSD tag of the target form.\n\nQuestion: What is the effect of encoding the full context with an LSTM?\n\nAnswer: The effect of encoding the full context with an LSTM is to increase the variance of the observed results.\n\nQuestion: What is the effect of adding the auxiliary objective of MSD prediction?\n\nAnswer: The effect of adding the auxiliary objective of M", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Yes. WordNet is a comprehensive lexical ontology that provides semantic context for words, making it useful for taxonomic reasoning.\n\nQuestion: What is the main focus of this paper?\n\nAnswer: The main focus of this paper is to assess the competence of transformer-based models in probing open-domain multiple-choice question answering (MCQA) tasks.\n\nQuestion: What is the main methodology used to construct the datasets?\n\nAnswer: The main methodology used to construct the datasets is to systematically generate synthetic datasets from expert knowledge sources such as WordNet and GC", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": "The baselines were the acoustic and pronunciation models, which were replaced with a convolutional neural network.\n\nQuestion: what was the architecture of the Jasper model?\n\nAnswer: The Jasper model had a block architecture with a Jasper INLINEFORM0 x INLINEFORM1 model having INLINEFORM2 blocks, each with INLINEFORM3 sub-blocks. Each sub-block applied the following operations: a 1D-convolution, batch norm, ReLU, and dropout. All sub-blocks in a block had the same number of output channels.\n\n", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "unanswerable\n\nQuestion: What is the purpose of the study?\n\nAnswer: unanswerable\n\nQuestion: What is the main contribution of the study?\n\nAnswer: unanswerable\n\nQuestion: What is the main objective of the study?\n\nAnswer: unanswerable\n\nQuestion: What is the main finding of the study?\n\nAnswer: unanswerable\n\nQuestion: What is the main conclusion of the study?\n\nAnswer: unanswerable\n\nQuestion: What is the main result of the study?\n\nAnswer: unanswerable\n\nQuestion: What is", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "We evaluate our models on perplexity, user-ranking, and qualitative analysis of generated recipes.\n\nQuestion: What is the size of the dataset used for training and evaluation?\n\nAnswer: The dataset consists of 180K+ recipes and 700K+ user reviews from Food.com.\n\nQuestion: What is the size of the vocabulary used for tokenization?\n\nAnswer: The vocabulary is 15K tokens across 19M total mentions.\n\nQuestion: What is the size of the hidden state used for the enc", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": "They create a label for each symptom and attribute.\n\nQuestion: What is the purpose of the simulated human-human dialogue dataset?\n\nAnswer: The purpose of the simulated human-human dialogue dataset is to bootstrap a prototype dialogue comprehension system.\n\nQuestion: What is the purpose of the simulated human-human dialogue dataset?\n\nAnswer: The purpose of the simulated human-human dialogue dataset is to bootstrap a prototype dialogue comprehension system.\n\nQuestion: What is the purpose of the simulated human-human dialogue dataset?\n\nAnswer: The purpose", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": "The training set contains only crowdsourced annotations. To label the training data, we use a 10-fold validation like setting. We iteratively retrain the LSTM-CRF-Pattern sequence tagger of Patel et al. patel2018syntactic on 9 folds of the training data and use that trained model to predict labels for the 10th. In this way we obtain predictions on the full training set. We then use predicted spans as proxy `ground truth' annotations to calculate the difficulty score of sentences as described above; we normalize these to the [", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": "The tasks used for evaluation are machine translation tasks of different training sizes: IWSLT 2017 German $\\rightarrow $ English, KFTT Japanese $\\rightarrow $ English, WMT 2016 Romanian $\\rightarrow $ English, and WMT 2014 English $\\rightarrow $ German.\n\nQuestion: What is the computational overhead of the models?\n\nAnswer: The computational overhead of the models is relatively small. The sparsity of the attention heads allows the models to be more efficient, with a speed-up of 75% for the softmax model and 90%", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": "The improvement in performance for Estonian in the NER task is significant.\n\nQuestion: What is the size of the training dataset used for the Latvian ELMo model?\n\nAnswer: The size of the training dataset used for the Latvian ELMo model is 270 million tokens.\n\nQuestion: What is the size of the training dataset used for the Swedish ELMo model?\n\nAnswer: The size of the training dataset used for the Swedish ELMo model is 20 million tokens.\n\nQuestion: What is the size of the training dataset used for the Croatian ELMo", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": "They have a background in the humanities and the social sciences.\n\nQuestion: What is the main goal of the research?\n\nAnswer: The main goal of the research is to study a “big question” that has been attracting interest for years.\n\nQuestion: What is the main goal of the research?\n\nAnswer: The main goal of the research is to study a “big question” that has been attracting interest for years.\n\nQuestion: What is the main goal of the research?\n\nAnswer: The main goal of the research is to study a “big question” that has been attract", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "No.\n\nQuestion: What is the purpose of the LDA model?\n\nAnswer: To model the topic distribution of documents.\n\nQuestion: What is the difference between the red bars and green bars in Figure 1?\n\nAnswer: The red bars represent spammers, and the green bars represent legitimate users.\n\nQuestion: What is the purpose of the topic-based features?\n\nAnswer: To detect spammers by means of the difference of their topic distribution patterns.\n\nQuestion: What is the purpose of the two sets of extracted topic-based features?\n\nAnswer: To", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": "The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages.\n\nQuestion: What is the purpose of language identification?\n\nAnswer: Language identification is the first step in many natural language processing and machine comprehension pipelines.\n\nQuestion: What is the purpose of language identification in harvesting language resources?\n\nAnswer: Language identification is used to bootstrap more accurate language resources.\n\nQuestion: What is the purpose of language identification in developing countries?\n\nAnswer: Language identification is used to bootstrap more accurate language resources in developing countries.\n\nQuestion", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "they compared the lstm models with the conventional rnn models.\n\nQuestion: what is the purpose of the lstm models?\n\nAnswer: the purpose of the lstm models is to improve the performance of the speech recognition system.\n\nQuestion: what is the difference between the lstm models and the conventional rnn models?\n\nAnswer: the lstm models have a long-range dependencies more accurately for temporal sequence conditions, while the conventional rnn models have a short-range dependencies.\n\nQuestion: what is the purpose of the lstm models in the real-time applications?\n\nAnswer:", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "29,794 articles\n\nQuestion: What is the Wikipedia grading scheme?\n\nAnswer: FA, GA, B, C, Start, Stub\n\nQuestion: What is the number of pages for papers in cs.ai, cs.cl, and cs.lg?\n\nAnswer: 11, 10, and 12\n\nQuestion: What is the number of pages for papers in cs.ai, cs.cl, and cs.lg?\n\nAnswer: 11, 10, and 12\n\nQuestion: What is the", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "The human judgements were assembled by means of a group of 50 native people who were well-versed in both English and Tamil languages.\n\nQuestion: What was the vocabulary size of the RNNMorph model?\n\nAnswer: The vocabulary size of the RNNMorph model was 41,906.\n\nQuestion: What was the vocabulary size of the RNNSearch model?\n\nAnswer: The vocabulary size of the RNNSearch model was 340,325.\n\n", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "No, they do not test their framework performance on commonly used language pairs, such as English-to-German.\n\nQuestion: What is the main difference between the multi-source translation approach and the approach proposed by BIBREF11?\n\nAnswer: The main difference between the multi-source translation approach and the approach proposed by BIBREF11 is that the multi-source translation approach requires a separate encoder and decoder for each language pair, while the approach proposed by BIBREF11 requires a single encoder and decoder for all language pairs.\n\nQuestion: What is the main reason", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": "The models are evaluated by measuring the efficiency of a communication scheme, which is measured as the fraction of tokens that are kept in the keywords, and the accuracy of a scheme, which is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence.\n\nQuestion: What is the main technical contribution of this work?\n\nAnswer: The main technical contribution is to draw inspiration from the multi-objective optimization literature and view the tradeoff as a sequence of constrained optimization problems, where the objective is to minimize the expected cost subject to varying expected reconstruction error constraints.", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": "The evaluation metrics looked at for classification tasks are precision, recall, and F-measure.\n\nQuestion: What is the purpose of the PA system in the article?\n\nAnswer: The purpose of the PA system in the article is to periodically measure and evaluate every employee's performance.\n\nQuestion: What is the purpose of the PA process in the article?\n\nAnswer: The purpose of the PA process in the article is to link the goals established by the organization to its each employee's day-to-day activities and performance.\n\nQuestion: What is the purpose of the PA corpus in the", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": "The source domain is the domain from which the labeled data is available, and the target domain is the domain from which the unlabeled data is available.\n\nQuestion: What is the main difference between the proposed method and previous works?\n\nAnswer: The proposed method aims to reduce the domain discrepancy by minimizing the distance between the source and target feature representations, while previous works highly rely on the selection of pivot features.\n\nQuestion: What is the purpose of semi-supervised learning in the proposed method?\n\nAnswer: The purpose of semi-supervised learning is to leverage unl", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": "LSTMs\n\nQuestion: what is the main difference between the input and context vector?\n\nAnswer: the semantics of the input and context vector are different\n\nQuestion: what is the main difference between the pyramidal transformation and the linear transformation?\n\nAnswer: the pyramidal transformation uses subsampling to effect multiple views of the input vector, while the linear transformation uses a single view\n\nQuestion: what is the main difference between the grouped linear transformation and the linear transformation?\n\nAnswer: the grouped linear transformation breaks the linear interactions by factoring the linear transformation into two steps, while the linear", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": "NeuronBlocks includes modules for word/character embedding, neural network layers, loss functions, metrics, and model architecture.\n\nQuestion: What is the purpose of the Block Zoo in NeuronBlocks?\n\nAnswer: The Block Zoo provides common layers like RNN, CNN, QRNN, Transformer, attention mechanisms, and regularization layers.\n\nQuestion: What is the Model Zoo in NeuronBlocks?\n\nAnswer: The Model Zoo provides end-to-end network templates for common NLP tasks.\n\nQuestion: What is the workflow of building", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": "The Wiktionary pronunciation corpus was used for training and testing.\n\nQuestion: what is the purpose of the artificial token?\n\nAnswer: It identifies the language of the source sequence, which is used to ease comparison to previous results.\n\nQuestion: what is the difference between the baseline and the sequence-to-sequence models?\n\nAnswer: The baseline models are purely monolingual, while the sequence-to-sequence models are multilingual.\n\nQuestion: what is the purpose of the artificial token in the multilingual model?\n\nAnswer: It identifies", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": "The baselines were BERT, RoBERTa, and XLNet.\n\nQuestion: What was the methodology used for speculation detection and scope resolution?\n\nAnswer: The methodology used was to preprocess the input sentence, split it into tokens, and then feed it to the model. The model's output was then compared with the true label for each token.\n\nQuestion: What was the input format for speculation detection and scope resolution?\n\nAnswer: The input format was a sequence of tokens, where each token was represented by a token ID. The input was then fed to the model", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": "English and Spanish\n\nQuestion: What is the main idea of their work?\n\nAnswer: To analyze the effect of both human and machine translation in cross-lingual models.\n\nQuestion: What is the goal of their work?\n\nAnswer: To analyze the effect of both human and machine translation in cross-lingual models.\n\nQuestion: What is the main result of their work?\n\nAnswer: They show that the translation artifacts revealed by their analysis are not exclusive to NLI, as they also show up on QA for the Translate-Test approach, but their actual impact can be highly", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": "They test their method on the task of predicting hashtags for a held-out set of posts.\n\nQuestion: What is the main benefit of the character-based approach?\n\nAnswer: The main benefit of the character-based approach is that it can generalize to unseen words at test time.\n\nQuestion: What is the objective function used to optimize the model?\n\nAnswer: The objective function used to optimize the model is the categorical cross-entropy loss between predicted and true hashtags.\n\nQuestion: What is the dimension of the character vector space?\n\nAnswer: The", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "No.\n\nQuestion: What is the vocabulary size of the English dataset?\n\nAnswer: 400K words.\n\nQuestion: What is the vocabulary size of the French and German datasets?\n\nAnswer: 297K and 143K words respectively.\n\nQuestion: What is the number of unique words in the corpus?\n\nAnswer: 400K words.\n\nQuestion: What is the number of examples in the English dataset?\n\nAnswer: 700K examples.\n\nQuestion: What is the number", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "Yes, PolyReponse was evaluated against a baseline system that uses a task-oriented dialogue system.\n\nQuestion: What is the purpose of PolyReponse?\n\nAnswer: The purpose of PolyReponse is to provide a conversational search engine that can assist users in finding a relevant restaurant according to their preference, and then additionally help them to make a booking in the selected restaurant.\n\nQuestion: What is the main challenge of using PolyReponse?\n\nAnswer: The main challenge of using PolyReponse is the need to collect domain-specific data labelled with", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": "They use the Meaning Extraction Method (MEM) BIBREF10 .\n\nQuestion: What is the purpose of the blogging community?\n\nAnswer: It is used by political consultants and news services as a tool for outreach and opinion forming as well as by businesses as a marketing tool to promote products and services BIBREF3 , BIBREF4 .\n\nQuestion: What is the name of the tool that can generate maps for word categories that reflect a certain psycholinguistic or semantic property?\n\nAnswer: The tool is called the LIWC.\n\nQuestion", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": "The ML methods aim to identify argument components in the discourse.\n\nQuestion: What is the main contribution of the article?\n\nAnswer: The main contribution of the article is the creation of a new corpus for argumentation mining.\n\nQuestion: What is the main contribution of the article?\n\nAnswer: The main contribution of the article is the creation of a new corpus for argumentation mining.\n\nQuestion: What is the main contribution of the article?\n\nAnswer: The main contribution of the article is the creation of a new corpus for argumentation mining.\n\nQuestion:", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "n-grams of order 1\n\nQuestion: Which of the following is the best metric for evaluating table-to-text generation?\n\nAnswer: PARENT\n\nQuestion: Which of the following is the best metric for evaluating table-to-text generation?\n\nAnswer: PARENT\n\nQuestion: Which of the following is the best metric for evaluating table-to-text generation?\n\nAnswer: PARENT\n\nQuestion: Which of the following is the best metric for evaluating table-to-text generation?\n\nAnswer: PARENT\n\nQuestion:", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": "The Twitter dataset contains 1,873 conversation threads, roughly 14k tweets.\n\nQuestion: What is the purpose of the analysis in this paper?\n\nAnswer: The purpose of the analysis is to verify the presence of therapeutic factors in online support groups.\n\nQuestion: What is the main conclusion of the paper?\n\nAnswer: The main conclusion is that online support groups are more beneficial for users than Twitter conversations.\n\nQuestion: What is the purpose of the analysis in this paper?\n\nAnswer: The purpose of the analysis is to verify the presence of", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": "The 12 languages covered are: English, French, Spanish, Mandarin Chinese, Polish, Russian, Welsh, Kiswahili, Estonian, Finnish, and Yue Chinese.\n\nQuestion: What is the number of word pairs?\n\nAnswer: The number of word pairs is 1,888.\n\nQuestion: What is the number of word pairs in the English dataset?\n\nAnswer: The number of word pairs in the English dataset is 999.\n\nQuestion: What is the number of word pairs in the cross-lingual dataset?\n\nAnswer", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": "Wikipedia conversations dataset and Reddit CMV data.\n\nQuestion: What is the main limitation of the current analysis?\n\nAnswer: It relies on balanced datasets, while derailment is a relatively rare event for which a more restrictive trigger threshold would be appropriate.\n\nQuestion: What is the purpose of the pre-training component of the model?\n\nAnswer: To learn an unsupervised representation of conversational dynamics.\n\nQuestion: What is the purpose of the prediction component of the model?\n\nAnswer: To fine-tune the pre-trained representation to forecast future", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "No, the pipeline components were not based on deep learning models.\n\nQuestion: What is the purpose of the lexicon matching module?\n\nAnswer: The lexicon matching module links words that are found in the text source with the data available not only on Eurovoc thesaurus but also on the EU's terminology database IATE.\n\nQuestion: What is the purpose of the subject-verb-object extraction module?\n\nAnswer: The subject-verb-object extraction module extracts subject-verb-object triples from the text.\n\nQuestion: What is the purpose of the", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": "The quality of the data is empirically evaluated by measuring the perplexity of the translations using a language model trained on a large amount of clean monolingual data.\n\nQuestion: What is the purpose of the Common Voice speech recognition corpus? \n\nAnswer: The purpose of the Common Voice speech recognition corpus is to create a crowdsourcing corpus with an open CC0 license.\n\nQuestion: What is the CC0 license for the Common Voice corpus? \n\nAnswer: The Common Voice corpus is licensed under CC0 license.\n\nQuestion", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": "They use two RNNs to encode audio and text sequences independently, and then combine the information from these sources using a feed-forward neural model to predict the emotion class.\n\nQuestion: What is the performance of the ARE model compared to the TRE model?\n\nAnswer: The ARE model shows the baseline performance because it uses minimal audio features, such as MFCC and prosodic features with simple architectures. The TRE model shows higher performance gain compared to the ARE model, and the MDRE model outperforms the best existing research results.\n\nQuestion: What is the performance", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": "2.11 BLEU, 1.7 FKGL and 1.07 SARI\n\nQuestion: what is the main limitation of the aforementioned NMT models for text simplification?\n\nAnswer: the amount of available parallel ordinary-simplified sentence pairs\n\nQuestion: what is the main reason for using simplified corpora in text simplification?\n\nAnswer: to improve the text fluency\n\nQuestion: what is the main limitation of the aforementioned NMT models for text simplification?\n\nAnswer: the amount of available parallel ordinary-simpl", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": "unanswerable\n\nQuestion: what is the name of the dataset used in the experiments?\n\nAnswer: OpenSubtitles2018\n\nQuestion: what is the number of sentences in the test set?\n\nAnswer: 30m\n\nQuestion: what is the number of sentences in the general development set?\n\nAnswer: 6m\n\nQuestion: what is the number of sentences in the contrastive development sets?\n\nAnswer: 500 examples from each\n\nQuestion: what is the number of sentences in the test set for ellipsis?\n\nAnswer:", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": "tweets that were retweeted more than 1000 times by the 8th of November 2016\n\nQuestion: What is the main goal of this paper?\n\nAnswer: To provide a preliminary characterization of fake news in Twitter by looking into meta-data.\n\nQuestion: What is the main finding of this paper?\n\nAnswer: The main finding is that there are specific pieces of meta-data about tweets that may allow the identification of fake news.\n\nQuestion: What is the main conclusion of this paper?\n\nAnswer: The main conclusion is that", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": "CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": "The data was collected using crowdsourcing.\n\nQuestion: how many speakers are included in the evaluation set?\n\nAnswer: 1969 speakers.\n\nQuestion: what is the main goal of the DeepMine project?\n\nAnswer: The main goal was to collect speech from at least a few thousand speakers, enabling research and development of deep learning methods.\n\nQuestion: what is the number of unique phrases in each part of the database?\n\nAnswer: The number of unique phrases in each part is shown in Table TABREF11.\n\nQuestion", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": "We used two machine learning and deep learning methods for RQE: Logistic Regression and a deep learning model based on a convolutional neural network.\n\nQuestion: What is the definition of RQE?\n\nAnswer: RQE is defined as: a question INLINEFORM0 entails a question INLINEFORM1 if every answer to INLINEFORM2 is also a complete or partial answer to INLINEFORM3 .\n\nQuestion: What is the definition of textual entailment?\n\nAnswer: Textual entailment is defined as: a question INLINEFORM0 entails a question", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "The benchmark dataset is the Honeypot dataset and its quality is high.\n\nQuestion: What is the difference between the red bars and green bars in the topic distribution of spammers and legitimate users?\n\nAnswer: The red bars represent spammers and the green bars represent legitimate users.\n\nQuestion: What is the difference between the two sets of extracted topic-based features?\n\nAnswer: The two sets of extracted topic-based features contain both local and global information, and the combination of them can distinguish human-like spammers effectively.\n\nQuestion: What is the difference between the", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": "The decoder has an LSTM encoder.\n\nQuestion: What is the main objective of the auxiliary objective?\n\nAnswer: The main objective of the auxiliary objective is to predict the MSD tag of the target form.\n\nQuestion: What is the effect of encoding the full context with an LSTM?\n\nAnswer: The effect of encoding the full context with an LSTM is to increase the variance of the observed results.\n\nQuestion: What is the effect of adding the auxiliary objective of MSD prediction?\n\nAnswer: The effect of adding the auxiliary objective of M", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "No.\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: A novel GAN-based event extraction model called AEM is proposed.\n\nQuestion: What is the objective function of the proposed AEM?\n\nAnswer: To compare the different GAN losses, Kurach kurach2018gan takes a sober view of the current state of GAN and suggests that the Jansen-Shannon divergence used in BIBREF3 performs more stable than variant objectives.\n\nQuestion: What is the event number INLINEFORM3 for three datasets?\n\nAnswer", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": "The best performing model among the author's submissions is the ensemble+ of (r4, r7 r12) from fold1-5, which achieved a F1 score of 0.673 on the test set.\n\nQuestion: What is the best performing model among the author's submissions for the task of sentence-level classification?\n\nAnswer: The best performing model among the author's submissions for the task of sentence-level classification is the ensemble+ of (r19 and r21) from the test set, which achieved a F1 score of 0", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "The baseline was a strong baseline established with monolingual data.\n\nQuestion: what was the purpose of the first fine-tuning stage?\n\nAnswer: The purpose of the first fine-tuning stage was to push up the translation quality for Ja INLINEFORM1 Ru pair.\n\nQuestion: what was the purpose of the second fine-tuning stage?\n\nAnswer: The purpose of the second fine-tuning stage was to push up the translation quality for Ja INLINEFORM2 Ru pair.\n\nQuestion: what was the purpose of the third fine-tuning stage?\n", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "'23.3%'\n\nQuestion: What was their highest 'MRR' score?\n\nAnswer: '0.6103'\n\nQuestion: What was their lowest 'MRR' score?\n\nAnswer: '4%'\n\nQuestion: What was their training accuracy?\n\nAnswer: '99.3%'\n\nQuestion: What was their test accuracy?\n\nAnswer: '26%'\n\nQuestion: What was their test accuracy in the second test batch set?\n\nAnswer: '4%'\n\nQuestion: What was their test accuracy in the third test batch", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "The paper explores word embedding techniques such as word2vec and neural networks.\n\nQuestion: What is the goal of the paper?\n\nAnswer: The goal of the paper is to discover methods that automatically reduce the amount of noise in a second-order co-occurrence vector.\n\nQuestion: What is the corpus used in the paper?\n\nAnswer: The corpus used in the paper is the NLM 2015 Medline baseline.\n\nQuestion: What is the reference standard used in the paper?\n\nAnswer: The reference standard used in the paper is the University of Minnesota", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "They match words by swapping the position of the noun phrase followed by a transitive verb with the transitive verb.\n\nQuestion: What is the initial learning rate used in the experiments?\n\nAnswer: The initial learning rate is INLINEFORM0.\n\nQuestion: What is the number of epochs used in the training?\n\nAnswer: The number of epochs is INLINEFORM1.\n\nQuestion: What is the vocabulary size of the English and Hindi vocabulary?\n\nAnswer: The vocabulary size of the English vocabulary is INLINE", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": "\n\nQuestion: What is the main problem in relation extraction in the biomedical domain?\n\nAnswer:\n\nQuestion: What is the state of biomedical text mining?\n\nAnswer:\n\nQuestion: What is the purpose of the paper?\n\nAnswer:\n\nQuestion: What is the main challenge in relation extraction in the biomedical domain?\n\nAnswer:\n\nQuestion: What is the purpose of the paper?\n\nAnswer:\n\nQuestion: What is the main challenge in relation extraction in the biomedical domain?\n\nAnswer:\n\nQuestion:", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": "We recruit seven experts with legal training to construct answers to Turker questions.\n\nQuestion: What is the purpose of the PrivacyQA dataset?\n\nAnswer: The purpose of the PrivacyQA dataset is to kickstart the development of question-answering methods for this domain, to address the (unrealistic) expectation that a large population should be reading many policies per day.\n\nQuestion: What is the distribution of questions in the corpus across OPP-115 categories?\n\nAnswer: The distribution of questions in the corpus across OPP-115 categories is as", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "The painting embedding model is a CNN-RNN generative model, while the language style transfer model is a seq2seq model with global attention.\n\nQuestion: What is the average content score for the Shakespearean prose generated by the model?\n\nAnswer: The average content score is 3.7.\n\nQuestion: What is the average creativity score for the Shakespearean prose generated by the model?\n\nAnswer: The average creativity score is 3.9.\n\nQuestion: What is the average style score for the Shakespearean prose generated by the model?\n\nAnswer: The", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": "The transformer layer works better.\n\nQuestion: What is the main limitation of BERT?\n\nAnswer: The main limitation of BERT is its quadratic complexity in the number of segments.\n\nQuestion: What is the computational complexity of RoBERT?\n\nAnswer: The computational complexity of RoBERT is $O(\\frac{n^2}{k^2})$.\n\nQuestion: What is the computational complexity of ToBERT?\n\nAnswer: The computational complexity of ToBERT is $O(\\frac{n^2}{k^2})$.\n\nQuestion: What is the computational complexity of", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": "No, the authors do not hypothesize that humans' robustness to noise is due to their general knowledge.\n\nQuestion: What is the main contribution of KAR?\n\nAnswer: The main contribution of KAR is that it explicitly uses the general knowledge extracted by the data enrichment method to assist its attention mechanisms.\n\nQuestion: What is the purpose of the data enrichment method?\n\nAnswer: The purpose of the data enrichment method is to extract inter-word semantic connections from each passage-question pair in the MRC dataset, which are used as general knowledge to assist the attention", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "They addressed three topics of cyberbullying: personal attack, racism, and sexism.\n\nQuestion: What is the main bottleneck of previous works on cyberbullying detection?\n\nAnswer: The main bottleneck of previous works on cyberbullying detection is the lack of consideration of multiple social media platforms and topics of cyberbullying.\n\nQuestion: What is the effect of using swear words in cyberbullying detection?\n\nAnswer: The effect of using swear words in cyberbullying detection is that it can lead to low precision and recall.\n", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "They use two contexts: a combination of the left context, the left entity and the middle context; and a combination of the middle context, the right entity and the right context.\n\nQuestion: What is the objective function used for training the RNNs?\n\nAnswer: The objective function is the ranking loss function proposed in deSantos2015.\n\nQuestion: What is the difference between the CNN and RNN models?\n\nAnswer: The CNN models use a discrete convolution on an input matrix with a set of different filters, while the RNN models use a recurrent neural network for sentence", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "The dataset contains four different types of entities: PER, LOC, ORG, and MISC.\n\nQuestion: What is the total number of entities in the dataset?\n\nAnswer: The total number of entities in the dataset is 16225.\n\nQuestion: What is the total number of words in the dataset?\n\nAnswer: The total number of words in the dataset is 16225.\n\nQuestion: What is the total number of sentences in the dataset?\n\nAnswer: The total number of sentences in the dataset is 6400.\n", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": "The resulting annotated data is higher quality.\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: The main contribution of this paper is the introduction of the task of predicting annotation difficulty for biomedical information extraction (IE).\n\nQuestion: What is the hypothesis that is tested in this paper?\n\nAnswer: The hypothesis that is tested in this paper is whether expert annotations should be collected whenever possible.\n\nQuestion: What is the motivating hypothesis for this paper?\n\nAnswer: The motivating hypothesis for this paper is that annotation quality for difficult instances is important for final model performance", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": "33.16% of the speakers are women, and 22.57% of the total speech time is represented by women.\n\nQuestion: What is the role of speaker's gender in ASR performance?\n\nAnswer: Gender is a factor of variation in ASR performance, with a WER increase of 24% for women compared to men.\n\nQuestion: What is the role of speaker's role in ASR performance?\n\nAnswer: Speaker's role seems to have an impact on WER, with a WER increase of 27.", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "The article achieves state of the art results on the Multi30K dataset.\n\nQuestion: What is the main metric used to evaluate the performance of the models?\n\nAnswer: The main metric used to evaluate the performance of the models is Meteor BIBREF31 .\n\nQuestion: What is the difference between the deliberation models and the base models?\n\nAnswer: The deliberation models are enriched with image information, while the base models are not.\n\nQuestion: What is the difference between the RND and AMB degradation strategies?\n\nAnswer: The RND de", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": "The strong baselines model is compared to are BIBREF13, BIBREF4, BIBREF6, BIBREF7, BIBREF9, BIBREF11, BIBREF18, BIBREF10, BIBREF17, BIBREF18, BIBREF33, BIBREF34, BIBREF36, BIBREF37, BIBREF23, BIBREF24, BIBREF25, BIBREF26, BIBREF27, BIBREF28, BIBREF", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "Logistic Regression (LR) and Multilayer Perceptron (MLP)\n\nQuestion: What is the purpose of the keyword discovery task?\n\nAnswer: To discover new keywords that are most informative for model training with respect to existing keywords.\n\nQuestion: How does the model's expectation influence the target model's performance?\n\nAnswer: It is used as a regularization term in the target model's objective function to constrain the posterior distribution of the model predictions.\n\nQuestion: How does the keyword discovery method compare to a query expansion approach?\n\nAnswer: Our approach out", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "BIBREF17 and BIBREF18\n\nQuestion: What is the sentiment analysis task?\n\nAnswer: To determine the sentiment of a tweet towards a candidate\n\nQuestion: What is the accuracy of the crowdworkers for sentiment analysis?\n\nAnswer: 31.7%\n\nQuestion: What is the accuracy of the automated systems for named-entity recognition?\n\nAnswer: 77.2% to 96.7%\n\nQuestion: What is the accuracy of the automated systems for sentiment analysis?\n\nAnswer: 31.7%\n", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": "The experiments are performed on the SQuAD dataset.\n\nQuestion: What is the task definition?\n\nAnswer: The task definition is to find the best question based on the answer-aware relation.\n\nQuestion: What is the answer-aware relation extraction method?\n\nAnswer: The answer-aware relation extraction method is to use an off-the-shelf toolbox of OpenIE to extract structured answer-relevant relations.\n\nQuestion: What is the decoder used in the model?\n\nAnswer: The decoder used in the model is an LSTM.\n\nQuestion", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "The existing approaches include the use of bag-of-words representations, the use of vector space embeddings, and the use of word embedding models.\n\nQuestion: what is the main hypothesis of the paper?\n\nAnswer: The main hypothesis is that by using vector space embeddings instead of bag-of-words representations, the ecological information which is implicitly captured by Flickr tags can be utilized in a more effective way.\n\nQuestion: what is the main finding of the paper?\n\nAnswer: The main finding is that the use of vector space embeddings leads to substantially better", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": "yes\n\nQuestion: What is the name of the model?\n\nAnswer: SAN\n\nQuestion: What is the main difference between the proposed model and SAN?\n\nAnswer: The unanswerable classifier is a pair-wise classification model.\n\nQuestion: What is the objective function of the joint model?\n\nAnswer: The objective function of the joint model has two parts: span loss function and binary classifier.\n\nQuestion: What is the contribution of this work?\n\nAnswer: The contribution of this work is to propose a simple yet efficient model for MRC that handles unanswerable", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": "They used the CSAT dataset for CSAT prediction, the 20 newsgroups dataset for topic identification, and the Fisher Phase 1 corpus for topic identification.\n\nQuestion: What is the main limitation of BERT?\n\nAnswer: The main limitation of BERT is its inability to handle long sequences due to its quadratic complexity in the number of segments.\n\nQuestion: What is the computational complexity of RoBERT?\n\nAnswer: The computational complexity of RoBERT is $O(\\frac{n^2}{k^2})$, which is asymptotically inferior to ToBERT but", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": "IMDb movie review dataset\n\nQuestion: What is the number of units per layer in the best performing QRNN model for language modeling?\n\nAnswer: 640\n\nQuestion: What is the number of units per layer in the best performing QRNN model for character-level machine translation?\n\nAnswer: 320\n\nQuestion: What is the number of layers in the QRNN model for language modeling?\n\nAnswer: 2\n\nQuestion: What is the number of layers in the QRNN model for character-level machine translation?\n\nAnswer: 4", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": "No, the tasks were not evaluated in any previous work.\n\nQuestion: What is the main difference between the BERT model and the LSTM model?\n\nAnswer: The BERT model is bi-directional, while the LSTM model is uni-directional.\n\nQuestion: What is the main difference between the BERT model and the BERT-Large model?\n\nAnswer: The BERT model is smaller and more efficient, while the BERT-Large model is larger and more powerful.\n\nQuestion: What is the main difference between the BERT model and the B", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": "Yes.\n\nQuestion: What is the accuracy of the automated systems for named-entity recognition?\n\nAnswer: The CCR of the automated systems ranged from 77.2% to 96.7%.\n\nQuestion: What is the accuracy of the automated systems for sentiment analysis?\n\nAnswer: The CCR of the automated systems was 31.7%.\n\nQuestion: What is the accuracy of the crowdworkers for named-entity recognition?\n\nAnswer: The CCR of the crowdworkers was 98.6%.\n\nQuestion: What", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": "The invertibility condition is that the Jacobian determinant of the neural projector is equal to one, which ensures that the projection is volume-preserving and invertible.\n\nQuestion: What is the purpose of the invertibility condition?\n\nAnswer: The invertibility condition prevents information loss during optimization by ensuring that the volume of the projection space is preserved.\n\nQuestion: What is the purpose of the Jacobian regularization term?\n\nAnswer: The Jacobian regularization term prevents information loss by ensuring that the Jacobian determinant is nonzero and differentiable.\n\nQuestion: What is the", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "The proposed qualitative annotation schema includes categories such as reasoning, knowledge, and complexity, as well as metrics based on lexical cues to approximate a lower bound for the complexity of the reading comprehension task.\n\nQuestion: What is the purpose of the proposed framework?\n\nAnswer: The purpose of the proposed framework is to systematically analyse MRC gold standard data, to categorise the linguistic complexity, required reasoning, and factual correctness, and to approximate the complexity of gold standards.\n\nQuestion: What are the dimensions of interest in the framework?\n\nAnswer: The dimensions of interest in the framework", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": "The size of WikiSmall is 89,042 sentence pairs, and the size of WikiLarge is 296,402 sentence pairs.\n\nQuestion: what is the main limitation of the aforementioned NMT models for text simplification?\n\nAnswer: The main limitation of the aforementioned NMT models for text simplification is the lack of parallel ordinary-simplified sentence pairs.\n\nQuestion: what is the size of vocabulary used in the simplified dataset?\n\nAnswer: The size of vocabulary used in the simplified", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": "The baselines are the vanilla ST baseline, pre-training baseline, multi-task baseline, many-to-many+pre-training, and triangle+pre-train.\n\nQuestion: What is the purpose of the article?\n\nAnswer: The purpose of the article is to investigate the end-to-end method for ST and to discuss why there is a huge gap between pre-training and fine-tuning in previous methods.\n\nQuestion: What is the role of the text encoder in the TCEN architecture?\n\nAnswer: The text encoder consumes speech encoder", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": "English\n\nQuestion: What is the purpose of the shared task on fine-grained propaganda detection?\n\nAnswer: To develop models that identify linguistic propaganda techniques.\n\nQuestion: What is the granularity of the propaganda detection task?\n\nAnswer: Sentence-level classification (SLC) and field-level classification (FLC).\n\nQuestion: What is the p-value threshold used to determine the similarity of corpora?\n\nAnswer: 0.001.\n\nQuestion: What is the minimum p-value for the similarity between 25% of a", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": "The models used in the experiment are a linear SVM trained on word unigrams, a bidirectional Long Short-Term-Memory (BiLSTM) model, and a Convolutional Neural Network (CNN) model.\n\nQuestion: What is the purpose of the hierarchical annotation model proposed in this paper?\n\nAnswer: The purpose of the hierarchical annotation model proposed in this paper is to capture the different types of offensive language and the targets of the offensive language.\n\nQuestion: What is the breakdown of the data into training and testing for the labels from each level", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "No, the answer is not provided in the article.\n\nQuestion: What is the main goal of the article?\n\nAnswer: The main goal of the article is to study the answerability of questions on Quora.\n\nQuestion: What is the main goal of the article?\n\nAnswer: The main goal of the article is to study the answerability of questions on Quora.\n\nQuestion: What is the main goal of the article?\n\nAnswer: The main goal of the article is to study the answerability of questions on Quora.\n\nQuestion: What is the main goal of the article", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "Edinburgh embeddings BIBREF14 are obtained by training skip-gram model on Edinburgh corpus BIBREF15 .\n\nQuestion: what is the name of the system that was used to extract features?\n\nAnswer: EmoInt .\n\nQuestion: what is the name of the task that the article is about?\n\nAnswer: The Task 1 of WASSA-2017 BIBREF0 poses a problem of finding emotion intensity of four emotions namely anger, fear, joy, sadness from tweets.\n\nQuestion: what is the name of the tool", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": "The results of the new dataset were not mentioned in the article.\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: The main contribution of the paper is to explore a new task of generating plausible and personalized recipes from incomplete input specifications by leveraging historical user preferences.\n\nQuestion: What is the input specification of the model?\n\nAnswer: The input specification of the model includes the recipe name as a sequence of tokens, a partial list of ingredients, and a caloric level (high, medium, low).\n\nQuestion: What is the output", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": "The combination of rewards for reinforcement learning is the harmonic mean of irony accuracy and sentiment preservation.\n\nQuestion: What is the main reason for the issue that some of the outputs prefer to repeat the same word?\n\nAnswer: The main reason for the issue that some of the outputs prefer to repeat the same word is that reinforcement learning rewards encourage the model to generate words which can get high scores from classifiers and even back-translation cannot stop it.\n\nQuestion: What is the conclusion of the transformation from ironic sentences to non-ironic sentences?\n\nAnswer:", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": "The authors demonstrate that their model does not work well with Shakespeare style transfer for some paintings, such as \"Starry Night\" with a low average content score. This happens when the style transfer dataset does not have similar words in the training set of sentences.\n\nQuestion: What is the goal of the Shakespearean prose generation model?\n\nAnswer: The goal of the Shakespearean prose generation model is to generate a sequence of words as a poem for an image to maximize the expected return.\n\nQuestion: What is the main idea of the article?\n\nAnswer: The main idea of the article is to", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": "The Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset.\n\nQuestion: What was the purpose of the ISEAR dataset?\n\nAnswer: To gather insights in cross-cultural aspects of emotional reactions.\n\nQuestion: What was the purpose of the SemEval 2007 Task 14?\n\nAnswer: To classify emotions and valence in news headlines.\n\nQuestion: What was the purpose of the Google News and CNN datasets?\n\nAnswer: To collect headlines from news websites.\n\nQuestion", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": "The distribution of friends/followers was different for accounts spreading fake news and those spreading viral content.\n\nQuestion: What was the number of viral tweets containing fake news?\n\nAnswer: 136\n\nQuestion: What was the number of viral tweets not containing fake news?\n\nAnswer: 1327\n\nQuestion: What was the number of viral tweets that went viral?\n\nAnswer: 1327\n\nQuestion: What was the number of viral tweets that were retweeted more than 1000 times", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": "The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset.\n\nQuestion: What is the goal of the pairwise ranking model?\n\nAnswer: The goal of the pairwise ranking model is to divide a given hashtag into a sequence of meaningful words.\n\nQuestion: What is the architecture of the pairwise neural ranking model?\n\nAnswer: The architecture of the pairwise neural ranking model is presented in Figure FIGREF11 (a).\n\nQuestion: What is the architecture of the multi-task learning model?\n\nAnswer: The architecture of the", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": "The corpus contains Persian accents.\n\nQuestion: what is the number of speakers in the corpus?\n\nAnswer: The corpus contains 1969 speakers.\n\nQuestion: what is the number of sessions recorded by females?\n\nAnswer: The corpus contains 13200 sessions recorded by females.\n\nQuestion: what is the number of trials for Persian 1-sess?\n\nAnswer: The corpus contains 100-spk with 1-session enrolment (1-sess) as the main evaluation condition", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "Word subspace can represent sets of word vectors, retaining most of the variability of the data.\n\nQuestion: What is the purpose of the word subspace formulation?\n\nAnswer: The purpose of the word subspace formulation is to model word vectors from each class in the Reuters-8 database into a word subspace.\n\nQuestion: What is the purpose of the TF weighted word subspace extension?\n\nAnswer: The purpose of the TF weighted word subspace extension is to incorporate the frequency of words directly in the modeling of the subspace by using a weight", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": "B1. The first baseline uses only the salience-based features by Dunietz and Gillick.\n\nQuestion: What is the overall performance of INLINEFORM0 and comparison to baselines?\n\nAnswer: The overall performance is INLINEFORM1 P=0.93, R=0.514, F1=0.676.\n\nQuestion: What is the robustness of INLINEFORM0?\n\nAnswer: The robustness is INLINEFORM2 P=+0.64.\n\nQuestion: What is the impact of the individual feature groups", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": "No.\n\nQuestion: What is the number of Transformer blocks in the GlossBERT model?\n\nAnswer: 12.\n\nQuestion: What is the number of the hidden layer in the GlossBERT model?\n\nAnswer: 768.\n\nQuestion: What is the number of self-attention heads in the GlossBERT model?\n\nAnswer: 12.\n\nQuestion: What is the total number of parameters of the pre-trained GlossBERT model?\n\nAnswer: 110M.\n\nQuestion: What is", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": "Augmented LibriSpeech dataset is 110 hours long.\n\nQuestion: What is the purpose of Common Voice corpus?\n\nAnswer: The purpose of Common Voice corpus is to create a crowdsourcing speech recognition corpus with an open CC0 license.\n\nQuestion: What is the language of the Tatoeba evaluation set?\n\nAnswer: The language of the Tatoeba evaluation set is French, German, Dutch, Russian and Spanish.\n\nQuestion: What is the license of the Tatoeba evaluation set?\n\nAnswer: The license of", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": "The dataset for fine-grained classification is split in training, development, development_test and test parts.\n\nQuestion: What is the primary task?\n\nAnswer: The fine-grained dataset is highly unbalanced and skewed towards the positive sentiment: only INLINEFORM0 of the training examples are labeled with one of the negative classes.\n\nQuestion: What is the secondary task?\n\nAnswer: The ternary task is a binary classification problem.\n\nQuestion: What is the goal of multitask learning?\n\nAnswer: The goal is to learn a model jointly for them", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": "They use the pre-trained uncased BERT$_\\mathrm {BASE}$ model for fine-tuning.\n\nQuestion: What is the number of Transformer blocks in the GlossBERT model?\n\nAnswer: The number of Transformer blocks is 12.\n\nQuestion: What is the number of the hidden layer in the GlossBERT model?\n\nAnswer: The number of the hidden layer is 768.\n\nQuestion: What is the number of self-attention heads in the GlossBERT model?\n\nAnswer: The number of self-att", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": "Yes, the automatically constructed datasets are subject to quality control. The authors have conducted careful validation experiments to ensure that the datasets are free from systematic biases and that the results are reliable.\n\nQuestion: What is the main purpose of the probing methodology?\n\nAnswer: The main purpose of the probing methodology is to answer the empirical questions posed at the beginning of the article. The authors aim to understand the knowledge contained in language models after they have been trained for a QA end-task using benchmark datasets in which such knowledge is expected to be widespread.\n\nQuestion: What is", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": "unanswerable\n\nQuestion: What is the purpose of the article?\n\nAnswer: unanswerable\n\nQuestion: What is the main focus of the article?\n\nAnswer: unanswerable\n\nQuestion: What is the main point of the article?\n\nAnswer: unanswerable\n\nQuestion: What is the main idea of the article?\n\nAnswer: unanswerable\n\nQuestion: What is the main conclusion of the article?\n\nAnswer: unanswerable\n\nQuestion: What is the main message of the article?\n\nAnswer: unanswerable\n\nQuestion: What is", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": "The best model (B-M) on development data relies entirely on automatically obtained information, both in terms of training data as well as features.\n\nQuestion: What is the name of the dataset used for the evaluation of emotion classification?\n\nAnswer: The ISEAR dataset is the name of the dataset used for the evaluation of emotion classification.\n\nQuestion: What is the name of the dataset used for the evaluation of emotion classification?\n\nAnswer: The ISEAR dataset is the name of the dataset used for the evaluation of emotion classification.\n\nQuestion: What is the name of the", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": "The tagging scheme consists of two tags: INLINEFORM0 and INLINEFORM1.\n\nQuestion: What is the INLINEFORM0 tag used for?\n\nAnswer: The INLINEFORM0 tag indicates that the current word appears before the pun in the given context.\n\nQuestion: What is the INLINEFORM1 tag used for?\n\nAnswer: The INLINEFORM1 tag highlights the current word is a pun.\n\nQuestion: What is the INLINEFORM2 tag used for?\n\nAnswer: The INLINEFORM2 tag indicates that the current word appears after the pun.\n", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": "No.\n\nQuestion: What is the total speech duration of the CoVost corpus?\n\nAnswer: 708 hours.\n\nQuestion: What is the total number of speakers in the CoVost corpus?\n\nAnswer: Over 11,000.\n\nQuestion: What is the total number of accents in the CoVost corpus?\n\nAnswer: Over 60.\n\nQuestion: What is the total number of sentences in the CoVost corpus?\n\nAnswer: Over 708,000.\n\nQuestion", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": "The robustness of a model is defined as its ability to handle bias in the prior knowledge that we supply to the learning model.\n\nQuestion: What is the main contribution of this work?\n\nAnswer: The main contribution of this work is to show three terms to make the model more robust.\n\nQuestion: What is the purpose of the article?\n\nAnswer: The purpose of the article is to investigate into the problem of how to leverage prior knowledge robustly in learning models.\n\nQuestion: What is the framework of Generalized Expectation Criteria?\n\nAnswer: The framework of Generalized Ex", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": "tf-idf, average GloVe embeddings, InferSent, and Universal Sentence Encoder.\n\nQuestion: What is the purpose of SentEval?\n\nAnswer: To evaluate the quality of sentence embeddings.\n\nQuestion: What is the performance of SBERT on the STS benchmark dataset?\n\nAnswer: It achieves a new state-of-the-art for the SentEval toolkit.\n\nQuestion: What is the pooling strategy used in SBERT?\n\nAnswer: MEAN, MAX, and CLS.\n\nQuestion:", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "The method's improvements of F1 for NER task for English and Chinese datasets are:\n\n- English datasets: +0.29 and +0.96 for CoNLL2003 and OntoNotes5.0, respectively.\n- Chinese datasets: +0.97 and +2.36 for MSRA and OntoNotes4.0, respectively.\n\nQuestion: What are the datasets used for MRC task?\n\nAnswer: The datasets used for MRC task are:\n\n- SQuAD v1.1 and SQuAD v2", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": "They test their conflict method on two tasks: Quora duplicate question pair detection and Bing's People Also Ask.\n\nQuestion: What is the main objective of the attention mechanism?\n\nAnswer: The main objective of the attention mechanism is to compute alignment scores (or weight) between every word representation pairs from two different sequences.\n\nQuestion: What is the difference between the conflict model and the attention model?\n\nAnswer: The conflict model computes element wise difference between two vectors followed by a linear transformation to produce a scalar weight. The attention model uses dot product or sometimes addition followed by a linear projection to a scalar", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": "They compared against the following baselines:\n- Syntactic tree-based models\n- Latent tree-based models\n- Sequential neural models\n- Non-tree models\n\nQuestion: What was the purpose of the ablation study?\n\nAnswer: The purpose of the ablation study was to explore the effectiveness of the core modules of the model, specifically the leaf-LSTM and structure-aware tag embeddings.\n\nQuestion: What was the result of the ablation study?\n\nAnswer: The result of the ablation study was that the leaf-LSTM was the", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": "The core component for KBQA is the relation detection model.\n\nQuestion: What is the main focus of this work?\n\nAnswer: The main focus of this work is to improve the relation detection subtask and further explore how it can contribute to the KBQA system.\n\nQuestion: What is the main difference between general relation detection tasks and KB-specific relation detection?\n\nAnswer: The main difference is that the number of target relations is limited, normally smaller than 100.\n\nQuestion: What is the main problem with relation detection for KBQA?\n\nAnswer", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "The baseline models are the encoder-decoder model and the encoder-decoder model with ingredient attention.\n\nQuestion: What is the purpose of the personalized models?\n\nAnswer: The purpose of the personalized models is to generate plausible and personalized recipes from incomplete input specifications and user histories.\n\nQuestion: What is the dataset used for the experiments?\n\nAnswer: The dataset used for the experiments is a novel dataset of 180K recipes and 700K reviews from Food.com.\n\nQuestion: What is the evaluation strategy", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": "The methods considered to find examples of biases and unwarranted inferences are manual detection, part-of-speech information, and coreference graph.\n\nQuestion: What is the purpose of the Flickr30K dataset?\n\nAnswer: The purpose of the Flickr30K dataset is to train and evaluate neural network models that generate image descriptions.\n\nQuestion: What is the assumption behind the Flickr30K dataset?\n\nAnswer: The assumption behind the Flickr30K dataset is that the descriptions are based on the images, and nothing else.\n", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": "French\n\nQuestion: What is the name of the challenge?\n\nAnswer: Winograd Schema Challenge\n\nQuestion: What is the name of the challenge sponsored by Nuance Inc?\n\nAnswer: Winograd Schema Challenge\n\nQuestion: What is the name of the challenge offered for the first time at IJCAI-2016?\n\nAnswer: Winograd Schema Challenge\n\nQuestion: What is the name of the challenge that is administered by commonsensereasoning.org?\n\nAnswer: Winograd Schema Challenge\n\nQuestion:", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": "They experimented with the following models: (i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0 , (iii) models without INLINEFORM1 , and (iv) models that integrate lower contexts via peephole connections.\n\nQuestion: What is the purpose of the additional forget gate?\n\nAnswer: The additional forget gate is used to control the amount of information transmitted from INLINEFORM2 and INLINEFORM3 , the candidate cell state and the previous cell state, to the new cell state INLINEFORM4 .\n\nQuestion: What", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": "No, they report results on English data and also on a snapshot of English Wikipedia measuring 8GB in size.\n\nQuestion: What is the purpose of the proposed method?\n\nAnswer: The purpose of the proposed method is to obtain improved word representations under some performance metric.\n\nQuestion: What is the problem with the previous studies?\n\nAnswer: The problem with the previous studies is that they do not need external resources, which results in fewer resources but also lack the aid of information extracted from these resources.\n\nQuestion: What is the objective function used in the proposed method?\n\nAnswer: The objective", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": "The authors experimented with several summarization algorithms, including those provided by the Sumy package.\n\nQuestion: What is the purpose of the PA system in the organization used in the paper?\n\nAnswer: The PA system in the organization used in the paper is used to periodically measure and evaluate every employee's performance.\n\nQuestion: What is the purpose of the PA process in the organization used in the paper?\n\nAnswer: The PA process in the organization used in the paper is used to link the goals established by the organization to its each employee's day-to-day activities and performance.\n\n", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": "The previous state of the art for this task was proposed by BIBREF0 .\n\nQuestion: What is the primary problem of predicting instructor intervention in MOOCs?\n\nAnswer: The primary problem of predicting instructor intervention in MOOCs is to infer the appropriate amount of context to intervene.\n\nQuestion: What is the secondary problem of predicting instructor intervention in MOOCs?\n\nAnswer: The secondary problem of predicting instructor intervention in MOOCs is to infer the context that triggers instructor intervention.\n\nQuestion: What is the role of", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": "The question cannot be answered based on the information in the article.\n\nQuestion: Which component is the most impactful?\n\nAnswer: The question cannot be answered based on the information in the article.\n\nQuestion: Which component is the most impactful?\n\nAnswer: The question cannot be answered based on the information in the article.\n\nQuestion: Which component is the most impactful?\n\nAnswer: The question cannot be answered based on the information in the article.\n\nQuestion: Which component is the most impactful?\n\nAnswer: The question cannot be answered based on the information in the article", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": "The corpus used for the task is the diachronic corpus pair from BIBREF0: DTA18 and DTA19.\n\nQuestion: What is the goal of the shared task?\n\nAnswer: The goal of the shared task is to create an architecture to detect semantic change and to rank words according to their degree of lexical semantic change between two different time periods.\n\nQuestion: What is the metric used to assess the performance of the models?\n\nAnswer: The metric used to assess the performance of the models is Spearman's $\\rho $.\n\nQuestion: What are", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "Kannada, Hindi, Telugu, Malayalam, Bengali, and English.\n\nQuestion: What is the accuracy of the i-vector+svm system?\n\nAnswer: 98.43% F1-Score.\n\nQuestion: What is the accuracy of the TDNN+stat-pool system?\n\nAnswer: 98.24% F1-Score.\n\nQuestion: What is the accuracy of the GhostVLAD pooling approach?\n\nAnswer: 98.43% F1-Score.\n\nQuestion", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": "The model performance on target language reading comprehension is not answered in the article.\n\nQuestion: What is the model performance on target language reading comprehension?\n\nAnswer: The model performance on target language reading comprehension is not answered in the article.\n\nQuestion: What is the model performance on target language reading comprehension?\n\nAnswer: The model performance on target language reading comprehension is not answered in the article.\n\nQuestion: What is the model performance on target language reading comprehension?\n\nAnswer: The model performance on target language reading comprehension is not answered in the article.\n\nQuestion:", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": "The proposed model outperforms the baselines significantly.\n\nQuestion: What is the main contribution of the proposed model?\n\nAnswer: The main contribution of the proposed model is the ability to recover the language style of specific characters without their dialogue.\n\nQuestion: What is the purpose of the proposed model?\n\nAnswer: The purpose of the proposed model is to imitate human-like qualities by incorporating human-like attributes of characters.\n\nQuestion: What is the task of the proposed model?\n\nAnswer: The task of the proposed model is to retrieve the language style of a specific character without", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "ARAML improves the quality of generated text samples and enhances the stability of training process.\n\nQuestion: What is the main difference between ARAML and MaliGAN?\n\nAnswer: ARAML gets samples from a stationary distribution near the real data, while MaliGAN acquires samples from the generator's distribution.\n\nQuestion: What is the impact of temperature on the performance of ARAML?\n\nAnswer: As the temperature becomes larger, forward perplexity increases gradually while Self-BLEU decreases.\n\nQuestion: What is the impact of sampling strategy on", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": "The authors present evidence that the model can capture some biases in data annotation and collection by examining the confusion matrices for the test datasets and manually inspecting a subset of the data. They also discuss the misclassifications that occur due to biases in the data collection and annotation rules.\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: The main contribution of this paper is to propose a transfer learning approach using the pre-trained language model BERT to enhance the performance of a hate speech detection system and to generalize it to new datasets. The authors also introduce new fine-tuning strateg", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "Yes, we tested several baselines, including a majority-class baseline, a word count baseline, and a human performance baseline.\n\nQuestion: What is the name of the corpus used to collect privacy policy questions and answers?\n\nAnswer: The corpus is called PrivacyQA.\n\nQuestion: What is the goal of the PrivacyQA corpus?\n\nAnswer: The goal of the PrivacyQA corpus is to promote question-answering research in the specialized privacy domain.\n\nQuestion: What is the distribution of questions in the PrivacyQA", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "The dataset is divided into three parts with 64%, 16% and 20% of the total dataset into training set, development set and test set respectively.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset is divided into three parts with 64%, 16% and 20% of the total dataset into training set, development set and test set respectively.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset is divided into three parts with 64%, 16% and 20% of the total", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": "The proposed method improves F1 by 0.58 for MRPC and 0.73 for QQP.\n\nQuestion: What are the datasets used for NER task?\n\nAnswer: The datasets used for NER task are CTB5, CTB6, OntoNotes5.0, MSRA 96.72, and OntoNotes4.0.\n\nQuestion: What are the hyperparameters used in TI?\n\nAnswer: The hyperparameters used in TI are $\\alpha $ and $\\beta $.\n\nQuestion: What are the datasets", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "The datasets used are the ERP data from BIBREF0 and the eye-tracking and self-paced reading time data from BIBREF0 .\n\nQuestion: What is the purpose of the article?\n\nAnswer: The purpose of the article is to explore the relationship between ERP components and language models.\n\nQuestion: What is the dual-stream model of speech comprehension?\n\nAnswer: The dual-stream model of speech comprehension is a model that suggests that there are two streams of processing involved in speech comprehension: a ventral stream that maps phonemes onto words and semantic", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": "The data presented to the subjects was EEG data corresponding to imagined speech.\n\nQuestion: What was the purpose of the development set in the training process?\n\nAnswer: The purpose of the development set was to evaluate the best architectures and hyperparameters for the networks with a reasonable number of runs.\n\nQuestion: What was the purpose of the leave-one-subject out cross-validation experiment?\n\nAnswer: The purpose of the leave-one-subject out cross-validation experiment was to perform a fair comparison with the previous methods reported on the same dataset.\n\nQuestion: What was the name of the", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": "The baselines used for evaluation are Pointer-Gen, Pointer-Gen-RL+ROUGE, Pointer-Gen-RL-SEN, and Pointer-Gen-ARL-SEN.\n\nQuestion: What is the sensationalism score used to evaluate the sensationalism of headlines?\n\nAnswer: The sensationalism score used to evaluate the sensationalism of headlines is the ratio of sensational headlines to non-sensational headlines.\n\nQuestion: What is the reward function used in the reinforcement learning training?\n\nAnswer: The reward function used", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": "The learning models used on the dataset are traditional machine learning classifiers and neural network based models.\n\nQuestion: What is the most accurate learning model on the dataset?\n\nAnswer: The most accurate learning model on the dataset is the RNN with LTC modules.\n\nQuestion: What is the highest F1 score for the RNN model?\n\nAnswer: The highest F1 score for the RNN model is 0.551 for the \"spam\" label.\n\nQuestion: What is the highest F1 score for the CNN model?\n\nAnswer: The highest F1 score for the", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": "The language model architectures used are a bi-directional language model to augment the sequence to sequence encoder and a uni-directional model to augment the decoder.\n\nQuestion: What is the most effective strategy for adding pre-trained representations to the encoder network?\n\nAnswer: The most effective strategy for adding pre-trained representations to the encoder network is to input language model representations to the encoder.\n\nQuestion: What is the most effective strategy for adding pre-trained representations to the decoder?\n\nAnswer: The most effective strategy for adding pre-trained representations to", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": "The weights are dynamically adjusted based on the training loss, which encourages learning easier examples first.\n\nQuestion: What is the purpose of using the Sørensen–Dice coefficient?\n\nAnswer: The Sørensen–Dice coefficient is used to gauge the similarity of two sets, and is a F1-oriented statistic.\n\nQuestion: What is the purpose of using the Tversky index?\n\nAnswer: The Tversky index is used to control the tradeoff between false-negatives and false-positives, and is a more general version of the $F_{\\beta", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": "The proposed strategies are able to pass the bottleneck in Zork1, and outperform the baseline A2C and KG-A2C.\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: The main contribution is the introduction of a method that detects bottlenecks in text-games using the overall reward gained and the knowledge graph state.\n\nQuestion: What is the state representation used in the paper?\n\nAnswer: The state representation is a knowledge graph, which is a set of 3-tuples of the form $\\langle subject, relation", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": "An individual model consists of monolingual models for each language and crosslingual latent variables to incorporate soft role agreement between aligned constituents.\n\nQuestion: What is the generative process for sampling roles in the multilingual model?\n\nAnswer: The generative process for sampling roles in the multilingual model involves considering three factors: two corresponding to probabilities of generating the aligned roles, and the third one corresponding to selecting the CLV according to CRP.\n\nQuestion: What is the metric used for evaluation?\n\nAnswer: The metric used for evaluation is the metric proposed by lang20", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": "The non-standard pronunciation is identified by the transcriber using the Transcription Bottleneck.\n\nQuestion: What is the main criterion for choosing alphabetic characters in the Mapudungun alphabet?\n\nAnswer: The main criterion for choosing alphabetic characters in the Mapudungun alphabet is to use the current Spanish keyboard that was available on all computers in Chilean offices and schools.\n\nQuestion: What is the purpose of the dialogues in the corpus?\n\nAnswer: The purpose of the dialogues in the corpus is to be natural in Map", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": "A semichar architecture is a type of architecture that uses a single layer BiLSTM with a hidden dimension size of 50.\n\nQuestion: What is the sensitivity of a word recognition model?\n\nAnswer: The sensitivity of a word recognition model is the expected number of unique outputs it assigns to a set of adversarial perturbations.\n\nQuestion: What is the robustness of a classifier to an adversary?\n\nAnswer: The robustness of a classifier to an adversary is the worst-case adversarial performance of the classifier.\n\nQuestion: What is the", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": "16 languages.\n\nQuestion: what is the main goal of the article?\n\nAnswer: to compare the impact of external lexicons and word vector representations on the accuracy of PoS models.\n\nQuestion: what is the starting point of the article?\n\nAnswer: the MElt system.\n\nQuestion: what is the feature set used by MElt?\n\nAnswer: a superset of the feature sets used by BIBREF5 and BIBREF24.\n\nQuestion: what is the best performing feature set?\n\nAnswer: the one described in Table TABREF1", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": "NCEL outperforms the state-of-the-art collective methods across five different datasets.\n\nQuestion: What is the main limitation of the global approach?\n\nAnswer: The third issue of inadequate training data.\n\nQuestion: What is the main goal of NCEL?\n\nAnswer: The goal is to find an assignment.\n\nQuestion: What is the main idea of BIBREF26?\n\nAnswer: The main idea is to enhance the features from the hidden states of the mention itself as well as its neighbors.\n\nQuestion: What is the", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": "Yes.\n\nQuestion: What is the percentage of times dosage is correct in the case of the best performing model?\n\nAnswer: 71.75%.\n\nQuestion: What is the percentage of times frequency is correct in the case of the best performing model?\n\nAnswer: 73.58%.\n\nQuestion: What is the percentage of times dosage is correct in the case of the best performing model?\n\nAnswer: 71.75%.\n\nQuestion: What is the percentage of times frequency is correct in the case of the best performing model?\n\nAnswer", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "The baseline used was the original FCE training set.\n\nQuestion: What was the main evaluation measure used?\n\nAnswer: The main evaluation measure used was INLINEFORM0.\n\nQuestion: What was the main finding of the paper?\n\nAnswer: The main finding of the paper was that artificial data can significantly improve error detection performance.\n\nQuestion: What was the main contribution of the paper?\n\nAnswer: The main contribution of the paper was the development of two AEG methods and their evaluation on error detection benchmarks.\n\nQuestion: What was the main limitation of the paper?\n\n", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": "The annotated clinical notes were obtained from the i2b2/VA BIBREF0 dataset.\n\nQuestion: what is the purpose of the CE task in 2013 ShARe/CLEF Task 1?\n\nAnswer: The purpose of the CE task in 2013 ShARe/CLEF Task 1 is to design an algorithm to tag a set of predefined entities of interest in clinical notes.\n\nQuestion: what is the difference between the CE task in 2013 ShARe/CLEF and the CE task in ", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": "Masking words can help to generate summaries.\n\nQuestion: Why is the summary length important?\nAnswer: The summary length is important to generate summaries.\nQuestion: Why is the summary length important?\nAnswer: The summary length is important to generate summaries.\nQuestion: Why is the summary length important?\nAnswer: The summary length is important to generate summaries.\nQuestion: Why is the summary length important?\nAnswer: The summary length is important to generate summaries.\nQuestion: Why is the summary length important?\nAnswer: The summary length is important to generate summaries.\n", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": "PPDB\n\nQuestion: What is the objective function of the model?\n\nAnswer: Predicting adjacent word (within-tweet relationships), adjacent tweet (inter-tweet relationships), the tweet itself (autoencoder), modeling from structured resources like paraphrase databases and weak supervision.\n\nQuestion: What is the motivation behind modeling within-tweet relationships?\n\nAnswer: Every tweet is assumed to have a latent topic vector, which influences the distribution of the words in the tweet.\n\nQuestion: What is the motivation behind model", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": "TF-IDF features are used.\n\nQuestion: What is the primary objective of the study?\n\nAnswer: The primary objective of the study is to analyze the efficacy of existing machine learning approaches for the automated classification of pathology reports into different diagnosis categories.\n\nQuestion: What is the distribution of reports across different primary diagnoses and primary sites?\n\nAnswer: The distribution of reports across different primary diagnoses and primary sites is reported in tab:report-distribution.\n\nQuestion: What is the performance of the XGBoost classifier?\n\nAnswer: The XGBoost", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": "The dataset is annotated with binary labels for each tweet, indicating whether it contains evidence of depression or not.\n\nQuestion: What is the purpose of the feature ablation study?\n\nAnswer: The purpose of the feature ablation study is to assess the contribution of each feature group to classification performance.\n\nQuestion: What is the purpose of the feature elimination study?\n\nAnswer: The purpose of the feature elimination study is to determine the optimal percentile of top ranked features for classifying Twitter tweets in the depression schema hierarchy.\n\nQuestion: What is the optimal percentile of top ranked", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": "The eight NER tasks they evaluated on were:\n\n1. PubMed+PMC (PubMed Central)\n2. PubMed\n3. CORD-19 (Covid-19 Open Research Dataset)\n4. PubMed+PMC+CORD-19\n5. PubMed+PMC+CORD-19+CORD-19\n6. PubMed+PMC+CORD-19+CORD-19+CORD-19\n7. PubMed+PMC+CORD-19+CORD-1", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": "The training data was translated from English to Spanish.\n\nQuestion: What was the method used to create the word embeddings?\n\nAnswer: The word embeddings were created using word2vec in the gensim library.\n\nQuestion: What was the parameter search method used for the neural networks?\n\nAnswer: A parameter search was conducted for the number of layers, the number of nodes, and dropout.\n\nQuestion: What was the semi-supervised learning method used for the Spanish data?\n\nAnswer: The semi-supervised learning method used for the Spanish data was to translate the English", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": "They used a content-based classifier.\n\nQuestion: What was the purpose of their research?\n\nAnswer: To explore the potential of predicting a user's industry from their language.\n\nQuestion: What was the main contribution of their research?\n\nAnswer: They built a large, industry-annotated dataset and explored the effectiveness of using textual features from the users' profile metadata in industry inference.\n\nQuestion: What was the main limitation of their research?\n\nAnswer: They used a micro-blogging platform, which inherently restricts the number of characters that a post can", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "The baseline system for the SLC task used a very simple logistic regression classifier with default parameters, where we represented the input instances with a single feature: the length of the sentence.\n\nQuestion: What was the purpose of the shared task?\n\nAnswer: The purpose of the shared task was to create a corpus of news articles annotated with an inventory of 18 propaganda techniques at the fragment level.\n\nQuestion: What was the corpus used for the shared task?\n\nAnswer: The corpus used for the shared task was a subset of the one used for a previous shared task", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": "They compare with prior works that do not adopt joint learning.\n\nQuestion: What is the INLINEFORM0 tagging scheme?\n\nAnswer: The INLINEFORM0 tagging scheme indicates that the current word appears before the pun in the given context.\n\nQuestion: What is the INLINEFORM1 tagging scheme?\n\nAnswer: The INLINEFORM1 tagging scheme highlights the current word is a pun.\n\nQuestion: What is the INLINEFORM2 tagging scheme?\n\nAnswer: The INLINEFORM2 tagging scheme indicates that the current word appears after the pun.\n\n", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": "The political bias of different sources is included in the model by training only on left-biased or right-biased articles from the US dataset.\n\nQuestion: What is the purpose of the layer of pure tweets?\n\nAnswer: The layer of pure tweets is used to encode information about the number of pure tweets and the number of unique users authoring those tweets.\n\nQuestion: What is the purpose of the layer of mentions?\n\nAnswer: The layer of mentions is used to encode information about the number of mentions and the number of unique users mentioned.\n\nQuestion: What is the purpose", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": "The ancient Chinese dataset comes from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era.\n\nQuestion: What is the purpose of the ancient-modern Chinese translation dataset?\n\nAnswer: The purpose of the ancient-modern Chinese translation dataset is to promote people to absorb and develop Chinese culture.\n\nQuestion: What is the most critical step in creating the ancient-modern Chinese dataset?\n\nAnswer: The most critical step in creating the ancient-modern Chinese dataset is the clause alignment step, which", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "English\n\nQuestion: What is the purpose of the article?\n\nAnswer: To introduce a new dataset for offensive language identification.\n\nQuestion: What is the key contribution of the paper?\n\nAnswer: A hierarchical three-level annotation model for offensive language identification.\n\nQuestion: What is the first level of the annotation model?\n\nAnswer: Offensive language detection.\n\nQuestion: What is the second level of the annotation model?\n\nAnswer: Categorization of offensive language.\n\nQuestion: What is the third level of the annotation model?\n\nAnswer: Off", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": "The Chinese datasets used in this work are the Penn Treebank (PTB) and the Chinese Parsed Treebank (CPTB).\n\nQuestion: what is the generative process of the compound pcfg?\n\nAnswer: The generative process of the compound PCFG is as follows: we first obtain rule probabilities via a prior with parameters, then a tree/sentence is sampled from a PCFG with rule probabilities given by the prior.\n\nQuestion: what is the difference between the compound pcfg and the neural pcfg?\n\nAnswer: The compound", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": "The UTCNN model has three layers: a general CNN, a semantic transformation layer, and a fully connected layer.\n\nQuestion: What is the purpose of the maximum pooling layer in the UTCNN model?\n\nAnswer: The maximum pooling layer is used to select the most important feature for comments.\n\nQuestion: What is the purpose of the LDA topic model in the UTCNN model?\n\nAnswer: The LDA topic model is used to determine the latent topics with which to build topic embeddings.\n\nQuestion: What is the purpose of the user matrix embedding layer in the UTCNN model", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": "The dataset used in this paper is the set of locations INLINEFORM0 .\n\nQuestion: what is the main hypothesis of the paper?\n\nAnswer: The main hypothesis of the paper is that by using vector space embeddings instead of bag-of-words representations, the ecological information which is implicitly captured by Flickr tags can be utilized in a more effective way.\n\nQuestion: what is the main value of using vector space embeddings in this context?\n\nAnswer: The main value of using vector space embeddings in this context is not so much about abstracting away from specific", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": "The clinical datasets used in the paper are NUBes-PHI and MEDDOCAN.\n\nQuestion: What is the purpose of the MEDDOCAN challenge?\n\nAnswer: The purpose of the MEDDOCAN challenge is to evaluate how good a BERT-based model performs without language nor domain specialisation apart from the training data labelled for the task at hand.\n\nQuestion: What is the purpose of the NUBes-PHI dataset?\n\nAnswer: The purpose of the NUBes-PHI dataset is to evaluate how good a BERT-based model performs without language", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": "Unigrams and Pragmatic features BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 (b) Stylistic patterns BIBREF4 and patterns related to situational disparity BIBREF5 and (c) Hastag interpretations BIBREF6 , BIBREF7 .\n\nQuestion: What is the purpose of the eye-tracking experiment?\n\nAnswer: To test the statistical significance of the average fixation duration per word for sarcastic and non-sarcastic texts.\n\nQuestion: What is the purpose of the sarc", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are the predictive performance and strategy formulation ability.\n\nQuestion: What is the purpose of the lifelong interactive learning and inference (LiLi) approach? \n\nAnswer: The purpose of the lifelong interactive learning and inference (LiLi) approach is to build a generic engine for continuous knowledge learning in human-machine conversations.\n\nQuestion: What is the main difference between the lifelong learning and the lifelong interactive learning and inference (LiLi) approach? \n\nAnswer: The", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "yes\n\nQuestion: What is the name of the dataset that gives the least portion of overlaps between question-answer pairs?\n\nAnswer: InfoboxQA\n\nQuestion: What is the name of the dataset that is used for the evaluation of another selection-based QA task?\n\nAnswer: SelQA\n\nQuestion: What is the name of the dataset that is used for the evaluation of answer retrieval?\n\nAnswer: WikiQA\n\nQuestion: What is the name of the dataset that is used for the evaluation of answer triggering?\n\nAnswer: SelQA\n\n", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": "Galatasaray and Fenerbahçe\n\nQuestion: What is the domain of the tweets?\n\nAnswer: Sports-related tweets\n\nQuestion: What is the purpose of the stance detection systems?\n\nAnswer: To facilitate the use of the opinions of the football followers by these clubs\n\nQuestion: What is the purpose of the stance detection systems?\n\nAnswer: To facilitate the use of the opinions of the football followers by these clubs\n\nQuestion: What is the purpose of the stance detection systems?\n\nAnswer: To facilitate the use of", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": "Experimental results demonstrate that our model achieves a high irony accuracy with well-preserved sentiment and content.\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: The main contribution of the paper is to provide the first large-scale irony dataset and make our model as a benchmark for the irony generation.\n\nQuestion: What is the purpose of the article?\n\nAnswer: The purpose of the article is to explore irony generation based on style transfer.\n\nQuestion: What is the sentiment reward used to control sentiment preservation?\n\nAnswer: The sentiment reward is used to", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": "Gaussian-masked directional multi-head attention is a variant of scaled dot-product attention which uses a triangular matrix mask to let the self-attention focus on different weights. It ensures that the relationship between adjacent characters is weaker than adjacent characters.\n\nQuestion: What is the purpose of using bi-affine attention scorer?\n\nAnswer: Bi-affine attention scorer is used to label the gap. It uses the forward information of character in front of the gap and the backward information of the character behind the gap to distinguish the position of characters.\n\nQuestion: What is the", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "They considered social media texts from Facebook.\n\nQuestion: What is the purpose of this paper?\n\nAnswer: The purpose of this paper is to introduce models for both (a) causality prediction and (b) causal explanation identification.\n\nQuestion: What is the contribution of this paper?\n\nAnswer: The contribution of this paper is to build a recursive neural network model which uses distributed representation of discourse arguments as this approach can even capture latent properties of causal relations which may exist between distant discourse arguments.\n\nQuestion: What is the dataset used for the evaluation of the causality prediction of the", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": "The network's baseline features are the features extracted from the pre-trained models.\n\nQuestion: What is the purpose of the article?\n\nAnswer: The purpose of the article is to introduce the proposed approach for sarcasm detection.\n\nQuestion: What is the main contribution of the proposed approach?\n\nAnswer: The main contribution of the proposed approach is the use of a relatively smaller feature set, automatic feature extraction, the use of deep networks, and the adoption of pre-trained NLP models.\n\nQuestion: What is the aim of the baseline method?\n\nAnswer", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": "The number of clusters and the number of clusters were varied in the experiments on the four tasks.\n\nQuestion: What is the goal of the fine-grained sentiment classification task?\n\nAnswer: The goal of the fine-grained sentiment classification task is to predict the sentiment of an input text according to a five point scale.\n\nQuestion: What is the evaluation measure used for the fine-grained sentiment classification task?\n\nAnswer: The evaluation measure used for the fine-grained sentiment classification task is the macro-averaged Mean Absolute Error scores.\n\nQuestion: What is the goal of the fine", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": "The scores of their system were 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": "53 documents, containing an average number of 156.1 sentences per document, each with 19.55 tokens on average.\n\nQuestion: What is the most frequently annotated type of entity?\n\nAnswer: Findings are the most frequently annotated type of entity.\n\nQuestion: What is the scope of the manual annotation?\n\nAnswer: The scope of the manual annotation is limited to the presentation of a patient's signs and symptoms.\n\nQuestion: What is the corpus format?\n\nAnswer: The corpus is distributed in BioC JSON format.\n", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": "Yes.\n\nQuestion: What is the purpose of the cloze-style questions?\n\nAnswer: To generate a large number of clozes given any corpus.\n\nQuestion: What is the process of constructing cloze-style questions?\n\nAnswer: We use a standard NLP pipeline to identify entities and phrases, and then construct cloze-style questions by replacing the answer bearing sentence in the introduction with a placeholder.\n\nQuestion: What is the performance of the cloze-style questions on the Wikipedia corpus?\n\nAnswer: We were able to answer 76% times for the", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": "They consider text categorization, sentiment classification, and web-page classification.\n\nQuestion: What is the purpose of the article?\n\nAnswer: The purpose of the article is to investigate into the problem of how to leverage prior knowledge robustly in learning models.\n\nQuestion: What is the framework of GE-FL?\n\nAnswer: The framework of GE-FL is the generalized expectation criteria.\n\nQuestion: What is the objective function of GE-FL?\n\nAnswer: The objective function of GE-FL is to minimize the KL divergence between the predicted and reference class", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": "The model is compared to previous methods such as TREC question classification, biomedical question classification, and consumer health question classification.\n\nQuestion: What is the performance of the BERT-QC model at 57.8% P@1?\n\nAnswer: The performance of the BERT-QC model at 57.8% P@1 is shown to be statistically significant, with a ceiling gain of up to 12% P@1.\n\nQuestion: What is the error analysis of the BERT-QC model?\n\nAnswer: The error analysis of", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": "The training sets of these versions of ELMo are significantly larger than the previous ones.\n\nQuestion: What is the purpose of the ELMo model's architecture?\n\nAnswer: The purpose of the ELMo model's architecture is to capture the context of a sentence.\n\nQuestion: What is the difference between the ELMoForManyLangs models and the EMBEDDIA models?\n\nAnswer: The ELMoForManyLangs models were trained on a significantly smaller datasets, while the EMBEDDIA models were trained on larger datasets.\n\nQuestion: What is the purpose of the word", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": "The dataset contains 10000 sentences.\n\nQuestion: What is the name of the dataset used in the experiments?\n\nAnswer: The dataset used in the experiments is OurNepali dataset.\n\nQuestion: What is the name of the dataset received from ILPRL lab?\n\nAnswer: The dataset received from ILPRL lab is ILPRL dataset.\n\nQuestion: What is the name of the model architecture used in the experiments?\n\nAnswer: The model architecture used in the experiments is BiLSTM+CNN(grapheme-level).\n\nQuestion: What is", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": "They compare to MLP, Eusboost, and MWMOTE.\n\nQuestion: What is the purpose of the proposed s2s framework?\n\nAnswer: The purpose of the proposed s2s framework is to effectively learn the class discriminative characteristics, even from low data resources.\n\nQuestion: What is the main intention for the task of classifying speech and music?\n\nAnswer: The main intention for the task of classifying speech and music is not better feature selection, but to demonstrate the effectiveness of the proposed approach.\n\nQuestion: What is the purpose of the proposed s2s", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": "Yes.\n\nQuestion: What is the main contribution of their work?\n\nAnswer: They propose a new multimodal NER (MNER) task on short social media posts, and demonstrate an effective MNER system that combines visual information with textual information to outperform traditional text-based NER baselines.\n\nQuestion: What is the main result of their experiments?\n\nAnswer: The main result is that the modality attention module is able to improve the model more robust to missing tokens without having to train an indefinitely large word embeddings matrix for arbitrarily noisy social media text", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": "No, they evaluate on the Wall Street Journal (WSJ) portion of the Penn Treebank.\n\nQuestion: What is the dimensionality of the input layer in the neural projector?\n\nAnswer: The dimensionality INLINEFORM0 is set to 100.\n\nQuestion: What is the number of coupling layers used in the neural projector?\n\nAnswer: The number of coupling layers are varied as 4, 8, 16 for both tasks.\n\nQuestion: What is the dimensionality of the input layer in the neural projector?\n\nAnswer: The dimensional", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": "'32%'\n\nQuestion: What was their highest 'MRR' score?\n\nAnswer: '32%'\n\nQuestion: What was their highest 'MRR' score?\n\nAnswer: '32%'\n\nQuestion: What was their highest 'MRR' score?\n\nAnswer: '32%'\n\nQuestion: What was their highest 'MRR' score?\n\nAnswer: '32%'\n\nQuestion: What was their highest 'MRR' score?\n\nAnswer: '32%'\n\nQuestion: What was their highest 'MRR' score?", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": "They evaluate on the Wall Street Journal (WSJ) portion of the Penn Treebank.\n\nQuestion: What is the number of tag clusters used in the POS tagging experiment?\n\nAnswer: 45 tag clusters.\n\nQuestion: What is the dimensionality of the input layer in the invertible network?\n\nAnswer: The dimensionality of the input layer is the same as the number of input units.\n\nQuestion: What is the number of coupling layers used in the invertible network?\n\nAnswer: The number of coupling layers is varied as 4, 8, 1", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": "The authors evidence the claim by conducting a survey among engineers and identifying a spectrum of three typical personas. The survey showed that about 87.5% NLP related jobs belong to a few common tasks, such as sentence classification, text matching, sequence labeling, MRC, etc. It further suggested that more than 90% of the networks were composed of several common components, such as embedding, CNN/RNN, Transformer and so on.\n\nQuestion: What is the purpose of the Block Zoo in NeuronBlocks?\n\nAnswer: The Block Zoo is built on", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": "They achieve the state of the art on the SimpleQuestions and WebQSP benchmarks.\n\nQuestion: What is the main focus of this work?\n\nAnswer: The main focus of this work is to improve the relation detection subtask and further explore how it can contribute to the KBQA system.\n\nQuestion: What is the main difference between general relation detection tasks and KB-specific relation detection?\n\nAnswer: The main difference is that the number of target relations is limited, normally smaller than 100.\n\nQuestion: What is the main problem with relation detection for KBQ", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
