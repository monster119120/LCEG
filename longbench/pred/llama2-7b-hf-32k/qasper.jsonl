{"pred": "The dataset BIBREF8 , manually labelled by an expert, has been publicly released and is available to researchers and interested parties.\n\nAnswer: unanswerable\n\nQuestion: What is the difference between the number of followers of accounts that spread fake news and those that do not?\n\nAnswer: accounts spreading fake news appear to have, on average, the same number of friends than those spreading viral content only.\n\nAnswer: unanswerable\n\nQuestion: What is the difference between the number of followers of accounts that spread fake news and those that do not?\n\nAnswer", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": "GhostVLAD is an extension of the NetVLAD approach, which we discussed in the previous section. The GhostVLAD model was proposed for face recognition by Y. Zhong [10]. GhostVLAD works exactly similar to NetVLAD except it adds Ghost clusters along with the NetVLAD clusters. So, now we will have a K+G number of clusters instead of K clusters. Where G is the number of ghost clusters, we want to add (typically 2-4). The Ghost clusters are added to map any noisy or irrelevant content into ghost clusters and are", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": "68.8% to 71.8%\n\nQuestion: What is the name of the dataset that they use?\n\nAnswer: IEMOCAP\n\nQuestion: What is the name of the model that they propose?\n\nAnswer: MDRE\n\nQuestion: What is the name of the model that they propose?\n\nAnswer: MDREA\n\nQuestion: What is the name of the model that they propose?\n\nAnswer: MDRE\n\nQuestion: What is the name of the model that they propose?\n\nAnswer: MDRE\n\nQuestion: What is the", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": "Context tweets are used as additional features.\n\nQuestion: What is the most accurate model?\n\nAnswer: RNN with LTC modules\n\nQuestion: What is the most accurate model?\n\nAnswer: RNN with LTC modules\n\nQuestion: What is the most accurate model?\n\nAnswer: RNN with LTC modules\n\nQuestion: What is the most accurate model?\n\nAnswer: RNN with LTC modules\n\nQuestion: What is the most accurate model?\n\nAnswer: RNN with LTC modules\n\nQuestion: What is the most accurate model?\n\nAnswer", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "The pages they looked at were FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.\n\nQuestion: What is the name of the dataset they used?\n\nAnswer: Affective Text dataset\n\nQuestion: What is the name of the dataset they used?\n\nAnswer: ISEAR\n\nQuestion: What is the name of the dataset they used?\n\nAnswer: Fairy T", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "Yes\n\nQuestion: What is the difference between the two datasets?\n\nAnswer: The STAN INLINEFORM0 dataset contains 1,108 hashtags from 1,268 tweets, while the STAN INLINEFORM1 dataset contains 12,594 hashtags from 126,800 tweets.\n\nQuestion: What is the difference between the two datasets?\n\nAnswer: The STAN INLINEFORM0 dataset contains 1,108 hashtags from 1,268 tweets, while the STAN IN", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": "unanswerable\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: unanswerable\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: unanswerable\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: unanswerable\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: unanswerable\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: unanswerable\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: unanswerable\n\nQuestion: What", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "The datasets used for evaluation are the CNN/DailyMail news highlights dataset, the New York Times Annotated Corpus (NYT), and XSum.\n\nQuestion: What is the architecture of the model?\n\nAnswer: The architecture of the model is a document-level encoder based on Bert, followed by inter-sentence Transformer layers.\n\nQuestion: What is the training schedule for the model?\n\nAnswer: The training schedule for the model is a warmup schedule with a learning rate schedule.\n\nQuestion: What is the training schedule for the model?\n\nAnswer:", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": "GM$\\_$KL performs better than other approaches on the benchmark word similarity and entailment datasets.\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": "The model is trained on the BookTest dataset.\n\n Question: What is the name of the dataset that was used to train the model?\n\nAnswer: BookTest\n\n Question: What is the name of the model that was used to train the model?\n\nAnswer: psr\n\n Question: What is the name of the model that was used to train the model?\n\nAnswer: psr\n\n Question: What is the name of the model that was used to train the model?\n\nAnswer: psr\n\n Question: What is the name of the model that was used to train the model", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": "The source of the datasets are different. Friends is speech-based dialogues and EmotionPush is chat-based dialogues. It makes datasets possess different characteristics.\n\nQuestion: What is the main goal of the present work?\n\nAnswer: The main goal of the present work is to predict the emotion of utterance within the dialogue.\n\nQuestion: What is the main difficulty of the present work?\n\nAnswer: The emotion of the utterances depends not only on the text but also on the interaction happened earlier.\n\nQuestion: What is the main difficulty of the present work?\n\n", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": "English\n\nQuestion: what is the main idea of this paper?\n\nAnswer: The paper proposes a method to use synthetic data to improve the performance of neural machine translation models for text simplification.\n\nQuestion: what is the main idea of this paper?\n\nAnswer: The paper proposes a method to use synthetic data to improve the performance of neural machine translation models for text simplification.\n\nQuestion: what is the main idea of this paper?\n\nAnswer: The paper proposes a method to use synthetic data to improve the performance of neural machine translation models for text simplification.\n", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": "IMDb dataset of movie reviews\n\nQuestion: What is the corpus used for named entity recognition?\n\nAnswer: Groningen Meaning Bank (GMB)\n\nQuestion: What is the corpus used for sentiment analysis?\n\nAnswer: English Wiki News Abstract\n\nQuestion: What is the corpus used for sentiment analysis?\n\nAnswer: English Wiki Simple (SW) Articles\n\nQuestion: What is the corpus used for sentiment analysis?\n\nAnswer: Billion Word (BW)\n\nQuestion: What is the corpus used for sentiment analysis?\n\nAnswer: English Wik", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "The proposed system achieves 86.5% F1 on the DL-PS dataset, 84.5% F1 on the EC-MT dataset, and 83.5% F1 on the EC-UQ dataset.\n\nQuestion: What is the main idea of the proposed system?\n\nAnswer: The main idea of the proposed system is to use adversarial training to learn worker independent features from crowd annotators.\n\nQuestion: What is the main contribution of the proposed system?\n\nAnswer: The main contribution of the proposed system is to use adversarial training to learn worker", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": "yes\n\nQuestion: What is the main difference between the two reading paradigms?\n\nAnswer: the second task involves annotating a specific relation in each sentence\n\nQuestion: What is the main difference between the two reading paradigms?\n\nAnswer: the second task involves annotating a specific relation in each sentence\n\nQuestion: What is the main difference between the two reading paradigms?\n\nAnswer: the second task involves annotating a specific relation in each sentence\n\nQuestion: What is the main difference between the two reading paradigms?\n\nAnswer: the second task involves annotating a", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": "The datasets are the ones that were created by the authors.\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: The main contribution of the paper is the conceptual architecture for MPCS and its implementation.\n\nQuestion: What is the main limitation of the paper?\n\nAnswer: The main limitation of the paper is that it does not provide a norms specification for MPCS.\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: The main contribution of the paper is the conceptual architecture for MPCS and its implementation.\n\nQuestion: What is", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "The best performance was achieved by the HealthCare sector.\n\n", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "RNN-based NMT and Transformer-NMT\n\nQuestion: what is the best setting for the proposed method?\n\nAnswer: the best setting is to involve all these three factors\n\nQuestion: what is the best setting for the proposed method?\n\nAnswer: the best setting is to involve all these three factors\n\nQuestion: what is the best setting for the proposed method?\n\nAnswer: the best setting is to involve all these three factors\n\nQuestion: what is the best setting for the proposed method?\n\nAnswer: the best setting is to involve all these three factors\n\nQuestion:", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": "The three regularization terms are: (1) a regularization term associated with neutral features; (2) the maximum entropy of class distribution regularization term; and (3) the KL divergence between reference and predicted class distribution.\n\nQuestion: What is the main contribution of this work?\n\nAnswer: The main contributions of this work are as follows: (1) The rest of the paper is structured as follows: In Section 2, we briefly describe the generalized expectation criteria and present the proposed regularization terms. In Section 3, we conduct extensive experiments to justify the proposed methods. We survey related work", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": "SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, two mature deep learning models on text classification, CNN, and RCNN.\n\nQuestion: What is the FBFans dataset?\n\nAnswer: It is a privately-owned, single-topic, Chinese, unbalanced, social media dataset.\n\nQuestion: What is the CreateDebate dataset?\n\nAnswer: It is a public, multiple-topic, English, balanced, forum dataset.\n\nQuestion: What is the FBF", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": "1.5\n\nQuestion: What is the name of the paper?\n\nAnswer: Multitask Learning for Sentiment Classification of Tweets\n\nQuestion: What is the name of the paper?\n\nAnswer: Multitask Learning for Sentiment Classification of Tweets\n\nQuestion: What is the name of the paper?\n\nAnswer: Multitask Learning for Sentiment Classification of Tweets\n\nQuestion: What is the name of the paper?\n\nAnswer: Multitask Learning for Sentiment Classification of Tweets\n\nQuestion: What is the name of the paper?\n\nAnswer:", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": "The adaptively sparse Transformer model is able to learn different sparsity patterns for each head, which allows for more confident predictions.\n\n Question: What is the difference between the adaptive sparse transformer and the fixed sparsity patterns transformer?\n\nAnswer: The adaptive sparse transformer is able to learn different sparsity patterns for each head, while the fixed sparsity patterns transformer has each head learn a different contiguous span of context tokens to attend to.\n\n Question: What is the difference between the adaptive sparse transformer and the adaptive span transformer?\n\nAnswer:", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "The baseline is the context-agnostic MT system.\n\nQuestion: what is the main novelty of this work?\n\nAnswer: The main novelty of this work is that our DocRepair model operates on groups of sentences and is thus able to fix consistency errors caused by the context-agnostic baseline MT system.\n\nQuestion: what is the main limitation of this work?\n\nAnswer: The main limitation of this work is that it requires monolingual document-level data.\n\nQuestion: what is the main limitation of this work?\n\nAnswer: The main limitation of", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": "The performance of our models are evaluated on two zero-shot cross-lingual transfer tasks: (1) Cross-lingual Natural Language Inference (XNLI) and (2) dependency parsing.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": "unanswerable\n\nQuestion: What is the role of the attention module?\n\nAnswer: unanswerable\n\nQuestion: What is the role of the attention module?\n\nAnswer: unanswerable\n\nQuestion: What is the role of the attention module?\n\nAnswer: unanswerable\n\nQuestion: What is the role of the attention module?\n\nAnswer: unanswerable\n\nQuestion: What is the role of the attention module?\n\nAnswer: unanswerable\n\nQuestion: What is the role of the attention module?\n\nAnswer: unanswerable\n\nQuestion: What", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": "unanswerable\n\nQuestion: What is the name of the dataset?\n\nAnswer: unanswerable\n\nQuestion: What is the name of the eye-tracking database?\n\nAnswer: unanswerable\n\nQuestion: What is the name of the sarcasm detection system?\n\nAnswer: unanswerable\n\nQuestion: What is the name of the feature set?\n\nAnswer: unanswerable\n\nQuestion: What is the name of the feature set?\n\nAnswer: unanswerable\n\nQuestion: What is the name of the feature set?\n\nAnswer: unanswerable", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "LSTM\n\nQuestion: What is the name of the shared task?\n\nAnswer: CoNLL–SIGMORPHON 2018\n\nQuestion: What is the name of the baseline system?\n\nAnswer: seq2seq\n\nQuestion: What is the name of the shared task?\n\nAnswer: CoNLL–SIGMORPHON 2018\n\nQuestion: What is the name of the baseline system?\n\nAnswer: seq2seq\n\nQuestion: What is the name of the shared task?\n\nAnswer: CoNLL–", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "yes\n\nQuestion: Is WordNet useful for taxonomic reasoning for this task?\n\nAnswer: yes\n\nQuestion: Is WordNet useful for taxonomic reasoning for this task?\n\nAnswer: yes\n\nQuestion: Is WordNet useful for taxonomic reasoning for this task?\n\nAnswer: yes\n\nQuestion: Is WordNet useful for taxonomic reasoning for this task?\n\nAnswer: yes\n\nQuestion: Is WordNet useful for taxonomic reasoning for this task?\n\nAnswer: yes\n\nQuestion: Is WordNet useful for taxonomic reasoning for this task?", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": "wav2letter\n\nQuestion: what was the architecture?\n\nAnswer: Jasper\n\nQuestion: what was the model?\n\nAnswer: Jasper\n\nQuestion: what was the model?\n\nAnswer: Jasper\n\nQuestion: what was the model?\n\nAnswer: Jasper\n\nQuestion: what was the model?\n\nAnswer: Jasper\n\nQuestion: what was the model?\n\nAnswer: Jasper\n\nQuestion: what was the model?\n\nAnswer: Jasper\n\nQuestion: what was the model?\n\nAnswer: Jasper\n\nQuestion:", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "unanswerable\n\nQuestion: What is the name of the paper?\n\nAnswer: unanswerable\n\nQuestion: What is the name of the paper?\n\nAnswer: unanswerable\n\nQuestion: What is the name of the paper?\n\nAnswer: unanswerable\n\nQuestion: What is the name of the paper?\n\nAnswer: unanswerable\n\nQuestion: What is the name of the paper?\n\nAnswer: unanswerable\n\nQuestion: What is the name of the paper?\n\nAnswer: unanswerable\n\nQuestion: What is the name of the paper", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "We evaluate our models on BPE perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, and recipe-level coherence.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset consists of 180K recipes and 700K user reviews.\n\nQuestion: What is the size of the vocabulary?\n\nAnswer: The vocabulary consists of 15K tokens.\n\nQuestion: What is the size of the user profile?\n\nAnswer: The user", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": "unanswerable\n\nQuestion: What is the purpose of the simulated data?\n\nAnswer: to bootstrap a prototype\n\nQuestion: What is the purpose of the simulated data?\n\nAnswer: to bootstrap a prototype\n\nQuestion: What is the purpose of the simulated data?\n\nAnswer: to bootstrap a prototype\n\nQuestion: What is the purpose of the simulated data?\n\nAnswer: to bootstrap a prototype\n\nQuestion: What is the purpose of the simulated data?\n\nAnswer: to bootstrap a prototype\n\nQuestion: What is the purpose of the simulated data?\n", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": "1000\n\nQuestion: What is the difference between the difficulty score and the inter-annotator agreement?\n\nAnswer: The difficulty score is calculated by the number of times a sentence is annotated by crowd workers, while the inter-annotator agreement is calculated by the number of times a sentence is annotated by experts.\n\nQuestion: What is the difference between the difficulty score and the inter-annotator agreement?\n\nAnswer: The difficulty score is calculated by the number of times a sentence is annotated by crowd workers, while the inter-annotator agreement is calculated by the number of times", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": "The Transformer architecture BIBREF0 for deep neural networks has quickly risen to prominence in NLP through its efficiency and performance, leading to improvements in the state of the art of Neural Machine Translation BIBREF1, BIBREF2, as well as inspiring other powerful general-purpose models like BERT BIBREF3 and GPT-2 BIBREF4. At the heart of the Transformer lie multi-head attention mechanisms: each word is represented by multiple different weighted averages of its relevant context. As suggested by recent works on interpreting attention head roles, separate attention", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": "The improvement in performance for Estonian in the NER task is significant.\n\nQuestion: What is the improvement in performance for Latvian in the NER task?\n\nAnswer: The improvement in performance for Latvian in the NER task is significant.\n\nQuestion: What is the improvement in performance for Slovenian in the NER task?\n\nAnswer: The improvement in performance for Slovenian in the NER task is significant.\n\nQuestion: What is the improvement in performance for Lithuanian in the NER task?\n\nAnswer: The improvement in performance for Lithuanian in the N", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": "They have a background in the humanities and social sciences.\n\nQuestion: What is the main goal of the research?\n\nAnswer: The main goal of the research is to study social and cultural phenomena using textual data.\n\nQuestion: What is the main goal of the research?\n\nAnswer: The main goal of the research is to study social and cultural phenomena using textual data.\n\nQuestion: What is the main goal of the research?\n\nAnswer: The main goal of the research is to study social and cultural phenomena using textual data.\n\nQuestion: What is the main", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "yes\n\nQuestion: What is the difference between the two datasets?\n\nAnswer: The Honeypot dataset is a public dataset, and the Weibo dataset is a self-built dataset.\n\nQuestion: What is the difference between the two datasets?\n\nAnswer: The Honeypot dataset is a public dataset, and the Weibo dataset is a self-built dataset.\n\nQuestion: What is the difference between the two datasets?\n\nAnswer: The Honeypot dataset is a public dataset, and the Weibo dataset is a self-built dataset.\n\nQuestion:", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": "zul, xho, nbl, ssw\n\nQuestion: Which languages are disjunctively written?\n\nAnswer: nso, sot, tsn\n\nQuestion: Which languages are conjunctively written?\n\nAnswer: nguni languages\n\nQuestion: Which languages are under resourced?\n\nAnswer: tshiVenda, Xitsonga, xho, nbl, ssw\n\nQuestion: Which languages are similar to each other?\n\nAnswer: nso, sot, tsn\n\nQuestion: Which languages are disjunctively written?\n", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "unanswerable\n\nQuestion: what is the difference between the two lstm models?\n\nAnswer: unanswerable\n\nQuestion: what is the difference between the two lstm models?\n\nAnswer: unanswerable\n\nQuestion: what is the difference between the two lstm models?\n\nAnswer: unanswerable\n\nQuestion: what is the difference between the two lstm models?\n\nAnswer: unanswerable\n\nQuestion: what is the difference between the two lstm models?\n\nAnswer: unanswerable\n\nQuestion: what is the difference between the two lstm models?", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "5000\n\nQuestion: What is the name of the dataset?\n\nAnswer: Wikipedia\n\nQuestion: What is the name of the dataset?\n\nAnswer: arXiv\n\nQuestion: What is the name of the dataset?\n\nAnswer: Wikipedia\n\nQuestion: What is the name of the dataset?\n\nAnswer: arXiv\n\nQuestion: What is the name of the dataset?\n\nAnswer: Wikipedia\n\nQuestion: What is the name of the dataset?\n\nAnswer: arXiv\n\nQuestion: What is the name of the dataset?\n\nAnswer", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "The intra-annotator values were computed for these metrics and the scores are shown in Table TABREF32 BIBREF12 , BIBREF13 .\n\nAnswer: unanswerable\n\nQuestion: What is the difference between the RNNSearch and RNNMorph models?\n\nAnswer: The RNNSearch model was a phrase based SMT model and the RNNMorph model was a neural machine translation model.\n\nAnswer: yes\n\nQuestion: What is the difference between the RNNSearch and RNNMorph models?\n\nAnswer: The RN", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "yes\n\nQuestion: What is the name of the work that is able to perform multi-task learning?\n\nAnswer: Luong2016\n\nQuestion: What is the name of the work that is able to perform multi-task learning?\n\nAnswer: Luong2016\n\nQuestion: What is the name of the work that is able to perform multi-task learning?\n\nAnswer: Luong2016\n\nQuestion: What is the name of the work that is able to perform multi-task learning?\n\nAnswer: Luong2016\n\nQuestion", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": "The accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence.\n\nQuestion: What is the main technical contribution of this paper?\n\nAnswer: Our main technical contribution is to draw inspiration from the multi-objective optimization literature and view the tradeoff as a sequence of constrained optimization problems, where we minimize the expected cost subject to varying expected reconstruction error constraints .\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: We propose a simple, unsupervised approach to an autocomplete system that is efficient,", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": "The evaluation metrics are the accuracy, precision, recall and F-measure.\n\nAnswer: unanswerable\n\nQuestion: What is the purpose of the sentence classification?\n\nAnswer: Sentence classification is used to identify strengths, weaknesses and suggestions for improvements found in the supervisor assessments.\n\nAnswer: unanswerable\n\nQuestion: What is the purpose of the sentence classification?\n\nAnswer: Sentence classification is used to identify strengths, weaknesses and suggestions for improvements found in the supervisor assessments.\n\nAnswer: unanswerable\n\nQuestion: What is the purpose", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": "The source domain is the domain from which the labeled data is drawn, and the target domain is the domain from which the unlabeled data is drawn.\n\nQuestion: What is the main intuition of the proposed method?\n\nAnswer: The proposed method is motivated by the idea that even though certain opinion words are completely distinct for each domain, they can be aligned if they have high correlation with some domain-invariant opinion words (pivot words) such as “excellent” or “terrible”.\n\nQuestion: What is the main limitation of the proposed method?\n\nAnswer: The proposed method", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": "LSTMs\n\nQuestion: what is the pyramidal transformation?\n\nAnswer: The pyramidal transformation is a linear transformation INLINEFORM0 defined as: DISPLAYFORM0\nwhere INLINEFORM0 are learned weights that linearly map INLINEFORM1 to INLINEFORM2 .\n\nQuestion: what is the grouped linear transformation?\n\nAnswer: Grouped linear transformations break the linear interactions by factoring the linear transformation into two steps. First, a GLT splits the input vector INLINEFORM3 into INLINEFORM4 smaller groups such that INLINEFORM5 . Second, a", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": "word/character embedding, CNN/LSTM and CRF.\n\nQuestion: What is the purpose of NeuronBlocks?\n\nAnswer: to provide a DNN toolkit for NLP tasks.\n\nQuestion: What is the main idea of NeuronBlocks?\n\nAnswer: to provide two layers of support to the engineers.\n\nQuestion: What is the main idea of NeuronBlocks?\n\nAnswer: to provide two layers of support to the engineers.\n\nQuestion: What is the purpose of NeuronBlocks?\n\nAnswer: to provide", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": "The corpus consists of spelling–pronunciation pairs extracted from Wiktionary.\n\n Question: what is the difference between the two models?\n\nAnswer: The difference is that LangID prepends each training, validation, and test sample with an artificial token identifying the language of the sample. NoLangID omits this token.\n\n Question: what is the difference between the two models?\n\nAnswer: The difference is that LangID prepends each training, validation, and test sample with an artificial token identifying the language of the sample. NoLangID omits this token.\n", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": "BIBREF12\n\nQuestion: What was the approach used?\n\nAnswer: We use the methodology by Khandelwal and Sawant (BIBREF12), and modify it to support experimentation with multiple models.\n\nQuestion: What was the methodology?\n\nAnswer: We use the methodology by Khandelwal and Sawant (BIBREF12), and modify it to support experimentation with multiple models.\n\nQuestion: What was the experimentation setup?\n\nAnswer: We use a default train-validation-test split of 70-1", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": "English\n\nQuestion: What is the main goal of their work?\n\nAnswer: To analyze the effect of both human and machine translation in cross-lingual models.\n\nQuestion: What is the main goal of their work?\n\nAnswer: To analyze the effect of both human and machine translation in cross-lingual models.\n\nQuestion: What is the main goal of their work?\n\nAnswer: To analyze the effect of both human and machine translation in cross-lingual models.\n\nQuestion: What is the main goal of their work?\n\nAnswer: To analyze the effect of both human and", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": "unanswerable\n\nQuestion: What is the size of the character lookup table?\n\nAnswer: unanswerable\n\nQuestion: What is the size of the word lookup table?\n\nAnswer: unanswerable\n\nQuestion: What is the size of the GRU hidden state?\n\nAnswer: unanswerable\n\nQuestion: What is the size of the embedding layer?\n\nAnswer: unanswerable\n\nQuestion: What is the size of the output layer?\n\nAnswer: unanswerable\n\nQuestion: What is the size of the input layer?\n\nAnswer: unanswerable\n", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "yes\n\nQuestion: Do they use a bidirectional GRU cell?\n\nAnswer: yes\n\nQuestion: Do they use a copying mechanism?\n\nAnswer: yes\n\nQuestion: Do they use a forget gate?\n\nAnswer: yes\n\nQuestion: Do they use a gated orthogonalization mechanism?\n\nAnswer: yes\n\nQuestion: Do they use a bifocal attention mechanism?\n\nAnswer: yes\n\nQuestion: Do they use a GRU cell to take contextual information from neighboring fields?\n\nAnswer: yes\n\nQuestion: Do they use a GRU cell", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "yes\n\nQuestion: What is the size of the dataset used to train PolyReponse?\n\nAnswer: yes\n\nQuestion: What is the size of the dataset used to train PolyReponse?\n\nAnswer: yes\n\nQuestion: What is the size of the dataset used to train PolyReponse?\n\nAnswer: yes\n\nQuestion: What is the size of the dataset used to train PolyReponse?\n\nAnswer: yes\n\nQuestion: What is the size of the dataset used to train PolyReponse?\n\nAnswer: yes\n\nQuestion: What is the", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": "The authors use the LIWC lexicon to generate maps of the U.S. that reflect psycholinguistic and semantic word classes.\n\nQuestion: How do they obtain linguistic dimensions of people?\n\nAnswer: The authors use the LIWC lexicon to generate maps of the U.S. that reflect linguistic dimensions of people.\n\nQuestion: How do they obtain demographic dimensions of people?\n\nAnswer: The authors use the LIWC lexicon to generate maps of the U.S. that reflect demographic dimensions of people.\n\nQuestion: How do they obtain geographical dimensions", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": "The ML methods aim to identify argument components in the discourse.\n\n Question: What is the main focus of the experiment?\n\nAnswer: The main focus of the experiment is to identify argument components in the discourse.\n\n Question: What is the main focus of the experiment?\n\nAnswer: The main focus of the experiment is to identify argument components in the discourse.\n\n Question: What is the main focus of the experiment?\n\nAnswer: The main focus of the experiment is to identify argument components in the discourse.\n\n Question: What is the main focus of the experiment?\n\nAnswer", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "n-grams of length INLINEFORM0\n\nQuestion: What is the difference between PARENT and BLEU?\n\nAnswer: PARENT is a task-specific metric, whereas BLEU is a general metric.\n\nQuestion: What is the difference between PARENT and BLEU-T?\n\nAnswer: PARENT is a task-specific metric, whereas BLEU-T is a general metric.\n\nQuestion: What is the difference between PARENT and BLEU-T?\n\nAnswer: PARENT is a task-specific metric, whereas", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": "14k tweets\n\nQuestion: What is the sentiment of the original posters?\n\nAnswer: positive\n\nQuestion: What is the sentiment of the replying comments?\n\nAnswer: positive\n\nQuestion: What is the sentiment of the final text provided by the original poster?\n\nAnswer: positive\n\nQuestion: What is the sentiment of the replying comments?\n\nAnswer: neutral\n\nQuestion: What is the sentiment of the final text provided by the original poster?\n\nAnswer: neutral\n\nQuestion: What is the sentiment of the replying comments?\n\nAnswer: neutral", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": "The languages covered are typologically diverse and include also under-resourced ones, such as Welsh and Kiswahili.\n\nQuestion: What is the main goal of the Multi-SimLex resource?\n\nAnswer: The main goal of the Multi-SimLex resource is to create a large-scale semantic resource for multilingual NLP research.\n\nQuestion: What is the main difference between Multi-SimLex and SimLex-999?\n\nAnswer: Multi-SimLex is a more comprehensive lexical semantic similarity dataset for the English language spanning", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": "Wikipedia conversations and Reddit CMV\n\nQuestion: What is the main insight of the model?\n\nAnswer: The model is a sequential neural model that can process comments as they happen and take the full conversational context into account to make an updated prediction at each step.\n\nQuestion: What is the main limitation of the model?\n\nAnswer: The model is limited by the scarcity of labeled data.\n\nQuestion: What is the main limitation of the model?\n\nAnswer: The model is limited by the scarcity of labeled data.\n\nQuestion: What is the main limitation", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "yes\n\nQuestion: What is the name of the ontology that was created?\n\nAnswer: criminal law\n\nQuestion: What is the name of the ontology that was created?\n\nAnswer: criminal law\n\nQuestion: What is the name of the ontology that was created?\n\nAnswer: criminal law\n\nQuestion: What is the name of the ontology that was created?\n\nAnswer: criminal law\n\nQuestion: What is the name of the ontology that was created?\n\nAnswer: criminal law\n\nQuestion: What is the name of the ontology that was created?\n", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": "We provide baselines using the official train-development-test split on the following tasks: automatic speech recognition (ASR), machine translation (MT) and speech translation (ST).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": "The audio-RNN encodes MFCC features from the audio signal using equation EQREF2 . The last hidden state of the audio-RNN is concatenated with the prosodic features to form the final vector representation INLINEFORM0 , and this vector is then passed through a fully connected neural network layer to form the audio encoding vector A. On the other hand, the text-RNN encodes the word sequence of the transcript using equation EQREF2 . The final hidden states of the text-RNN are also passed through another fully connected neural network layer to form a textual encoding vector T.", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": "2.11\n\nQuestion: What is the name of the model that was used for the experiments?\n\nAnswer: NMT\n\nQuestion: What is the name of the model that was used for the experiments?\n\nAnswer: NMT\n\nQuestion: What is the name of the model that was used for the experiments?\n\nAnswer: NMT\n\nQuestion: What is the name of the model that was used for the experiments?\n\nAnswer: NMT\n\nQuestion: What is the name of the model that was used for the experiments?\n\nAnswer: NMT\n\nQuestion", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": "1\n\nQuestion: what is the main limitation of the model?\n\nAnswer: the model is trained on monolingual data only\n\nQuestion: what is the main limitation of the model?\n\nAnswer: the model is trained on monolingual data only\n\nQuestion: what is the main limitation of the model?\n\nAnswer: the model is trained on monolingual data only\n\nQuestion: what is the main limitation of the model?\n\nAnswer: the model is trained on monolingual data only\n\nQuestion: what is the main limitation of the model?\n", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": "tweets that were retweeted more than 1000 times\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": "BERT\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": "The data collection project was mainly supported by Sharif DeepMine company.\n\nQuestion: what is the size of the dataset?\n\nAnswer: The main goal of the DeepMine project was to collect speech from at least a few thousand speakers, enabling research and development of deep learning methods.\n\nQuestion: what is the purpose of the dataset?\n\nAnswer: The main goal of the DeepMine project was to collect speech from at least a few thousand speakers, enabling research and development of deep learning methods.\n\nQuestion: what is the license of the dataset?\n\nAnswer:", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": "The DL model with GloVe embeddings achieved better results on three datasets, with 82.80% Accuracy on SNLI, 78.52% Accuracy on MultiNLI, and 83.62% Accuracy on Quora. Logistic Regression achieved the best Accuracy of 98.60% on Clinical-RQE. We also performed a 10-fold cross-validation on the full Clinical-QE data of 8,588 question pairs, which gave 98.", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "The benchmark dataset is the Social Honeypot dataset. It is a dataset collected by Lee et al. lee2011seven. It contains 2218 legitimate users and 2947 spammers.\n\nQuestion: What is the difference between the red bars and green bars?\n\nAnswer: The red bars represent spammers and the green bars represent legitimate users.\n\nQuestion: What is the difference between the red bars and green bars?\n\nAnswer: The red bars represent spammers and the green bars represent legitimate users.\n\nQuestion: What", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": "LSTM\n\nQuestion: What is the name of the shared task?\n\nAnswer: CoNLL–SIGMORPHON 2018\n\nQuestion: What is the name of the baseline system?\n\nAnswer: seq2seq\n\nQuestion: What is the name of the shared task?\n\nAnswer: CoNLL–SIGMORPHON 2018\n\nQuestion: What is the name of the baseline system?\n\nAnswer: seq2seq\n\nQuestion: What is the name of the shared task?\n\nAnswer: CoNLL–", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "unanswerable\n\nQuestion: Do they report results only on English data?\n\nAnswer: unanswerable\n\nQuestion: Do they report results only on English data?\n\nAnswer: unanswerable\n\nQuestion: Do they report results only on English data?\n\nAnswer: unanswerable\n\nQuestion: Do they report results only on English data?\n\nAnswer: unanswerable\n\nQuestion: Do they report results only on English data?\n\nAnswer: unanswerable\n\nQuestion: Do they report results only on English data?\n\nAnswer: unanswerable\n\nQuestion: Do", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": "The best performing model is the ensemble+ of (r4, r7 r12) on dev (internal) set.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "The baseline was a uni-directional PBSMT model.\n\nQuestion: what was the best model?\n\nAnswer: The best model was a multilingual multistage fine-tuning model.\n\nQuestion: what was the best model for Ja INLINEFORM0 Ru?\n\nAnswer: The best model for Ja INLINEFORM0 Ru was a multilingual multistage fine-tuning model.\n\nQuestion: what was the best model for Ja INLINEFORM1 En?\n\nAnswer: The best model for Ja INLINEFORM1 En was a multilingual mult", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "0.7033\n\nQuestion: What was their highest recall score?\n\nAnswer: 0.7033\n\nQuestion: What was their highest recall score?\n\nAnswer: 0.7033\n\nQuestion: What was their highest recall score?\n\nAnswer: 0.7033\n\nQuestion: What was their highest recall score?\n\nAnswer: 0.7033\n\nQuestion: What was their highest recall score?\n\nAnswer: 0.7033\n\nQuestion: What was their highest recall score?", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "unanswerable\n\nQuestion: What is the goal of the second order co-occurrence vectors?\n\nAnswer: unanswerable\n\nQuestion: What is the difference between the UMNSRS and MiniMayoSRS datasets?\n\nAnswer: unanswerable\n\nQuestion: What is the difference between the UMNSRS and MiniMayoSRS datasets?\n\nAnswer: unanswerable\n\nQuestion: What is the difference between the UMNSRS and MiniMayoSRS datasets?\n\nAnswer: unanswerable\n\nQuestion: What is the difference between the", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "The CFILT-preorder system for reordering English sentences to match the Indian language word order.\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": "yes\n\nQuestion: Does the paper explore extraction from biological literature?\n\nAnswer: yes\n\nQuestion: Does the paper explore extraction from biomedical literature?\n\nAnswer: yes\n\nQuestion: Does the paper explore extraction from medical literature?\n\nAnswer: yes\n\nQuestion: Does the paper explore extraction from biomedical literature?\n\nAnswer: yes\n\nQuestion: Does the paper explore extraction from biomedical literature?\n\nAnswer: yes\n\nQuestion: Does the paper explore extraction from biomedical literature?\n\nAnswer: yes\n\nQuestion:", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": "The experts were legal experts with training in privacy law.\n\nQuestion: What is the distribution of questions over OPP-115 categories?\n\nAnswer: The distribution of questions over OPP-115 categories is as shown in Table.TABREF16. First party and third party related questions are the largest categories, forming nearly 66.4% of all questions asked to the privacy assistant.\n\nQuestion: What is the distribution of questions over OPP-115 categories?\n\nAnswer: The distribution of questions over OPP-115 categories is as", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "seq2seq with parallel text corpus\n\nQuestion: What is the main idea of the paper?\n\nAnswer: The paper proposes a method for generating Shakespearean prose for a given painting.\n\nQuestion: What is the main idea of the paper?\n\nAnswer: The paper proposes a method for generating Shakespearean prose for a given painting.\n\nQuestion: What is the main idea of the paper?\n\nAnswer: The paper proposes a method for generating Shakespearean prose for a given painting.\n\nQuestion: What is the main idea of the paper?\n\nAnswer: The", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": "transformer layer\n\nQuestion: What is the main limitation of BERT?\n\nAnswer: BERT is limited to a particular input length\n\nQuestion: What is the main limitation of BERT?\n\nAnswer: BERT is limited to a particular input length\n\nQuestion: What is the main limitation of BERT?\n\nAnswer: BERT is limited to a particular input length\n\nQuestion: What is the main limitation of BERT?\n\nAnswer: BERT is limited to a particular input length\n\nQuestion: What is the main limitation of BERT?\n\nAnswer: BERT is limited", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": "yes\n\nQuestion: Do the authors hypothesize that humans' robustness to noise is due to their general knowledge?\n\nAnswer: yes\n\nQuestion: Do the authors hypothesize that humans' robustness to noise is due to their general knowledge?\n\nAnswer: yes\n\nQuestion: Do the authors hypothesize that humans' robustness to noise is due to their general knowledge?\n\nAnswer: yes\n\nQuestion: Do the authors hypothesize that humans' robustness to noise is due to their general knowledge?\n\nAnswer: yes\n\nQuestion: Do the authors hypothesize that", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "Twitter dataset contains examples of racism and sexism. Wikipedia dataset contains examples of personal attack. However, Formspring dataset is not specifically about any single topic.\n\nQuestion: What is the size of a post in terms of the number of words in the post?\n\nAnswer: We truncate such large posts to the size of post ranked at 95 percentile in that dataset. For example, in Wikipedia dataset, the largest post has 2846 words. However, size of post ranked at 95 percentile in that dataset is only 231.\n\nQuestion: What is the size", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "The contexts are split into three disjoint regions based on the two relation arguments: the left context, the middle context and the right context.\n\nQuestion: What is the difference between the CNN and RNN models?\n\nAnswer: CNNs perform a discrete convolution on an input matrix with a set of different filters. For NLP tasks, the input matrix represents a sentence: Each column of the matrix stores the word embedding of the corresponding word. By applying a filter with a width of, e.g., three columns, three neighboring words (trigram) are convolved. Afterwards, the results of", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "3\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": "3.5\n\nQuestion: What is the difference in performance between the model trained on the random subset and the model trained on the difficult subset?\n\nAnswer: 1.5\n\nQuestion: What is the difference in performance between the model trained on the random subset and the model trained on the difficult subset?\n\nAnswer: 1.5\n\nQuestion: What is the difference in performance between the model trained on the random subset and the model trained on the difficult subset?\n\nAnswer: 1.5\n\nQuestion: What is the difference in performance between the model trained on the random subset and", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": "65% of the speakers are men, speaking more than 75% of the time.\n\nQuestion: What is the impact of the observed disparity on ASR performance?\n\nAnswer: A WER increase of 24% for women compared to men, exhibiting a clear gender bias.\n\nQuestion: Is this as simple as a problem of gender proportion in the training data or are other factors entangled?\n\nAnswer: Gender bias varies across speaker's role and speech spontaneity level. Performance for Punctual speakers respectively spontaneous speech seems to", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "English-German\n\nQuestion: What is the main metric used to evaluate the models?\n\nAnswer: Meteor\n\nQuestion: What is the main metric used to evaluate the models?\n\nAnswer: Meteor\n\nQuestion: What is the main metric used to evaluate the models?\n\nAnswer: Meteor\n\nQuestion: What is the main metric used to evaluate the models?\n\nAnswer: Meteor\n\nQuestion: What is the main metric used to evaluate the models?\n\nAnswer: Meteor\n\nQuestion: What is the main metric used to evaluate the models?\n\nAnswer: Meteor\n\nQuestion:", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": "BIBREF24\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: We propose a CWS model with only attention structure. The encoder and decoder are both based on attention structure.\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: We propose a CWS model with only attention structure. The encoder and decoder are both based on attention structure.\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: We propose a CWS model with only attention structure. The encoder and decoder are both based on attention structure.", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "Logistic Regression (LR) and Multilayer Perceptron (MLP)\n\nQuestion: What is the main goal of the paper?\n\nAnswer: To demonstrate the effectiveness of our approach as a new model training technique\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: Our approach improves LR by 5.17% (Accuracy) and 18.38% (AUC), and MLP by 10.71% (Accuracy) and 30.27% (AUC) on average.\n", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "NLTK, Stanford CoreNLP, TwitterNLP, TensiStrength, SentiStrength, CogComp-NLP, Stanford NER, and spaCy.\n\nQuestion: What is the average CCR of the crowdworkers?\n\nAnswer: 98.6%\n\nQuestion: What is the average CCR of the automated systems?\n\nAnswer: 77.2% to 96.7%\n\nQuestion: What is the average CCR of the crowdworkers for NER?\n\nAnswer: 98.", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": "The experiments are performed on the SQuAD dataset.\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: The main contribution of the paper is to propose a question generation system which combines unstructured sentences and structured answer-relevant relations for generation. The unstructured sentences maintain the informativeness of generated questions while structured answer-relevant relations keep the faithfulness of questions. Extensive experiments demonstrate that our proposed model achieves state-of-the-art performance across several metrics. Furthermore, our model can generate diverse questions with different structured answer-relevant relations.\n", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "BIBREF31 , BIBREF32 , BIBREF33 , BIBREF34 , BIBREF35 , BIBREF36 , BIBREF37 , BIBREF38 , BIBREF39 , BIBREF40 , BIBREF41 , BIBREF42 , BIBREF43 , BIBREF44 , BIBREF45 , BIBREF46 , BIBREF47 , BIBREF48 , BIBREF49 , BIBREF50 , BIBREF51 , BIB", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": "yes\n\nQuestion: Do they use a pre-trained language model?\n\nAnswer: yes\n\nQuestion: Do they use a pre-trained language model?\n\nAnswer: yes\n\nQuestion: Do they use a pre-trained language model?\n\nAnswer: yes\n\nQuestion: Do they use a pre-trained language model?\n\nAnswer: yes\n\nQuestion: Do they use a pre-trained language model?\n\nAnswer: yes\n\nQuestion: Do they use a pre-trained language model?\n\nAnswer: yes\n\nQuestion: Do they use", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": "CSAT, 20newsgroups and Fisher.\n\nQuestion: What is the main limitation of BERT?\n\nAnswer: BERT is limited to a particular input length.\n\nQuestion: What is the main limitation of RoBERT?\n\nAnswer: BERT is limited to a particular input length.\n\nQuestion: What is the main limitation of ToBERT?\n\nAnswer: BERT is limited to a particular input length.\n\nQuestion: What is the main limitation of Transformer?\n\nAnswer: BERT is limited to a particular input length.\n\nQuestion: What is", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": "IMDb movie review dataset\n\nQuestion: What is the name of the dataset used for language modeling?\n\nAnswer: Penn Treebank\n\nQuestion: What is the name of the dataset used for character-based neural machine translation?\n\nAnswer: IWSLT German–English spoken-domain translation\n\nQuestion: What is the name of the model architecture used for the character-based neural machine translation?\n\nAnswer: sequence-to-sequence QRNN\n\nQuestion: What is the name of the model architecture used for the language modeling?\n\nAnswer: gated QRNN\n\n", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": "yes\n\nQuestion: What is the name of the model that was used in the study?\n\nAnswer: BERT\n\nQuestion: What is the name of the model that was used in the study?\n\nAnswer: BERT\n\nQuestion: What is the name of the model that was used in the study?\n\nAnswer: BERT\n\nQuestion: What is the name of the model that was used in the study?\n\nAnswer: BERT\n\nQuestion: What is the name of the model that was used in the study?\n\nAnswer: BERT\n\nQuestion: What is the", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": "yes\n\nQuestion: What is the name of the toolkit that was used for the named-entity recognition experiment?\n\nAnswer: TwitterNLP\n\nQuestion: What is the name of the toolkit that was used for the sentiment analysis experiment?\n\nAnswer: TwitterNLP\n\nQuestion: What is the name of the toolkit that was used for the named-entity recognition experiment?\n\nAnswer: TwitterNLP\n\nQuestion: What is the name of the toolkit that was used for the sentiment analysis experiment?\n\nAnswer: TwitterNLP\n\nQuestion: What is the name of the tool", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": "The invertibility condition is that the Jacobian determinant of the neural projector is equal to one.\n\nQuestion: What is the Jacobian determinant?\n\nAnswer: The Jacobian determinant is the determinant of the Jacobian matrix of the neural projector.\n\nQuestion: What is the Jacobian determinant of the neural projector?\n\nAnswer: The Jacobian determinant of the neural projector is equal to one.\n\nQuestion: What is the Jacobian determinant of the neural projector?\n\nAnswer: The Jacobian determinant of the neural projector is equal to one", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "The proposed qualitative annotation schema is shown in Figure FIGREF10.\n\nQuestion: What is the qualitative analysis of the proposed qualitative annotation schema?\n\nAnswer: The qualitative analysis of the proposed qualitative annotation schema is shown in Figure FIGREF23.\n\nQuestion: What is the quantitative analysis of the proposed qualitative annotation schema?\n\nAnswer: The quantitative analysis of the proposed qualitative annotation schema is shown in Table TABREF41.\n\nQuestion: What is the qualitative analysis of the proposed qualitative annotation schema?\n\nAnswer: The qualitative analysis", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": "600K sentences with 11.6M words\n\nQuestion: what is the size of the vocabulary?\n\nAnswer: 82K\n\nQuestion: what is the size of the training set?\n\nAnswer: 89,042 sentence pairs\n\nQuestion: what is the size of the test set?\n\nAnswer: 100 pairs\n\nQuestion: what is the size of the training set?\n\nAnswer: 296,402 sentence pairs\n\nQuestion: what is the size of the test set?\n\nAnswer", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": "The baselines are the vanilla ST baseline, pre-training baselines, multi-task baselines, and many-to-many+pre-training baseline.\n\nQuestion: What is the proposed method?\n\nAnswer: The proposed method is a tandem connectionist encoder network (TCEN), which is able to reuse every sub-net and keep the role of sub-net consistent between pre-training and fine-tuning.\n\nQuestion: What is the experimental results?\n\nAnswer: The experimental results show that our method significantly outperforms the strong `many-to-many+", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": "English\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: We show that common (`easy') methods of data augmentation for dealing with class imbalance do not improve base BERT performance.\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: We provide a statistical method of establishing the similarity of datasets.\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: We incorporate cost-sensitivity into BERT to enable models to adapt to dissimilar datasets.\n\nQuestion: What is the main contribution of this paper?\n\n", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": "The models are compared against baselines of predicting all labels as the majority or minority classes.\n\nQuestion: What is the performance of the SVM model?\n\nAnswer: The performance of all systems for the OTH class is 0.\n\nQuestion: What is the performance of the CNN model?\n\nAnswer: The CNN system achieved higher performance in this experiment compared to the BiLSTM, with a macro-F1 score of 0.69.\n\nQuestion: What is the performance of the RNN model?\n\nAnswer: The CNN outperforms the RNN model,", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "yes\n\nQuestion: What is the difference between a question and an answer?\n\nAnswer: unanswerable\n\nQuestion: What is the difference between a question and an answer?\n\nAnswer: unanswerable\n\nQuestion: What is the difference between a question and an answer?\n\nAnswer: unanswerable\n\nQuestion: What is the difference between a question and an answer?\n\nAnswer: unanswerable\n\nQuestion: What is the difference between a question and an answer?\n\nAnswer: unanswerable\n\nQuestion: What is the difference between a question and an answer?\n\n", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "Edinburgh embeddings BIBREF14\n\nQuestion: what is the name of the system that was used to predict the emotion intensity?\n\nAnswer: EmoInt\n\nQuestion: what is the name of the system that was used to predict the emotion intensity?\n\nAnswer: EmoInt\n\nQuestion: what is the name of the system that was used to predict the emotion intensity?\n\nAnswer: EmoInt\n\nQuestion: what is the name of the system that was used to predict the emotion intensity?\n\nAnswer: EmoInt\n\nQuestion: what is the", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": "The personalized models outperform the baseline in BPE perplexity (tab:metricsontest) with Prior Name performing the best. While our models exhibit comparable performance to baseline in BLEU-1/4 and ROUGE-L, we generate more diverse (Distinct-1/2: percentage of distinct unigrams and bigrams) and acceptable recipes. BLEU and ROUGE are not the most appropriate metrics for generation quality. A `correct' recipe can be written in many ways with the same main entities (ingredients). As BLEU-", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": "The combination of rewards for reinforcement learning is the harmonic mean of irony reward and sentiment reward.\n\nQuestion: What is the main reason for the issue that the DualRL system tends to make few changes to the input sentence and output the same sentence?\n\nAnswer: The main reason for the issue that the DualRL system tends to make few changes to the input sentence and output the same sentence is that rewards for content preservation are too prominent and rewards for style accuracy cannot work well.\n\nQuestion: What is the main reason for the issue that the DualRL system tends to make", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": "unanswerable\n\nQuestion: What is the main goal of the model?\n\nAnswer: to have a neural network describe the painting artistically in a style of choice\n\nQuestion: What is the main goal of the model?\n\nAnswer: to have a neural network describe the painting artistically in a style of choice\n\nQuestion: What is the main goal of the model?\n\nAnswer: to have a neural network describe the painting artistically in a style of choice\n\nQuestion: What is the main goal of the model?\n\nAnswer: to have a neural network describe the painting artistically in a", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": "The Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset.\n\nQuestion: What is the name of the dataset that was used to develop the model?\n\nAnswer: Affective Text dataset\n\nQuestion: What is the name of the model that was used to develop the model?\n\nAnswer: B-M\n\nQuestion: What is the name of the model that was used to develop the model?\n\nAnswer: B-M\n\nQuestion: What is the name of the model that was used to develop the model?\n\nAnswer: B-M\n", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": "The distribution of friends/followers of accounts spreading fake news is different from those of accounts spreading viral tweets without fake news.\n\nAnswer: unanswerable\n\nQuestion: What was the conclusion of the study?\n\nAnswer: The only way to automatically identify those deceitful tweets (i.e. containing fake news) is by actually understanding and modelling them.\n\nAnswer: unanswerable\n\nQuestion: What was the conclusion of the study?\n\nAnswer: The only way to automatically identify those deceitful tweets (i.e. containing fake news) is by", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": "The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset BIBREF36 .\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset of hashtags consists of 12,594 unique hashtags and their associated tweets.\n\nQuestion: What is the size of the training dataset?\n\nAnswer: The training dataset consists of 2,518 manually segmented hashtags.\n\nQuestion: What is the size of the test dataset?\n\nAnswer: The test dataset consists of 12,2", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": "The corpus contains Persian accents from different regions of Iran.\n\nQuestion: what is the size of the corpus?\n\nAnswer: The corpus contains 1969 speakers, 1149 of them being male and 820 female.\n\nQuestion: what is the size of the corpus?\n\nAnswer: The corpus contains 1969 speakers, 1149 of them being male and 820 female.\n\nQuestion: what is the size of the corpus?\n\nAnswer: The corpus contains 19", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "word subspace can represent the semantic structure of words\n\nQuestion: What is the main goal of the word subspace?\n\nAnswer: The main goal of the word subspace is to compactly represent the context of the corresponding text.\n\nQuestion: What is the main goal of the TF weighted word subspace?\n\nAnswer: The main goal of the TF weighted word subspace is to compactly represent the context of the corresponding text.\n\nQuestion: What is the main goal of the mutual subspace method?\n\nAnswer: The main goal of the mutual subspace method is", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 .\n\nQuestion: What is the number of news references in Wikipedia?\n\nAnswer: INLINEFORM0\n\nQuestion: What is the number of news references in Wikipedia?\n\nAnswer: INLINEFORM0\n\nQuestion: What is the number of news references in Wikipedia?\n\nAnswer: INLINEFORM0\n\nQuestion: What is the number of news references in Wikipedia?\n\nAnswer: INLINEFORM0\n\nQuestion: What is the number of news references", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": "yes\n\nQuestion: What is the main difference between the GlossBERT and the GlossBERT(Sent-CLS) model?\n\nAnswer: Sent-CLS model does not highlight the target word in the sentence.\n\nQuestion: What is the main difference between the GlossBERT and the GlossBERT(Sent-CLS-WS) model?\n\nAnswer: Sent-CLS-WS model weakly highlights the target word in the sentence.\n\nQuestion: What is the main difference between the GlossBERT and the GlossBERT(Token-", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": "960 hours\n\nQuestion: What is the largest corpus among existing public ST corpora?\n\nAnswer: 708 hours\n\nQuestion: What is the largest corpus among existing public ST corpora?\n\nAnswer: 708 hours\n\nQuestion: What is the largest corpus among existing public ST corpora?\n\nAnswer: 708 hours\n\nQuestion: What is the largest corpus among existing public ST corpora?\n\nAnswer: 708 hours\n\nQuestion: What is the largest corpus among existing public ST corpora?\n\n", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": "The dataset for fine-grained classification is split in training, development, development_test and test parts.\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: We show how jointly learning the ternary and fine-grained sentiment classification problems in a multitask setting improves the state-of-the-art performance, and (ii) we demonstrate that recurrent neural networks outperform models previously proposed without access to huge corpora while being flexible to incorporate different sources of data.\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: We show how", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": "large\n\nQuestion: Do they use large or small BERT?\n\nAnswer: large\n\nQuestion: Do they use large or small BERT?\n\nAnswer: large\n\nQuestion: Do they use large or small BERT?\n\nAnswer: large\n\nQuestion: Do they use large or small BERT?\n\nAnswer: large\n\nQuestion: Do they use large or small BERT?\n\nAnswer: large\n\nQuestion: Do they use large or small BERT?\n\nAnswer: large\n\nQuestion: Do they use large or small BERT?\n\nAnswer: large\n", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": "Yes\n\nQuestion: What is the main motivation for using WordNet?\n\nAnswer: The availability of glosses and example sentences allows us to construct natural language questions that contextualize the types of concepts we want to probe.\n\nQuestion: What is the main motivation for using GCIDE?\n\nAnswer: The availability of glosses and example sentences allows us to construct natural language questions that contextualize the types of concepts we want to probe.\n\nQuestion: What is the main motivation for using WordNet?\n\nAnswer: The availability of glosses and", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": "unanswerable\n\nQuestion: What is the main focus of the article?\n\nAnswer: The main focus of the article is to propose a set of principled evaluation criteria which evaluate image captioning models for grammaticality, truthfulness and diversity.\n\nQuestion: What is the main focus of the article?\n\nAnswer: The main focus of the article is to propose a set of principled evaluation criteria which evaluate image captioning models for grammaticality, truthfulness and diversity.\n\nQuestion: What is the main focus of the article?\n\nAnswer: The main", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": "The best model (B-M) on development data relies entirely on automatically obtained information, both in terms of training data as well as features.\n\nAnswer: unanswerable\n\nQuestion: What is the name of the dataset they used?\n\nAnswer: Affective Text dataset\n\nQuestion: What is the name of the dataset they used?\n\nAnswer: ISEAR\n\nQuestion: What is the name of the dataset they used?\n\nAnswer: Fairy Tales dataset\n\nQuestion: What is the name of the dataset they used?\n\nAnswer: Affective Text dataset\n", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": "INLINEFORM0 tag means the current word is not a pun.\n\nQuestion: What is the tagging scheme employed?\n\nAnswer: INLINEFORM0 tag means the current word is not a pun.\n\nQuestion: What is the tagging scheme employed?\n\nAnswer: INLINEFORM0 tag means the current word is not a pun.\n\nQuestion: What is the tagging scheme employed?\n\nAnswer: INLINEFORM0 tag means the current word is not a pun.\n\nQuestion: What is the tagging scheme employed?\n\nAnswer: INLINEFORM0 tag means the current", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": "yes\n\nQuestion: What is the largest corpus among existing public ST corpora?\n\nAnswer: 110 hours\n\nQuestion: What is the second largest corpus among existing public ST corpora?\n\nAnswer: 38 hours\n\nQuestion: What is the third largest corpus among existing public ST corpora?\n\nAnswer: 11 hours\n\nQuestion: What is the fourth largest corpus among existing public ST corpora?\n\nAnswer: 10 hours\n\nQuestion: What is the fifth largest corpus among existing public ST corpora?\n\nAnswer: ", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": "robustness of a model is defined as the ability of the model to handle the bias in the prior knowledge.\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: The main contribution of this paper is to investigate into the problem of how to leverage prior knowledge robustly in learning models.\n\nQuestion: What is the main problem addressed in this paper?\n\nAnswer: The main problem addressed in this paper is how to leverage prior knowledge robustly in learning models.\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: The main contribution of this paper is to investigate", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": "InferSent BIBREF4 and Universal Sentence Encoder BIBREF5.\n\nQuestion: What is the difference between the pooling strategies?\n\nAnswer: The pooling strategy is the operation that is applied to the output of the BERT network. The three strategies are: 1) using the output of the CLS-token, 2) computing the mean of all output vectors, and 3) computing a max-over-time of the output vectors.\n\nQuestion: What is the difference between the objective functions?\n\nAnswer: The objective function is the function that is", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "+0.29 and +0.96 respectively\n\nAnswer: unanswerable\n\nQuestion: What are method's improvements of F1 for MRC task for English and Chinese datasets?\n\nAnswer: +0.84 and +0.97 respectively\n\nAnswer: unanswerable\n\nQuestion: What are method's improvements of F1 for POS task for English and Chinese datasets?\n\nAnswer: +0.97 and +2.36 respectively\n\nAnswer: unanswerable\n\nQuestion: What are method's improvements of F1 for PI task", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": "The conflict method is tested on the task of duplicate question detection and ranking questions in Bing's People Also Ask.\n\nQuestion: What is the conflict model?\n\nAnswer: The conflict model is a model that computes the difference between two word representations and then applies a softmax function on it.\n\nQuestion: What is the attention model?\n\nAnswer: The attention model is a model that computes the similarity between two word representations and then applies a softmax function on it.\n\nQuestion: What is the difference between conflict and attention?\n\nAnswer: Conflict is the inverse of attention. It", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": "unanswerable\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: unanswerable\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: unanswerable\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: unanswerable\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: unanswerable\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: unanswerable\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: unanswerable\n\nQuestion: What", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": "relation detection\n\nQuestion: What is the main focus of this work?\n\nAnswer: KB relation detection\n\nQuestion: What is the main focus of this work?\n\nAnswer: KB relation detection\n\nQuestion: What is the main focus of this work?\n\nAnswer: KB relation detection\n\nQuestion: What is the main focus of this work?\n\nAnswer: KB relation detection\n\nQuestion: What is the main focus of this work?\n\nAnswer: KB relation detection\n\nQuestion: What is the main focus of this work?\n\nAnswer: KB relation detection", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "The baseline models are a simple encoder-decoder model with ingredient attention and a neural checklist model.\n\nQuestion: What is the dataset used in the paper?\n\nAnswer: The dataset is a novel dataset of 180K recipes and 700K user reviews from Food.com.\n\nQuestion: What is the task?\n\nAnswer: The task is to generate a recipe from a partial recipe specification and a user profile.\n\nQuestion: What is the evaluation metric?\n\nAnswer: The evaluation metric is a combination of human evaluation and automatic coher", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": "manual methods\n\nQuestion: What is the main conclusion of the paper?\n\nAnswer: The paper concludes that the Flickr30K dataset is biased.\n\nQuestion: What is the main conclusion of the paper?\n\nAnswer: The paper concludes that the Flickr30K dataset is biased.\n\nQuestion: What is the main conclusion of the paper?\n\nAnswer: The paper concludes that the Flickr30K dataset is biased.\n\nQuestion: What is the main conclusion of the paper?\n\nAnswer: The paper concludes that the F", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": "English\n\nQuestion: What is the name of the person who is the author of the article?\n\nAnswer: Levesque\n\nQuestion: What is the name of the person who is the author of the article?\n\nAnswer: Levesque\n\nQuestion: What is the name of the person who is the author of the article?\n\nAnswer: Levesque\n\nQuestion: What is the name of the person who is the author of the article?\n\nAnswer: Levesque\n\nQuestion: What is the name of the person who is the author of the article?\n\nAnswer:", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": "The models they experimented with were: (i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0 , (iii) models without INLINEFORM1 , and (iv) models that integrate lower contexts via peephole connections.\n\nAnswer: unanswerable\n\nQuestion: What is the difference between the proposed architecture and the stacked LSTM architecture?\n\nAnswer: The difference between the proposed architecture and the stacked LSTM architecture is that the proposed architecture uses cell states from the previous layer in computation.\n\nAnswer: yes\n\n", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": "unanswerable\n\nQuestion: Do they report results only on English data?\n\nAnswer: unanswerable\n\nQuestion: Do they report results only on English data?\n\nAnswer: unanswerable\n\nQuestion: Do they report results only on English data?\n\nAnswer: unanswerable\n\nQuestion: Do they report results only on English data?\n\nAnswer: unanswerable\n\nQuestion: Do they report results only on English data?\n\nAnswer: unanswerable\n\nQuestion: Do they report results only on English data?\n\nAnswer: unanswerable\n\nQuestion: Do", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": "The authors experimented with Sumy package's summarization algorithms.\n\nAnswer: unanswerable\n\nQuestion: What is the name of the algorithm that was used to classify sentences in emails into classes such as APOLOGY, INSTRUCTION, QUESTION, REQUEST, SALUTATION, STATEMENT, SUGGESTION, THANKING?\n\nAnswer: The algorithm is called SVM and it was used by Qadir and Riloff.\n\nAnswer: unanswerable\n\nQuestion: What is the name of the algorithm that was used to", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": "BIBREF0 .\n\nQuestion: What is the primary problem?\n\nAnswer: A thread INLINEFORM0 consists of a series of posts INLINEFORM1 through INLINEFORM2 where INLINEFORM3 is an instructor's post when INLINEFORM4 is intervened, if applicable. INLINEFORM5 is considered intervened if an instructor had posted at least once. The problem of predicting instructor intervention is cast as a binary classification problem. Intervened threads are predicted as 1 given while non-intervened threads are predicted as 0 given posts INLINEFORM6 through", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": "unanswerable\n\nQuestion: What is the most important component?\n\nAnswer: unanswerable\n\nQuestion: What is the most important component?\n\nAnswer: unanswerable\n\nQuestion: What is the most important component?\n\nAnswer: unanswerable\n\nQuestion: What is the most important component?\n\nAnswer: unanswerable\n\nQuestion: What is the most important component?\n\nAnswer: unanswerable\n\nQuestion: What is the most important component?\n\nAnswer: unanswerable\n\nQuestion: What is the most important component?\n\nAnswer: unanswer", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": "DTA18 and DTA19\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "Kannada, Hindi, Telugu, Malayalam, Bengali, and English\n\nQuestion: What is the GhostVLAD pooling approach?\n\nAnswer: GhostVLAD is an extension of the NetVLAD approach, which we discussed in the previous section. The GhostVLAD model was proposed for face recognition by Y. Zhong [10]. GhostVLAD works exactly similar to NetVLAD except it adds Ghost clusters along with the NetVLAD clusters. So, now we will have a K+G number of clusters instead of K clusters. Where", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": "The model performance on target language reading comprehension is 53.8.\n\n Question: What is the model performance on target language reading comprehension?\n\nAnswer: The model performance on target language reading comprehension is 53.8.\n\n Question: What is the model performance on target language reading comprehension?\n\nAnswer: The model performance on target language reading comprehension is 53.8.\n\n Question: What is the model performance on target language reading comprehension?\n\nAnswer: The model performance on target language reading comprehension is 53.8.\n\n Question:", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": "0.7815\n\nQuestion: What is the difference between the proposed model and the baselines?\n\nAnswer: 0.7815\n\nQuestion: What is the difference between the proposed model and the baselines?\n\nAnswer: 0.7815\n\nQuestion: What is the difference between the proposed model and the baselines?\n\nAnswer: 0.7815\n\nQuestion: What is the difference between the proposed model and the baselines?\n\nAnswer: 0.7815\n\nQuestion: What is the difference between", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "ARAML outperforms other GAN baselines with lower training variance, yet producing better performance on three text generation tasks.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": "The authors present evidence that the model can capture some biases in data annotation and collection by examining the results of the model on the test datasets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "Yes\n\nQuestion: What is the answerability identification baseline?\n\nAnswer: SVM: We define 3 sets of features to characterize each question. The first is a simple bag-of-words set of features over the question (SVM-BOW), the second is bag-of-words features of the question as well as length of the question in words (SVM-BOW + LEN), and lastly we extract bag-of-words features, length of the question in words as well as part-of-speech tags for the question (SVM-BOW + LEN", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "16225\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": "+0.58\n\nQuestion: What are method improvements of F1 for paraphrase identification?\n\nAnswer: +0.58\n\nQuestion: What are method improvements of F1 for paraphrase identification?\n\nAnswer: +0.58\n\nQuestion: What are method improvements of F1 for paraphrase identification?\n\nAnswer: +0.58\n\nQuestion: What are method improvements of F1 for paraphrase identification?\n\nAnswer: +0.58\n\nQuestion: What are method improvements of F1 for paraphr", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "The datasets used are the same as those used in BIBREF0 .\n\nQuestion: What is the relationship between the ERP components and the behavioral data?\n\nAnswer: The relationship between the ERP components and the behavioral data is that the ERP components are predictive of the behavioral data.\n\nQuestion: What is the relationship between the ERP components and the neural data?\n\nAnswer: The relationship between the ERP components and the neural data is that the ERP components are predictive of the neural data.\n\nQuestion: What is the relationship between the ERP components and the behavior", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": "EEG data\n\nQuestion: What is the main purpose of the study?\n\nAnswer: To decode intended speech or motor activity from brain signals\n\nQuestion: What is the main purpose of the study?\n\nAnswer: To decode intended speech or motor activity from brain signals\n\nQuestion: What is the main purpose of the study?\n\nAnswer: To decode intended speech or motor activity from brain signals\n\nQuestion: What is the main purpose of the study?\n\nAnswer: To decode intended speech or motor activity from brain signals\n\nQuestion: What is the main purpose of the study?\n\nAnswer:", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": "Pointer-Gen, Pointer-Gen-RL+ROUGE, Pointer-Gen-RL-SEN, Pointer-Gen-ARL-SEN\n\nQuestion: What is the sensationalism scorer?\n\nAnswer: A CNN model is trained to predict the sensationalism score of a headline.\n\nQuestion: What is the sensationalism scorer trained on?\n\nAnswer: Headlines with many comments from popular online websites as positive samples. For the negative samples, we propose to use the generated headlines from a sentence summarization model.\n\nQuestion: What is", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": "The neural network models are implemented with the following specifications: CNN: We adopt Kim's kim2014convolutional implementation as the baseline. The word-level CNN models have 3 convolutional filters of different sizes [1,2,3] with ReLU activation, and a max-pooling layer. For the character-level CNN, we use 6 convolutional filters of various sizes [3,4,5,6,7,8], then add max-pooling layers followed by 1 fully-connected layer with a dimension of 1024.\n\n\n", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": "We use the transformer implementation of the fairseq toolkit.\n\nQuestion: What is the language model vocabulary?\n\nAnswer: We learn a joint BPE vocabulary of 37K types on the German and English newscrawl and train the language models with this vocabulary.\n\nQuestion: What is the language model architecture?\n\nAnswer: We consider two types of architectures: a bi-directional language model to augment the sequence to sequence encoder and a uni-directional model to augment the decoder.\n\nQuestion: What is the language", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": "The proposed method uses a decaying factor $(1-p)$ to multiply the soft probability $p$ with.\n\nQuestion: What is the difference between the proposed method and focal loss?\n\nAnswer: The proposed method uses a decaying factor $(1-p)$ to multiply the soft probability $p$ with.\n\nQuestion: What is the difference between the proposed method and focal loss?\n\nAnswer: The proposed method uses a decaying factor $(1-p)$ to multiply the soft probability $p$ with.\n\nQuestion: What is the difference between the proposed method and focal loss?", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": "The knowledge graph based agents are able to pass the bottleneck at a score of 40, whereas the baseline A2C and KG-A2C are not.\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: The main contribution is the introduction of a method to detect bottlenecks in text-games and a method to leverage knowledge graphs to improve exploration in text-games.\n\nQuestion: What is the main limitation of this paper?\n\nAnswer: The main limitation is that the knowledge graph is not sufficient to solve the text-game", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": "a monolingual model\n\nQuestion: What is the main motivation for jointly modeling SRL in multiple languages?\n\nAnswer: to capture cross-lingual patterns\n\nQuestion: What is the main motivation for jointly modeling SRL in multiple languages?\n\nAnswer: to capture cross-lingual patterns\n\nQuestion: What is the main motivation for jointly modeling SRL in multiple languages?\n\nAnswer: to capture cross-lingual patterns\n\nQuestion: What is the main motivation for jointly modeling SRL in multiple languages?\n\nAnswer: to", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": "The transcriptions were annotated for non-standardized orthography.\n\nQuestion: What is the size of the corpus?\n\nAnswer: 142 hours\n\nQuestion: What is the size of the corpus?\n\nAnswer: 142 hours\n\nQuestion: What is the size of the corpus?\n\nAnswer: 142 hours\n\nQuestion: What is the size of the corpus?\n\nAnswer: 142 hours\n\nQuestion: What is the size of the corpus?\n\nAnswer: 142 hours\n\nQuestion", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": "A semicharacter architecture is a word recognition model that is trained to recognize words by looking at the first and last characters of each word.\n\nQuestion: What is the main idea of the paper?\n\nAnswer: The paper is about the vulnerability of character-level models to adversarial attacks.\n\nQuestion: What is the main idea of the paper?\n\nAnswer: The paper is about the vulnerability of character-level models to adversarial attacks.\n\nQuestion: What is the main idea of the paper?\n\nAnswer: The paper is about the vulnerability of character-level models to advers", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": "16\n\nQuestion: what is the main goal of the paper?\n\nAnswer: to compare the relative impact of external lexical information and word vector representations on the accuracy of PoS models\n\nQuestion: what is the main goal of the paper?\n\nAnswer: to compare the relative impact of external lexical information and word vector representations on the accuracy of PoS models\n\nQuestion: what is the main goal of the paper?\n\nAnswer: to compare the relative impact of external lexical information and word vector representations on the accuracy of PoS models\n\nQuestion: what is the main goal of the", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": "NCEL outperforms the state-of-the-art collective methods across five different datasets.\n\nQuestion: What is the main limitation of the global approach?\n\nAnswer: The main drawback of the global approach has been alleviated through approximate optimization techniques, such as PageRank/random walks, graph pruning, ranking SVMs, or loopy belief propagation (LBP).\n\nQuestion: What is the main limitation of the local approach?\n\nAnswer: The limitation of local models is that they are not able to fully utilize the power of NN models for", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": "yes\n\nQuestion: What is the average duration of the conversations?\n\nAnswer: 9min 28s\n\nQuestion: What is the average number of words in the transcripts?\n\nAnswer: 1500\n\nQuestion: What is the average number of words in the summaries?\n\nAnswer: 9.7\n\nQuestion: What is the average number of words in the MR tags?\n\nAnswer: 10.1\n\nQuestion: What is the average number of words in the summaries?\n\nAnswer: 9.7\n\nQuestion", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "unanswerable\n\nQuestion: What was the best performing model?\n\nAnswer: Ann+PAT+MT\n\nQuestion: What was the best performing model?\n\nAnswer: Ann+PAT+MT\n\nQuestion: What was the best performing model?\n\nAnswer: Ann+PAT+MT\n\nQuestion: What was the best performing model?\n\nAnswer: Ann+PAT+MT\n\nQuestion: What was the best performing model?\n\nAnswer: Ann+PAT+MT\n\nQuestion: What was the best performing model?\n\nAnswer: Ann+PAT+", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": "synthesized user queries\n\nQuestion: what is the difference between the 2010 i2b2/VA and 2013 ShARe/CLEF?\n\nAnswer: disjoint entities\n\nQuestion: what is the difference between the 2010 i2b2/VA and 2013 ShARe/CLEF?\n\nAnswer: disjoint entities\n\nQuestion: what is the difference between the 2010 i2b2/VA and 2013 ShARe/CLEF?\n\nAnswer:", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": "Masking words in the decoder is helpful because it provides a more complete input sequence which is consistent with their pre-training processes.\n\nAnswer: unanswerable\n\nQuestion: What is the main issue of the previous abstractive methods?\n\nAnswer: The main issue of the previous abstractive methods is that they use left-context-only decoder, thus do not have complete context when predicting each word.\n\nAnswer: yes\n\nQuestion: What is the main issue of the previous abstractive methods?\n\nAnswer: The main issue of the previous abstractive methods is that they use left-", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": "unanswerable\n\nQuestion: What is the motivation for the modeling inter-tweet relationships?\n\nAnswer: unanswerable\n\nQuestion: What is the motivation for the modeling as an autoencoder?\n\nAnswer: unanswerable\n\nQuestion: What is the motivation for the modeling using weak supervision?\n\nAnswer: unanswerable\n\nQuestion: What is the motivation for the modeling from structured resources?\n\nAnswer: unanswerable\n\nQuestion: What is the motivation for the modeling within-tweet relationships?\n\nAnswer:", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": "TF-IDF features\n\nQuestion: What is the primary objective of the study?\n\nAnswer: To analyze the efficacy of existing machine learning approaches for the automated classification of pathology reports into different diagnosis categories.\n\nQuestion: What is the distribution of reports across different primary diagnoses and primary sites?\n\nAnswer: tab:report-distribution\n\nQuestion: What is the distribution of reports across different primary diagnoses and primary sites?\n\nAnswer: tab:report-distribution\n\nQuestion: What is the distribution of reports across different primary diagnoses and primary sites?\n\nAnswer: tab", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": "The dataset contains 9,473 annotations for 9,300 tweets.\n\nQuestion: What is the contribution of lexical features?\n\nAnswer: significant drops in F1-scores resulted from sans lexical for depressed mood (-35 points), disturbed sleep (-43 points), and depressive symptoms (-45 points).\n\nQuestion: What is the contribution of syntactic features?\n\nAnswer: sans syntactic: 16,935\n\nQuestion: What is the contribution of emotion features?\n\nAnswer: sans", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": "The eight NER tasks are:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": "The training data was translated using the machine translation platform Apertium.\n\nQuestion: What is the name of the package that was used to create word embeddings?\n\nAnswer: AffectiveTweets WEKA package\n\nQuestion: What is the name of the package that was used to create word embeddings?\n\nAnswer: AffectiveTweets WEKA package\n\nQuestion: What is the name of the package that was used to create word embeddings?\n\nAnswer: AffectiveTweets WEKA package\n\nQuestion: What is", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": "unanswerable\n\nQuestion: What is the name of the paper that they used to build their classifier?\n\nAnswer: unanswerable\n\nQuestion: What is the name of the paper that they used to build their classifier?\n\nAnswer: unanswerable\n\nQuestion: What is the name of the paper that they used to build their classifier?\n\nAnswer: unanswerable\n\nQuestion: What is the name of the paper that they used to build their classifier?\n\nAnswer: unanswerable\n\nQuestion: What is the name of the paper that they used to build their", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "The baseline for this task is a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence.\n\nQuestion: What was the best performing team?\n\nAnswer: Team newspeak\n\nQuestion: What was the best performing team for the FLC task?\n\nAnswer: Team newspeak\n\nQuestion: What was the best performing team for the SLC task?\n\nAnswer: Team CAUnLP\n\nQuestion: What was the best performing team for the FLC task?\n\nAnswer: Team newspeak\n", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": "The first block of Table TABREF11 .\n\nQuestion: What is the INLINEFORM1 score?\n\nAnswer: The scores in the end.\n\nQuestion: What is the INLINEFORM2 scheme?\n\nAnswer: The INLINEFORM1 -th instance in the training set.\n\nQuestion: What is the INLINEFORM3 scheme?\n\nAnswer: The INLINEFORM1 -th instance in the training set.\n\nQuestion: What is the INLINEFORM4 scheme?\n\nAnswer: The INLINEFORM1 -th instance in the training set.\n\nQuestion:", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": "We assigned a political bias label to different US outlets (and therefore news articles) following the procedure described in BIBREF2.\n\nQuestion: What is the difference between the two news domains?\n\nAnswer: We can notice the exact same set of features (with different relative orderings in the Top-3) in both countries; these correspond to two global network propertie–LWCC, which indicates the size of the largest cascade in the layer, and SCC, which correlates with the size of the network–associated to the same set of layers (Quotes, Retweets and M", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": "ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era.\n\nQuestion: What is the length of the unaugmented Train set?\n\nAnswer: INLINEFORM0 aligned bilingual clause pairs from 28K aligned bilingual paragraphs.\n\nQuestion: What is the length of the augmented Train set?\n\nAnswer: INLINEFORM0 aligned bilingual clause pairs from 28K aligned bilingual paragraphs.\n\nQuestion: What is the length of the un", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "English\n\nQuestion: What is the main goal of the paper?\n\nAnswer: To create a new large publicly available dataset of English tweets.\n\nQuestion: What is the main goal of the paper?\n\nAnswer: To create a new large publicly available dataset of English tweets.\n\nQuestion: What is the main goal of the paper?\n\nAnswer: To create a new large publicly available dataset of English tweets.\n\nQuestion: What is the main goal of the paper?\n\nAnswer: To create a new large publicly available dataset of English tweets.\n\nQuestion", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": "The Chinese datasets used are the same as those used in BIBREF17 .\n\nAnswer: The Chinese datasets used are the same as those used in BIBREF17 .\n\nAnswer: The Chinese datasets used are the same as those used in BIBREF17 .\n\nAnswer: The Chinese datasets used are the same as those used in BIBREF17 .\n\nAnswer: The Chinese datasets used are the same as those used in BIBREF17 .\n\nAnswer: The Chinese datasets used are the same as those used in BIBREF17 .\n\nAnswer: The Chinese", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": "3\n\nQuestion: What is the main difference between the SVM and the CNN?\n\nAnswer: The SVM is a linear classifier, while the CNN is a non-linear classifier.\n\nQuestion: What is the main difference between the SVM and the RCNN?\n\nAnswer: The SVM is a linear classifier, while the RCNN is a non-linear classifier.\n\nQuestion: What is the main difference between the SVM and the RCNN?\n\nAnswer: The SVM is a linear classifier, while the RCNN is a non-linear classifier", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": "The dataset used in this paper is the same as in BIBREF7 .\n\nQuestion: what is the main hypothesis in this paper?\n\nAnswer: The main hypothesis in this paper is that by using vector space embeddings instead of bag-of-words representations, the ecological information which is implicitly captured by Flickr tags can be utilized in a more effective way.\n\nQuestion: what is the main motivation for using vector space embeddings in this paper?\n\nAnswer: The main motivation for using vector space embeddings in this paper is that they allow us to integrate the", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": "NUBes-PHI and MEDDOCAN.\n\nQuestion: What is the main conclusion of the paper?\n\nAnswer: BERT is a good candidate for sensitive data detection and classification in Spanish clinical text.\n\nQuestion: What is the main conclusion of the paper?\n\nAnswer: BERT is a good candidate for sensitive data detection and classification in Spanish clinical text.\n\nQuestion: What is the main conclusion of the paper?\n\nAnswer: BERT is a good candidate for sensitive data detection and classification in Spanish clinical text.\n\nQuestion: What is the main conclusion of the", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": "Unigram (with principal components of unigram feature vectors),\n\nQuestion: What is the name of the system that they used to detect sarcasm?\n\nAnswer: Sarcasm (the feature-set reported by joshi2015harnessing subsuming unigram features and features from other reported systems)\n\nQuestion: What is the name of the system that they used to detect sarcasm?\n\nAnswer: Sarcasm (the feature-set reported by joshi2015harnessing subsuming unigram features and features from other reported systems)", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "The metrics are the predictive performance and strategy formulation ability.\n\nQuestion: What is the main difference between the proposed approach and the existing KBC approaches? \n\nAnswer: The main difference is that the existing KBC approaches are closed-world and cannot handle unknown entities and relations.\n\nQuestion: What is the main difference between the proposed approach and the existing lifelong learning approaches? \n\nAnswer: The main difference is that the existing lifelong learning approaches are not designed for knowledge learning in conversations.\n\nQuestion: What is the main difference between the proposed approach and the existing interactive learning approaches", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "yes\n\nAnswer: unanswerable\n\nQuestion: Do they use a convolutional neural network architecture to create a sample of a QA Wikipedia dataset?\n\nAnswer: yes\n\nAnswer: yes\n\nQuestion: Do they use a convolutional neural network architecture to create a sample of a QA Wikipedia dataset?\n\nAnswer: yes\n\nAnswer: yes\n\nQuestion: Do they use a convolutional neural network architecture to create a sample of a QA Wikipedia dataset?\n\nAnswer: yes\n\nAnswer: yes\n\nQuestion: Do they use a convolutional neural network architecture to create a sample of", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": "Galatasaray and Fenerbahçe\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": "We also conduct automatic evaluations and the conclusions are similar to those of the transformation from non-ironic sentences to ironic sentences. As for human evaluation results in Table TABREF47 , our model still can achieve the second-best results in sentiment and content preservation. Nevertheless, DualRL system and ours get poor performances in irony accuracy. The reason may be that the other four baselines tend to generate common and even not fluent sentences which are irrelevant to the input sentences and are hard to be identified as ironies. So annotators usually mark these output sentences as non-ironic sentences, which", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": "Gaussian-masked directional multi-head attention is a variant of multi-head attention which is proposed in this paper. It is a kind of self-attention network which is proposed in BIBREF24. It is a kind of attention network which is proposed in BIBREF24. It is a kind of attention network which is proposed in BIBREF24.\n\nQuestion: What is the difference between the Transformer and the model proposed in this paper?\n\nAnswer: The Transformer is a kind of self-attention network which is proposed in BIBREF24. The Transformer", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "Facebook\n\nQuestion: What is the purpose of the causality prediction?\n\nAnswer: to determine whether a message contains causal relation\n\nQuestion: What is the purpose of the causal explanation identification?\n\nAnswer: to identify causal explanations given the discourse arguments of the causality message\n\nQuestion: What is the purpose of the causality prediction?\n\nAnswer: to determine whether a message contains causal relation\n\nQuestion: What is the purpose of the causal explanation identification?\n\nAnswer: to identify causal explanations given the discourse arguments of the causality message\n\nQuestion", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": "Sentiment, emotion and personality features.\n\nQuestion: What is the best feature combination for sarcasm detection?\n\nAnswer: Sentiment, emotion and personality features.\n\nQuestion: What is the best feature combination for sarcasm detection on Dataset 3?\n\nAnswer: Sentiment, emotion and personality features.\n\nQuestion: What is the best feature combination for sarcasm detection on Dataset 1?\n\nAnswer: Sentiment, emotion and personality features.\n\nQuestion: What is the best feature combination for sarcasm detection on Dataset", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": "INLINEFORM0\n\nQuestion: What is the name of the model that was used to solve the NER segmentation task?\n\nAnswer: learning to search\n\nQuestion: What is the name of the model that was used to solve the NER classification task?\n\nAnswer: learning to search\n\nQuestion: What is the name of the model that was used to solve the fine-grained sentiment classification task?\n\nAnswer: learning to search\n\nQuestion: What is the name of the model that was used to solve the fine-grained sentiment quantification task?\n\nAnswer: learning to search", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": "second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc)\n\nQuestion: What was the best model for each subtask?\n\nAnswer: the best individual model for each subtask (bolded scores in Table TABREF18 ) is always either a translated or semi-supervised model\n\nQuestion: What was the best model for each subtask?\n\nAnswer: the best individual model for each subtask (bolded scores in Table TABREF18 ) is always either a translated or semi-supervised model\n", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": "53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average.\n\nQuestion: What is the average number of tokens per entity?\n\nAnswer: 19.55\n\nQuestion: What is the average number of tokens per entity?\n\nAnswer: 19.55\n\nQuestion: What is the average number of tokens per entity?\n\nAnswer: 19.55\n\nQuestion: What is the average number of tokens per entity?\n\nAnswer: 19.5", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": "yes\n\nQuestion: What is the best way to generate cloze questions?\n\nAnswer: We use a standard NLP pipeline based on Stanford CoreNLP (for SQuAD, TrivaQA and PubMed) and the BANNER Named Entity Recognizer (only for PubMed articles) to identify entities and phrases.\n\nQuestion: What is the best way to generate cloze questions?\n\nAnswer: We use a standard NLP pipeline based on Stanford CoreNLP (for SQuAD, TrivaQA and PubMed) and the BANNER N", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": "Sentiment classification\n\nQuestion: What is the problem they are trying to solve?\n\nAnswer: The problem is how to leverage prior knowledge robustly in learning models.\n\nQuestion: What is the proposed method?\n\nAnswer: They propose three regularization terms on top of generalized expectation criteria.\n\nQuestion: What is the experimental result?\n\nAnswer: The performance can be considerably improved when taking into account these factors.\n\nQuestion: What is the conclusion?\n\nAnswer: This paper investigates into the problem of how to leverage prior knowledge robustly in learning models.\n\nQuestion:", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": "The model is compared to a number of question classification methods, including a baseline bag-of-words model, dependency-based features, and a model using WordNet hypernyms.\n\nQuestion: What is the question classification taxonomy?\n\nAnswer: The question classification taxonomy is derived from the syllabus for the NY Regents exam, and expanded from there.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset contains 7,787 science exam questions.\n\nQuestion: What is the interannotator agreement?\n\nAnswer: The interannotator", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": "The training sets are larger by a factor of 100.\n\nQuestion: What is the difference between the original ELMo and the one used in this paper?\n\nAnswer: The original ELMo was trained on a corpus of 1 billion words, while the one used in this paper was trained on a corpus of 20 million words.\n\nQuestion: What is the difference between the original ELMo and the one used in this paper?\n\nAnswer: The original ELMo was trained on a corpus of 1 billion words, while the one used in this paper was trained on a cor", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": "16225\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": "MLP\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: The s2sL approach proposed to address low data resource problem is explained in this Section. In this work, we use MLP (modified to handle our data representation) as the base classifier.\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: The s2sL approach proposed to address low data resource problem is explained in this Section. In this work, we use MLP (modified to handle our data representation) as the base classifier.\n\nQuestion: What is the main", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": "yes\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: The main contribution of the paper is the proposed multimodal NER model, which incorporates visual contexts into the traditional NER model.\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: The main contribution of the paper is the proposed multimodal NER model, which incorporates visual contexts into the traditional NER model.\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: The main contribution of the paper is the proposed multimodal NER model", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": "yes\n\nQuestion: What is the dimensionality of the latent embedding?\n\nAnswer: 100\n\nQuestion: What is the dimensionality of the latent embedding?\n\nAnswer: 100\n\nQuestion: What is the dimensionality of the latent embedding?\n\nAnswer: 100\n\nQuestion: What is the dimensionality of the latent embedding?\n\nAnswer: 100\n\nQuestion: What is the dimensionality of the latent embedding?\n\nAnswer: 100\n\nQuestion: What is the dimensionality of the lat", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": "0.6103\n\nQuestion: What was their highest MRR score?\n\nAnswer: 0.6103\n\nQuestion: What was their highest MRR score?\n\nAnswer: 0.6103\n\nQuestion: What was their highest MRR score?\n\nAnswer: 0.6103\n\nQuestion: What was their highest MRR score?\n\nAnswer: 0.6103\n\nQuestion: What was their highest MRR score?\n\nAnswer: 0.6103\n\nQuestion: What was", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": "The datasets they evaluate on are the Wall Street Journal (WSJ) portion of the Penn Treebank.\n\nQuestion: What is the Markov-structured syntax model?\n\nAnswer: The Markov-structured syntax model is a popular structure for unsupervised tagging tasks.\n\nQuestion: What is the DMV-structured syntax model?\n\nAnswer: The DMV-structured syntax model is a generative model that defines a probability distribution over dependency parse trees and syntactic categories, generating tokens and dependencies in a head-outward fashion.\n\nQuestion: What is the", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": "The authors evidence the claim by showing the results of a survey among engineers.\n\nQuestion: What is the main idea of the paper?\n\nAnswer: The main idea of the paper is to provide a DNN toolkit for NLP tasks.\n\nQuestion: What is the main idea of the paper?\n\nAnswer: The main idea of the paper is to provide a DNN toolkit for NLP tasks.\n\nQuestion: What is the main idea of the paper?\n\nAnswer: The main idea of the paper is to provide a DNN toolkit for NLP tasks.\n\nQuestion:", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": "On the WebQuestions BIBREF25 and SimpleQuestions BIBREF2 data sets.\n\nQuestion: What is the difference between the two types of relation representation?\n\nAnswer: The word-level focuses more on local information (words and short phrases), and the relation-level focus more on global information (long phrases and skip-grams) but suffer from data sparsity.\n\nQuestion: What is the difference between the two types of relation representation?\n\nAnswer: The word-level focuses more on local information (words and short phrases), and the relation-level", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
