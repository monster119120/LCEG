{"pred": "The ground truth for fake news is established by an expert manually annotating the tweets in the dataset.\n\nQuestion: What is the main goal of this study?\nAnswer: The main goal of this study is to characterize fake news in Twitter by looking into meta-data embedded in tweets.\n\nQuestion: What is the main observation made from the analysis of the distribution of the number of hashtags used?\nAnswer: The main observation made from the analysis of the distribution of the number of hashtags used is that tweets containing fake news appear to use more hashtags than other viral tweets", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": "The GhostVLAD approach is an extension of the NetVLAD approach, which was originally proposed for place recognition by R. Arandjelovic et al. [11]. The GhostVLAD is an extension to VLAD [18] approach where they were able to replace the hard assignment based clustering with soft assignment based clustering so that it can be trained with neural network in an end to end fashion. In our case, we use the GhostVLAD layer to map N local features of dimension D into a fixed dimensional vector, as shown in Figure 1 (Right side).\n\n", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": "The MDRE model outperforms the best existing research results (WAP 0.690 to 0.688) BIBREF19 . However, the MDREA model does not match the performance of the MDRE model, even though it utilizes a more complex architecture. We believe that this result arises because insufficient data are available to properly determine the complex model parameters in the MDREA model. Moreover, we presume that this model will show better performance when the audio signals are aligned with the textual sequence while applying the attention mechanism. We leave the implementation of this point as", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": "The additional features and context proposed are the use of context tweets, which are tweets that are replied to or quoted by the original tweet. These context tweets provide important information that can help computers better understand the data and improve the accuracy of abusive language detection.\n\nQuestion: What is the most accurate traditional machine learning model?\nAnswer: The most accurate traditional machine learning model is the Logistic Regression (LR) model, followed by ensemble models such as Gradient Boosted Trees (GBT) and Random Forests (RF).\n\nQuestion: What is the highest F1 score for", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.\n\nQuestion: What is the name of the dataset used for evaluation?\nAnswer: Affective Text dataset, Fairy Tales dataset, ISEAR dataset.\n\nQuestion: What is the name of the model used for development?\nAnswer: B-M model.\n\nQuestion: What is the name of the feature set used", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "No, the hashtag dataset contains both English and non-English data, while the SemEval dataset is only English.\n\nQuestion: What is the main goal of the pairwise neural ranking model?\nAnswer: The main goal of the pairwise neural ranking model is to divide a given hashtag into a sequence of meaningful words.\n\nQuestion: What is the difference between the pairwise ranking model and the pairwise linear ranker?\nAnswer: The pairwise ranking model uses a feedforward network to compare each candidate segmentation with the rest, while the pairwise linear ranker uses a percept", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": "The evaluation protocol and baseline are provided along with the corpus.\n\nQuestion: What is the size of the document clusters in the corpus?\nAnswer: The document clusters are 15 times larger than typical DUC clusters of ten documents and five times larger than the 25-document-clusters.\n\nQuestion: What is the average Jensen-Shannon divergence between the word distribution of one document and the word distribution in the remaining documents?\nAnswer: The average Jensen-Shannon divergence is 0.3490.\n\nQuestion: What is the average length", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "The CNN/DailyMail, NYT, and XSum datasets are used for evaluation.\n\nQuestion: What is the proportion of novel n-grams in automatically generated summaries?\nAnswer: The proportion of novel n-grams in automatically generated summaries is much lower compared to reference summaries, but in XSum, this gap is much smaller.\n\nQuestion: What is the proportion of selected summary sentences that appear in the source document?\nAnswer: The proportion of selected summary sentences that appear in the source document is fairly smoothly distributed across documents.\n\nQuestion: What is the proportion of novel n", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": "The proposed approach compares better than other approaches on the benchmark word similarity and entailment datasets.\n\ntocsectionIntroduction\n\nThe purpose of this project is to create a web application that allows users to search for and view information about specific topics. The application will be built using the Django framework and will use a MySQL database to store the data.\n\nThe application will have a simple user interface that allows users to search for information about a specific topic. The search results will be displayed in a list format, and users will be able to view the details of each topic by clicking on the topic name.\n\nThe application", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": "The model is trained on the BookTest dataset, which is much larger than the original CBT dataset. The BookTest dataset is derived from books available through project Gutenberg, and it contains 3555 common noun examples and 10507 named entity examples. The model is trained on the BookTest dataset, and it is evaluated on the original CBT dataset. The model is trained on the BookTest dataset, and it is evaluated on the original CBT dataset. The model is trained on the BookTest dataset, and it is evaluated on the original CBT dataset. The model", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": "The datasets are composed of two subsets, Friends and EmotionPush, according to the source of the dialogues. The former comes from the scripts of the Friends TV sitcom. The other is made up of Facebook messenger chats.\n\nQuestion: What is the objective of the challenge?\nAnswer: The objective of the challenge is to recognize emotions for all utterances in EmotionLines dataset, a dataset consists of dialogues.\n\nQuestion: What is the size of the testing dataset?\nAnswer: The testing dataset consists of 240 dialogues including 3,296 and ", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": "English\n\nQuestion: what is the main limitation of the aforementioned NMT models for text simplification?\nAnswer: the amount of available simplified corpora is insufficient for NMT model if we want to NMT model can obtain the best parameters.\n\nQuestion: what is the main limitation of the aforementioned NMT models for text simplification?\nAnswer: the amount of available simplified corpora is insufficient for NMT model if we want to NMT model can obtain the best parameters.\n\nQuestion: what is the main limitation of the aforementioned NMT models for", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": "IMDb dataset\n\nQuestion: What is the objective of this work?\n\nAnswer: To determine the optimal combinations of word2vec hyper-parameters for intrinsic and extrinsic NLP tasks.\n\nQuestion: What is the scope of the extensive work?\n\nAnswer: To determine the optimal combinations of word2vec hyper-parameters for intrinsic evaluation (semantic and syntactic analogies) and extrinsic evaluation tasks.\n\nQuestion: What is the main contribution of this research?\n\nAnswer: The empirical establishment of optimal combinations of word2vec hyper-parameters for NLP tasks, discovering the", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "The proposed system achieves a +1.08 F1 improvement on DL-PS, +1.24 on EC-MT, and +2.38 on EC-UQ.\n\nQuestion: What is the purpose of the common and private Bi-LSTMs?\nAnswer: The common and private Bi-LSTMs are used for representing annotator-generic and -specific information, and learning a label Bi-LSTM from the crowd-annotated NE label sequences.\n\nQuestion: What is the purpose of the CNN module?\nAnswer: The CNN module is used", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": "yes\n\nQuestion: What is the main difference between the two reading tasks?\nAnswer: The first task was normal reading, while the second task was task-specific reading.\n\nQuestion: What is the average reading speed for each task?\nAnswer: The average reading speed for the normal reading task was 1.2 seconds per sentence, and for the task-specific reading task it was 1.0 seconds per sentence.\n\nQuestion: What is the average omission rate for each task?\nAnswer: The average omission rate for the normal reading task was 1.2%, and for the task-", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": "The datasets used are the ones presented in BIBREF33 .\n\nQuestion: What is the state of the art on the norms for MPCS?\n\nAnswer: The state of the art on the norms for MPCS is the one presented in BIBREF33 .\n\nQuestion: What is the state of the art on the architecture for MPCS?\n\nAnswer: The state of the art on the architecture for MPCS is the one presented in BIBREF33 .\n\nQuestion: What is the state of the art on the interaction norms for MPCS", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "The Energy sector achieved the best performance.\n\nQuestion: What is the best sentence encoder for the Natural Language Inference (NLI) task?\n\nAnswer: The best sentence encoder for the Natural Language Inference (NLI) task is a BiLSTM with max-pooling for the SNLI dataset.\n\nQuestion: What is the best sentence encoder for the Reuters RCV1 dataset?\n\nAnswer: The best sentence encoder for the Reuters RCV1 dataset is a BiLSTM with max-pooling for the SNLI dataset.\n\nQuestion:", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "RNN-based NMT and Transformer-based NMT.\n\nQuestion: what is the average length of the sentences translated by Transformer?\n\nAnswer: 16.78.\n\nQuestion: what is the average length of the sentences translated by SMT?\n\nAnswer: 15.50.\n\nQuestion: what is the average length of the sentences translated by RNN-based NMT?\n\nAnswer: 17.12.\n\nQuestion: what is the average length of the sentences translated by the reference?\n\nAnswer: 16.4", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": "The three regularization terms are neutral features, maximum entropy, and KL divergence.\n\nQuestion: What is the purpose of incorporating neutral features?\nAnswer: To prevent the model from drifting from the desired direction by providing a distribution of neutral features that are not informative indicator of any classes.\n\nQuestion: What is the purpose of incorporating maximum entropy?\nAnswer: To control the influence of this term on the overall objective function by tuning $\\lambda$ according to the difference in the number of labeled features of each class.\n\nQuestion: What is the purpose of incorporating KL divergence", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": "The baselines are SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, two mature deep learning models on text classification, CNN and RCNN, and the above SVM and deep learning models with comment information.\n\nQuestion: What is the main contribution of this paper?\nAnswer: The main contribution of this paper is to propose UTCNN, a neural network model that utilizes user, topic, and comment information for stance classification on social media texts.\n\nQuestion: What is the difference between the UTCNN model", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": "The neural network-based models outperform the baselines.\n\nQuestion: What is the optimal probability for the neural network-based models?\nAnswer: The optimal probability is INLINEFORM0 .\n\nQuestion: What is the main contribution of the paper?\nAnswer: The main contribution is to show how jointly learning the ternary and fine-grained sentiment classification problems in a multitask setting improves the state-of-the-art performance.\n\nQuestion: What is the experimental setting?\nAnswer: The experimental setting is to demonstrate how multitask learning can be successfully applied on the task", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": "The adaptively sparse Transformers with @!START@$\\alpha $@!END@-entmax learn to attend to a sparse set of words that are not necessarily contiguous, enabling different heads to learn to look for various relationships between tokens.\n\nQuestion: What is the main contribution of this work?\nAnswer: The main contribution of this work is the introduction of adaptively sparse attention into the Transformer architecture, showing that it eases interpretability and leads to slight accuracy gains.\n\nQuestion: What is the difference between the adaptive sparse Transformer and the standard softmax Transformer?\n", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "The baseline is the reference translation.\n\nQuestion: what is the main limitation of the previous work?\nAnswer: The main limitation of the previous work is that it assumes that parallel document-level training data is available.\n\nQuestion: what is the main novelty of this work?\nAnswer: The main novelty of this work is that our DocRepair model operates on groups of sentences and is thus able to fix consistency errors caused by the context-agnostic baseline MT system.\n\nQuestion: what is the main difference between the DocRepair model and the previous work?\nAnswer: The", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": "The performance of the bilingual LMs is evaluated on two zero-shot cross-lingual transfer tasks: (1) Cross-lingual Natural Language Inference (XNLI) and (2) dependency parsing.\n\nQuestion: What is the purpose of the first step in the approach to learn the initial foreign word embeddings?\nAnswer: To quickly build a bilingual LM, we directly adapt the English pre-traind model to the target model. Our approach consists of three steps. First, we initialize target language word-embeddings $_f$ in the English embedding space such that", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": "The attention module is pre-trained on MT data.\n\nQuestion: What is the length of the speech encoder output?\nAnswer: The length of the speech encoder output is proportional to the length of the input frame, so it is much longer than a natural sentence.\n\nQuestion: What is the length of the text encoder output?\nAnswer: The length of the text encoder output is proportional to the length of the input frame, so it is much longer than a natural sentence.\n\nQuestion: What is the length of the CTC path?\nAnswer: The length of the CTC", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": "Stylistic patterns BIBREF4 and patterns related to situational disparity BIBREF5\n\nQuestion: What is the main hypothesis of the paper?\nAnswer: The main hypothesis of the paper is that reading sarcastic texts induces distinctive eye movement patterns, compared to literal texts.\n\nQuestion: What is the main contribution of the paper?\nAnswer: The main contribution of the paper is to explore gaze-based cognition for sarcasm detection.\n\nQuestion: What is the main limitation of the paper?\nAnswer: The main limitation of the paper is that the eye-track", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "The encoder is an LSTM.\n\nQuestion: What is the main objective of the auxiliary objective?\nAnswer: The auxiliary objective is to predict the MSD tag of the target form.\n\nQuestion: What is the effect of encoding the full context with an LSTM?\nAnswer: The effect is to increase the variance of the observed results.\n\nQuestion: What is the effect of adding the auxiliary objective of MSD prediction?\nAnswer: The effect is variable, with some languages benefiting and others not.\n\nQuestion: What is the effect of multilingual training?\n", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "yes\n\nQuestion: What is the main focus of the article?\nAnswer: The main focus of the article is to probe transformer-based QA models using synthetic datasets.\n\nQuestion: What is the main methodology used in the article?\nAnswer: The main methodology used in the article is to construct challenge datasets from a target set of knowledge resources and use these to probe transformer-based QA models.\n\nQuestion: What is the main finding of the article?\nAnswer: The main finding of the article is that transformer-based QA models have a remarkable ability to answer", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": "The baselines are the models that were used as a reference for the Jasper model.\n\nQuestion: what is the architecture of the Jasper model?\nAnswer: The Jasper model has a block architecture: a Jasper INLINEFORM0 x INLINEFORM1 model has INLINEFORM2 blocks, each with INLINEFORM3 sub-blocks. Each sub-block applies the following operations: a 1D-convolution, batch norm, ReLU, and dropout. All sub-blocks in a block have the same number of output channels.\n\nQuestion: what is the normalization", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "unanswerable\n\nQuestion: What is the main goal of this paper?\n\nAnswer: to examine the relation between people's language and their industry\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: to build a large, industry-annotated dataset and to explore the potential of predicting a user's industry\n\nQuestion: What is the main limitation of this paper?\n\nAnswer: the limitation of the data collection process, which excluded users that have been verified by the Sina Weibo platform\n\nQuestion: What is the main difference between this paper and previous studies", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "BPE perplexity, user-ranking, and qualitative analysis on user-aware model outputs confirm that personalization indeed assists in generating plausible recipes from incomplete ingredients.\n\nQuestion: What is the average recipe length in the training data?\nAnswer: 117 tokens with a maximum of 256.\n\nQuestion: What is the average number of unique ingredients across all recipes?\nAnswer: 13K.\n\nQuestion: What is the average number of users with fewer than 4 reviews?\nAnswer: 50%.\n\n", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": "The labels are \"symptom\", \"attribute\", \"completed symptoms\", \"to-do symptoms\", \"completed attributes\", and \"to-do attributes\".\n\nQuestion: What is the size of the augmented set?\nAnswer: The augmented set has 1,280 samples.\n\nQuestion: What is the size of the real-world set?\nAnswer: The real-world set has 944 samples.\n\nQuestion: What is the size of the Base Set?\nAnswer: The Base Set has 1,264 samples.\n\nQuestion:", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": "The answer is \"unanswerable\" because the article does not provide any information about the amount of data needed to train the task-specific encoder.\n\nQuestion: What is the correlation between inter-annotator agreement and difficulty scores?\n\nAnswer: The answer is \"unanswerable\" because the article does not provide any information about the correlation between inter-annotator agreement and difficulty scores.\n\nQuestion: What is the correlation between inter-annotator agreement and difficulty scores?\n\nAnswer: The answer is \"unanswerable\" because the article does not provide any information about the correlation between inter-annotator", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": "The tasks used for evaluation are:\n\n1. IWSLT 2017 German $\\rightarrow $ English\n2. KFTT Japanese $\\rightarrow $ English\n3. WMT 2016 Romanian $\\rightarrow $ English\n4. WMT 2014 English $\\rightarrow $ German\n\nThese tasks are used to evaluate the performance of the adaptively sparse Transformers on different datasets.\n\nQuestion: What is the difference between the adaptive sparse Transformer and the standard softmax Transformer?\n\nAnswer: The adaptive sparse Transformer replaces the softmax with", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": "The ELMo embeddings show the largest improvement over fastText embeddings.\n\nQuestion: What is the size of the training dataset used for the Latvian ELMo model?\nAnswer: The Latvian ELMo model was trained on a corpus of 270 million tokens.\n\nQuestion: What is the size of the training dataset used for the English ELMo model?\nAnswer: The English ELMo model was trained on a corpus of 20 million tokens.\n\nQuestion: What is the size of the training dataset used for the Croatian ELMo model?\nAnswer", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": "They have a background in the humanities and the social sciences.\n\nQuestion: What is the main goal of the research process?\nAnswer: The main goal of the research process is to identify research questions, data sources, and operationalizations.\n\nQuestion: What are some of the challenges faced in computational analyses of text?\nAnswer: Some of the challenges faced in computational analyses of text include data acquisition, conceptualization, and operationalization.\n\nQuestion: What are some of the limitations of supervised models?\nAnswer: Some of the limitations of supervised models include", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "Yes.\n\nQuestion: What is the difference between the red bars and green bars in Figure 1?\nAnswer: The red bars represent spammers, and the green bars represent legitimate users.\n\nQuestion: What is the difference between the red bars and green bars in Figure 2?\nAnswer: The red bars represent spammers, and the green bars represent legitimate users.\n\nQuestion: What is the difference between the red bars and green bars in Figure 3?\nAnswer: The red bars represent spammers, and the green bars represent legitimate users.\n\nQuestion: What is", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": "The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages.\n\nQuestion: What is the purpose of the proposed algorithm?\nAnswer: The proposed algorithm is evaluated against existing approaches using existing test sets from previous works on South African languages as well as the Discriminating between Similar Languages (DSL) 2015 and 2017 shared tasks.\n\nQuestion: What is the accuracy of the proposed algorithm?\nAnswer: The accuracy of the proposed algorithm seems to be dependent on the support of the lexicon. Without a good lexicon", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "6-layers model, 8-layers model, 9-layers model, 2-layers model, 2-layers model distilled from 9-layers model\n\nQuestion: what is the CER of 2-layers distilled model?\n\nAnswer: 14% relative reduction, compared with 2-layers regular-trained Amap model\n\nQuestion: what is the CER of 2-layers model distilled from 9-layers model?\n\nAnswer: 8.1% relative reduction, compared with 2-layers regular-trained Amap model", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "29,794 articles\n\nQuestion: What is the dataset split into?\nAnswer: 5,000 articles from each quality class\n\nQuestion: What is the dataset split into?\nAnswer: 5,000 articles from each quality class\n\nQuestion: What is the dataset split into?\nAnswer: 5,000 articles from each quality class\n\nQuestion: What is the dataset split into?\nAnswer: 5,000 articles from each quality class\n\nQuestion: What is the dataset split into?\nAnswer: 5,000", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "The human judgements were assembled by a group of 50 native people who were well-versed in both English and Tamil languages.\n\nQuestion: What was the vocabulary size of the target language in the RNNMorph model?\nAnswer: The vocabulary size of the target language in the RNNMorph model was 41,906 morphs.\n\nQuestion: What was the vocabulary size of the target language in the RNNSearch model?\nAnswer: The vocabulary size of the target language in the RNNS", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "Yes, they test their framework performance on commonly used language pairs, such as English-to-German.\n\nQuestion: What is the main difference between the multi-source approach and the approach proposed by BIBREF11?\n\nAnswer: The main difference between the multi-source approach and the approach proposed by BIBREF11 is that the multi-source approach requires a separate decoder for each target language, while the approach proposed by BIBREF11 requires a single decoder for all target languages.\n\nQuestion: What is the main reason for the degrading performance of the system trained on", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": "The models are evaluated by measuring the accuracy of the reconstruction of the target sentence.\n\nQuestion: What is the main technical contribution of this work?\nAnswer: The main technical contribution is to draw inspiration from the multi-objective optimization literature and view the tradeoff as a sequence of constrained optimization problems, which allows for more stable optimization.\n\nQuestion: What is the objective of the constrained objective in Eq (DISPLAY_FORM6)?\nAnswer: The objective is to minimize the expected cost subject to varying expected reconstruction error constraints $\\epsilon $.\n\nQuestion: What is the main difference", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": "The evaluation metrics looked at for classification tasks are precision, recall, and F-measure.\n\nQuestion: What is the purpose of the PA system in the IT company?\nAnswer: The PA system in the IT company is used to periodically measure and evaluate every employee's performance.\n\nQuestion: What is the purpose of the PA process in the IT company?\nAnswer: The PA process in the IT company is used to link the goals established by the organization to its each employee's day-to-day activities and performance.\n\nQuestion: What is the purpose of the PA process in the IT company?", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": "The source domain is the domain with labeled data, and the target domain is the domain with unlabeled data.\n\nQuestion: What is the main challenge of domain adaptation?\nAnswer: The main challenge is that data in the source and target domains are drawn from different distributions.\n\nQuestion: What are the two main limitations of previous works?\nAnswer: The two main limitations are that they highly depend on the heuristic selection of pivot features and that they only utilize the unlabeled target data for representation learning while the sentiment classifier was solely trained on the source domain.\n\nQuestion: What is", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": "The PRU is compared with LSTM, RNN, and GRU.\n\nQuestion: What is the main difference between the PRU and LSTM?\nAnswer: The PRU uses pyramidal transformation for the input vector while LSTM uses linear transformation.\n\nQuestion: What is the main difference between the PRU and GRU?\nAnswer: The PRU uses pyramidal transformation for the input vector while GRU uses linear transformation.\n\nQuestion: What is the main difference between the PRU and RNN?\nAnswer: The PRU uses pyramidal transformation for the input", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": "The neural network modules included in NeuronBlocks are word/character embedding, RNN, CNN, QRNN, Transformer, Highway network, Encoder Decoder architecture, and attention mechanisms such as Linear/Bi-linear Attention, Full Attention, Bidirectional attention flow, and more.\n\nQuestion: What is the purpose of the Block Zoo in NeuronBlocks?\n\nAnswer: The Block Zoo provides alternative layers/modules for the networks, allowing users to build complex network architectures for different NLP tasks.\n\nQuestion: What is the Model Zoo in", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": "The Wiktionary pronunciation corpus collected by deri2016grapheme for all experiments.\n\nQuestion: what is the main goal of the paper?\nAnswer: The main goal of the paper is to leverage high resource data by treating g2p as a multisource neural machine translation (NMT) problem.\n\nQuestion: what is the main difference between the two models?\nAnswer: The main difference between the two models is that LangID prepends each training, validation, and test sample with an artificial token identifying the language of the sample, while NoLangID om", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": "BERT, RoBERTa, and XLNet\n\nQuestion: What was the methodology used for speculation cue detection?\nAnswer: The input sentence was preprocessed to get the target labels as per the annotation schema, and then the preprocessed sequence was fed to the model.\n\nQuestion: What was the methodology used for speculation scope resolution?\nAnswer: The input sentence was preprocessed to get the target labels as per the annotation schema, and then the preprocessed sequence was fed to the model.\n\nQuestion: What was the methodology used for negation cue", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": "English and Spanish\n\nQuestion: What is the main goal of this work?\nAnswer: To analyze the effect of both human and machine translation in cross-lingual models.\n\nQuestion: What is the main result of this work?\nAnswer: The real cross-lingual generalization ability of XLM-R is considerably better than what the accuracy numbers in XNLI reflect.\n\nQuestion: What is the main conclusion of this work?\nAnswer: The evaluation issues raised by our analysis do not have a simple solution.\n\nQuestion: What is the main recommendation of this work?\nAnswer: Future", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": "They test their method on Named Entity Recognition, POS tagging, text classification, and language modeling.\n\nQuestion: What is the main benefit of the character based approach?\nAnswer: The main benefit of the character based approach is that large word lookup tables can be compacted into character lookup tables and the compositional model scales to large data sets better than other state-of-the-art approaches.\n\nQuestion: What is the objective function used to optimize the model?\nAnswer: The objective function used to optimize the model is the categorical cross-entropy loss between predicted and true hasht", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "unanswerable\n\nQuestion: What is the vocabulary size of the WikiBio dataset?\nAnswer: 400K words\n\nQuestion: What is the vocabulary size of the French and German datasets?\nAnswer: 297K and 143K words\n\nQuestion: What is the number of unique words in the corpus?\nAnswer: 400K words\n\nQuestion: What is the number of examples in the French and German datasets?\nAnswer: 170K and 50K\n\nQuestion: What is the", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "Yes, it was evaluated against a baseline that used a standard retrieval model.\n\nQuestion: What is the main purpose of PolyReponse?\nAnswer: The main purpose of PolyReponse is to assist users in accomplishing a well-defined task such as restaurant search.\n\nQuestion: What is the current approach to task-oriented dialogue?\nAnswer: The current approach to task-oriented dialogue is to search and interact with large databases which contain information pertaining to a certain dialogue domain.\n\nQuestion: What is the main challenge of task-oriented dialogue systems", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": "They use the LIWC.\n\nQuestion: What is the name of the tool that can be used to generate maps?\nAnswer: Lit.eecs.umich.edu/geoliwc/\n\nQuestion: What is the name of the tool that can be used to generate maps?\nAnswer: Lit.eecs.umich.edu/geoliwc/\n\nQuestion: What is the name of the tool that can be used to generate maps?\nAnswer: Lit.eecs.umich.edu/geoliwc/\n\nQuestion: What is the", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": "The ML methods aim to identify argument components in the discourse.\n\nQuestion: What is the main focus of the current article?\nAnswer: The main focus of the current article is to summarize the results of the annotation study.\n\nQuestion: What is the main research question in the introduction?\nAnswer: The main research question in the introduction is to find the boundaries of argumentation in user-generated Web content.\n\nQuestion: What is the main research question in the introduction?\nAnswer: The main research question in the introduction is to find the boundaries of argumentation in user-generated Web content.\n\n", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "n-grams of order 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": "1,873 Twitter conversation threads, roughly 14k tweets.\n\nQuestion: What is the sentiment change in Twitter conversations?\nAnswer: Sentiment tends to decrease.\n\nQuestion: What is the sentiment change in OSG conversations?\nAnswer: Sentiment tends to increase.\n\nQuestion: What is the sentiment change in nominal polarity terms in OSG?\nAnswer: The number of users that changed polarity from negative to positive is more than the double of the users that have changed the polarity from positive to negative.\n\nQuestion: What is the sentiment change in", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": "The 12 languages covered are: English, Welsh, Kiswahili, Polish, Finnish, Spanish, Mandarin Chinese, Russian, French, Estonian, and Yue Chinese.\n\nQuestion: What is the main purpose of the resource?\n\nAnswer: The main purpose of the resource is to provide a benchmark for evaluating representation learning models in different languages.\n\nQuestion: What is the main finding of the article?\n\nAnswer: The main finding of the article is that the performance of representation learning models varies across different languages.\n\nQuestion: What is the main conclusion of the", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": "Wikipedia conversations dataset and Reddit CMV data\n\nQuestion: What is the main limitation of the present work?\nAnswer: It assigns a single label to each conversation.\n\nQuestion: What is the main challenge in forecasting conversational events?\nAnswer: Unknown horizon and capturing inter-comment dynamics.\n\nQuestion: What is the main objective of the pre-training component of the model?\nAnswer: To learn a neural representation of conversational dynamics in an unsupervised fashion.\n\nQuestion: What is the main difference between the Awry and Sliding Awry baselines?", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "\"unanswerable\"\n\nQuestion: What is the main goal of the Named Entity Recognition module?\nAnswer: \"to label each part of the sentence into different categories such as 'PERSON', 'LOCATION', or 'ORGANIZATION'\"\n\nQuestion: What is the purpose of the Lexicon Matching module?\nAnswer: \"to find important terms and/or concepts from the extracted text\"\n\nQuestion: What is the purpose of the Linked Data: Ontology, Thesaurus and Terminology module?\nAnswer: \"to populate a specialized ontology\"\n", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": "The quality of the data is empirically evaluated by measuring the perplexity of the translations using a language model trained on a large amount of clean monolingual data.\n\nQuestion: What is the total speech duration in CoVoST? \nAnswer: The total speech duration in CoVoST is 708 hours.\n\nQuestion: What is the total speech duration in Tatoeba? \nAnswer: The total speech duration in Tatoeba is 9.3 hours.\n\nQuestion: What is the total number of speakers in CoVoST? ", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": "The audio-RNN encodes MFCC features from the audio signal using equation EQREF2 . The last hidden state of the audio-RNN is concatenated with the prosodic features to form the final vector representation INLINEFORM0 , and this vector is then passed through a fully connected neural network layer to form a textual encoding vector T.\n\nQuestion: What is the main difference between the ARE and TRE models?\n\nAnswer: The ARE model uses only audio features, while the TRE model uses both audio and textual information.\n\nQuestion: What is the main difference between", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": "2.11 BLEU, 1.7 FKGL and 1.07 SARI\n\nQuestion: what is the name of the model that they used for comparison?\nAnswer: NMT\n\nQuestion: what is the name of the dataset that they used for evaluation?\nAnswer: WikiLarge\n\nQuestion: what is the name of the dataset that they used for evaluation?\nAnswer: WikiSmall\n\nQuestion: what is the name of the metric that they used for evaluation?\nAnswer: BLEU\n\nQuestion: what is the name of the metric", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": "unanswerable\n\nQuestion: what is the main novelty of this work?\n\nAnswer: the main novelty of this work is that our DocRepair model operates on groups of sentences and is thus able to fix consistency errors caused by the context-agnostic baseline MT system.\n\nQuestion: what is the main limitation of the previous work?\n\nAnswer: the main limitation of the previous work is that they assume that parallel document-level training data is available.\n\nQuestion: what is the main difference between the DocRepair model and the previous work?\n\nAnswer: the DocRep", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": "A tweet went viral if it was retweeted more than 1000 times by the 8th of November 2016.\n\nQuestion: What is the main goal of this study?\nAnswer: To provide a preliminary characterization of fake news in Twitter by looking into meta-data embedded in tweets.\n\nQuestion: What is the main observation from the results?\nAnswer: The findings resonate with similar work done on fake news such as the one from Allcot and Gentzkow.\n\nQuestion: What is the main hypothesis about the features that can help", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": "CNN\n\nQuestion: Which basic neural architecture perform best by itself?\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\nAnswer: CNN\n\nQuestion: Which basic neural architecture", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": "The data collection project was mainly supported by Sharif DeepMine company.\n\nQuestion: what is the main goal of the DeepMine project?\nAnswer: The main goal of the DeepMine project was to collect speech from at least a few thousand speakers, enabling research and development of deep learning methods.\n\nQuestion: what is the number of unique phrases in each part of the database?\nAnswer: Table TABREF11 shows the number of unique phrases in each part of the database.\n\nQuestion: what is the number of trials for Persian 1-sess", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": "Machine learning methods include Logistic Regression and Deep Learning methods include CNNs and LSTMs.\n\nQuestion: What is the definition of RQE?\nAnswer: RQE is defined as recognizing question entailment, which involves finding answers to a new question by retrieving entailed questions with associated answers.\n\nQuestion: What are the two main datasets used for RQE experiments?\nAnswer: The two main datasets are SNLI and multiNLI.\n\nQuestion: What is the best performance achieved by the DL model on the medical task at TREC 201", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "The benchmark dataset is the Social Honeypot dataset, and its quality is high.\n\nQuestion: What is the difference between the two types of spammers in the dataset?\nAnswer: The two types of spammers are content polluters and fake accounts.\n\nQuestion: What is the difference in the topic distribution of spammers and legitimate users?\nAnswer: The topic distribution of spammers is narrower than that of legitimate users.\n\nQuestion: What is the difference in the topic distribution of spammers and legitimate users?\nAnswer: The topic distribution of sp", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": "The decoder has an LSTM encoder.\n\nQuestion: What is the main objective of the auxiliary objective?\nAnswer: The main objective of the auxiliary objective is to predict the MSD tag of the target form.\n\nQuestion: What is the effect of encoding the full context with an LSTM?\nAnswer: The effect of encoding the full context with an LSTM is to increase the variance of the observed results.\n\nQuestion: What is the effect of adding the auxiliary objective of MSD prediction?\nAnswer: The effect of adding the auxiliary objective of MSD prediction is", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "unanswerable\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: A novel GAN-based event extraction model called AEM.\n\nQuestion: What is the main difference between AEM and previous models?\n\nAnswer: AEM uses a generator network to learn the projection function between the document-event distribution and four event related word distributions (entity, location, keyword, date).\n\nQuestion: What is the objective of the proposed approach?\n\nAnswer: To let the distribution INLINEFORM4 (produced by INLINEFORM5 network) to approximate the real data distribution IN", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": "The best performing model among the author's submissions is the ensemble+ of (r4, r7 r12) on dev (internal) set, which has an F1 of 0.673.\n\nQuestion: What is the best performing model among author's submissions, what performance it had?\nAnswer: The best performing model among the author's submissions is the ensemble+ of (r4, r7 r12) on dev (internal) set, which has an F1 of 0.673.\n\nQuestion: What is the best performing model among", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "The baseline is the strongest model, which is the M2M Transformer NMT model (b3) trained on the mixture of all the in-domain parallel and monolingual data.\n\nQuestion: what is the best model?\nAnswer: The best model is the M2M Transformer NMT model (b3) trained on the mixture of all the in-domain parallel and monolingual data.\n\nQuestion: what is the best model for the Ja INLINEFORM0 Ru translation?\nAnswer: The best model for the Ja INLINEFORM0 Ru translation is the M2", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "0.7033\n\nQuestion: What was their lowest accuracy?\n\nAnswer: 4%\n\nQuestion: What was their highest accuracy?\n\nAnswer: 26%\n\nQuestion: What was their highest MRR score?\n\nAnswer: 32%\n\nQuestion: What was their highest F-measure score?\n\nAnswer: 0.2862\n\nQuestion: What was their highest precision score?\n\nAnswer: 0.1119\n\nQuestion: What was their highest recall score?\n\nAnswer: 0.70", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "Word embedding techniques such as word2vec BIBREF9 have become very popular. Despite the prominent role that neural networks play in many of these approaches, at their core they remain distributional techniques that typically start with a word by word co–occurrence matrix, much like many of the more traditional approaches.\n\nQuestion: What is the goal of the retrofitting vector method proposed by YuCBJW16?\nAnswer: The goal of the retrofitting vector method proposed by YuCBJW16 is to incorporate ontological information into a vector representation by including semantically related words.\n\nQuestion", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "The pre-ordering system swaps the position of the noun phrase followed by a transitive verb with the transitive verb.\n\nQuestion: What is the initial learning rate?\nAnswer: INLINEFORM0\n\nQuestion: What is the number of epochs?\nAnswer: INLINEFORM1\n\nQuestion: What is the vocabulary size of English?\nAnswer: INLINEFORM0\n\nQuestion: What is the vocabulary size of Hindi?\nAnswer: INLINEFORM1\n\nQuestion: What is the vocabulary size of other source languages?\nAnswer", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": "Yes, the paper explores extraction from electronic health records.\n\nQuestion: What is the main problem in BioIE?\nAnswer: The main problem in BioIE is the explosion of available scientific articles in the Biomedical domain.\n\nQuestion: What is the state of biomedical text mining?\nAnswer: The state of biomedical text mining is reviewed regularly.\n\nQuestion: What is the main challenge in NER in the biomedical domain?\nAnswer: The main challenge in NER in the biomedical domain is the diversity of names of proteins, gen", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": "Seven experts with legal training.\n\nQuestion: What is the main goal of this work?\nAnswer: To promote question-answering research in the specialized privacy domain.\n\nQuestion: What is the performance of the strongest BERT variant on the answer sentence selection task?\nAnswer: 39.8 F1.\n\nQuestion: What is the distribution of questions in the corpus across OPP-115 categories?\nAnswer: First party and third party related questions are the largest categories, forming nearly 66.4% of all questions asked to the privacy assistant.\n", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "The painting embedding model is a CNN-RNN generative model, and the language style transfer model is a sequence-to-sequence model with global attention.\n\nQuestion: What is the average content score for the Shakespearean prose generated for the input paintings?\nAnswer: The average content score is 3.7.\n\nQuestion: What is the average creativity score for the Shakespearean prose generated for the input paintings?\nAnswer: The average creativity score is 3.9.\n\nQuestion: What is the average style score for the Shakespearean prose generated for the input paintings", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": "ToBERT outperforms RoBERT on pre-trained BERT features and fine-tuned BERT features for all our tasks.\n\nQuestion: What is the main limitation of BERT?\nAnswer: BERT suffers from major limitations in terms of handling long sequences. Firstly, the self-attention layer has a quadratic complexity $O(n^2)$ in terms of the sequence length $n$ BIBREF0. Secondly, BERT uses a learned positional embeddings scheme BIBREF1, which means that it won't likely be able to generalize to", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": "Yes, the authors hypothesize that humans' robustness to noise is due to their general knowledge.\n\nQuestion: What is the main contribution of this paper?\nAnswer: The main contribution of this paper is two-fold: (1) a data enrichment method that extracts inter-word semantic connections from each passage-question pair in our MRC dataset, and (2) an end-to-end MRC model named as Knowledge Aided Reader (KAR) that explicitly uses the extracted general knowledge to assist its attention mechanisms.\n\nQuestion: What is the key problem in the data", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "They addressed three topics of cyberbullying: personal attack, racism, and sexism.\n\nQuestion: What is the main bottleneck of existing works on cyberbullying detection?\nAnswer: The main bottleneck is that they target only one particular social media platform.\n\nQuestion: What is the effect of oversampling bullying posts on the performance of DNN models?\nAnswer: Oversampling bullying posts significantly improves the performance of all DNN models.\n\nQuestion: What is the effect of varying the replication rate for bullying posts?\nAnswer", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "The contexts are split into three disjoint regions based on the two relation arguments: the left context, the middle context and the right context. Since in most cases the middle context contains the most relevant information for the relation, we want to focus on it but not ignore the other regions completely. Hence, we propose to use two contexts: (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. Due to the repetition of the middle context, we force the network to pay special attention to it. The", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "The dataset contains three different types of entities: Person, Location, and Organization.\n\nQuestion: What is the total number of entities in the dataset?\nAnswer: The total number of entities in the dataset is 16225.\n\nQuestion: What is the total number of words in the dataset?\nAnswer: The total number of words in the dataset is 16225.\n\nQuestion: What is the total number of sentences in the dataset?\nAnswer: The total number of sentences in the dataset is 6400.\n\nQuestion: What is the total number of", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": "unanswerable\n\nQuestion: How many sentences are annotated by experts?\n\nAnswer: unanswerable\n\nQuestion: How many sentences are annotated by crowd workers?\n\nAnswer: unanswerable\n\nQuestion: How many sentences are annotated by both experts and crowd workers?\n\nAnswer: unanswerable\n\nQuestion: How many sentences are annotated by crowd workers only?\n\nAnswer: unanswerable\n\nQuestion: How many sentences are annotated by experts only?\n\nAnswer: unanswerable\n\nQuestion: How many sentences are annotated by both experts", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": "The gender representation in training data is 33.16% for women and 66.84% for men.\n\nQuestion: What is the proportion of men and women in French radio and TV media data?\nAnswer: 65% of the speakers are men, speaking more than 75% of the time.\n\nQuestion: What is the impact of the observed disparity on ASR performance?\nAnswer: A WER increase of 24% for women compared to men, exhibiting a clear gender bias.\n\nQuestion: What is the impact of the observed disparity", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "The WMT shared tasks on MMT BIBREF23 , BIBREF24 , BIBREF25 .\n\nQuestion: What is the main metric used in the human evaluation?\nAnswer: Meteor BIBREF31 .\n\nQuestion: What is the difference in performance for French and German for the RND and AMB setups?\nAnswer: The results show that – as with the standard setup – the performance of our deliberation models is overall better than that of the base models. The results of the multimodal models differ for German and French. For German, del+obj is", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": "The strong baselines model is compared to is the Transformer.\n\nQuestion: What is the main difference between the Transformer and the standard self-attention?\nAnswer: The main difference between the Transformer and the standard self-attention is that the Transformer uses a gaussian mask and directional mask while the standard self-attention does not.\n\nQuestion: What is the purpose of the Gaussian-masked directional multi-head attention?\nAnswer: The purpose of the Gaussian-masked directional multi-head attention is to capture the localness and directional information of the input.\n", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "Logistic Regression (LR) and Multilayer Perceptron (MLP)\n\nQuestion: What is the main goal of the human-AI loop approach?\nAnswer: To discover informative keywords and estimate their expectation to train a machine learning model.\n\nQuestion: What is the main challenge in involving crowd workers?\nAnswer: Their contributions are not fully reliable.\n\nQuestion: What is the main advantage of our approach using crowdsourcing at obtaining new keywords compared with an approach labelling microposts for model training under the same cost?\nAnswer: It is significantly more cost-effect", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "NLTK, Stanford CoreNLP, TwitterNLP, BIBREF17, BIBREF18, BIBREF19, BIBREF20, BIBREF21, BIBREF22, BIBREF23, BIBREF24, BIBREF25, BIBREF26, BIBREF27.\n\nQuestion: What is the sentiment analysis task?\nAnswer: The task is to determine the sentiment of a tweet towards a candidate.\n\nQuestion: What is the NER task?\nAnswer: The task is to identify", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": "The SQuAD dataset BIBREF3.\n\nQuestion: What is the name of the tool used to extract structured answer-relevant relations?\nAnswer: OpenIE toolbox BIBREF7.\n\nQuestion: What is the name of the model used for the sentence encoder?\nAnswer: Bidirectional LSTMs.\n\nQuestion: What is the name of the model used for the relation encoder?\nAnswer: Bidirectional LSTMs.\n\nQuestion: What is the name of the model used for the decoder?\nAnswer: LSTM.", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "The existing approaches are bag-of-words representations and vector space embeddings.\n\nQuestion: what is the main hypothesis?\nAnswer: The main hypothesis is that by using vector space embeddings instead of bag-of-words representations, the ecological information which is implicitly captured by Flickr tags can be utilized in a more effective way.\n\nQuestion: what is the main motivation for using vector space embeddings?\nAnswer: The main motivation for using vector space embeddings is that they allow us to integrate the textual information we get from Flickr with available structured", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": "unanswerable\n\nQuestion: What is the name of the model?\n\nAnswer: Joint SAN\n\nQuestion: What is the name of the dataset?\n\nAnswer: SQuAD 2.0\n\nQuestion: What is the name of the classifier?\n\nAnswer: Unanswerable classifier\n\nQuestion: What is the name of the objective function of the joint model?\n\nAnswer: The objective function of the joint model has two parts:\n\nQuestion: What is the name of the span loss function?\n\nAnswer: The span loss function is defined:\n\nQuestion", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": "CSAT dataset for CSAT prediction, consisting of spoken transcripts (automatic via ASR). 20 newsgroups for topic identification task, consisting of written text; Fisher Phase 1 corpus for topic identification task, consisting of spoken transcripts (manual).\n\nQuestion: What is the computational complexity of RoBERT?\nAnswer: $O(\\frac{n^2}{k^2})$\n\nQuestion: What is the computational complexity of ToBERT?\nAnswer: $O(\\frac{n^2}{k^2})$\n\nQuestion: What is the computational complexity of", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": "IMDb movie review dataset\n\nQuestion: What is the number of units in each layer of the QRNN model?\nAnswer: 640 units\n\nQuestion: What is the number of units in each layer of the QRNN model?\nAnswer: 640 units\n\nQuestion: What is the number of units in each layer of the QRNN model?\nAnswer: 640 units\n\nQuestion: What is the number of units in each layer of the QRNN model?\nAnswer: 640 units\n\nQuestion: What is the number of units in each", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": "Yes, the BERT models were evaluated in previous work.\n\nQuestion: What is the number of sentences discarded from BIBREF3?\nAnswer: 28 sentence-pairs (8 tokens)\n\nQuestion: What is the number of sentences discarded from BIBREF1?\nAnswer: 680 sentences\n\nQuestion: What is the number of sentences discarded from BIBREF2?\nAnswer: 23,368 discarded pairs (out of 152,300)\n\nQuestion: What is the number of sentences discarded from BIB", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": "unanswerable\n\nQuestion: What is the sentiment analysis task?\n\nAnswer: unanswerable\n\nQuestion: What is the sentiment analysis task?\n\nAnswer: unanswerable\n\nQuestion: What is the sentiment analysis task?\n\nAnswer: unanswerable\n\nQuestion: What is the sentiment analysis task?\n\nAnswer: unanswerable\n\nQuestion: What is the sentiment analysis task?\n\nAnswer: unanswerable\n\nQuestion: What is the sentiment analysis task?\n\nAnswer: unanswerable\n\nQuestion: What is the sentiment analysis task?\n\nAnswer: unanswer", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": "The invertibility condition is that the Jacobian determinant of the neural projector is equal to one, which ensures that the projection is volume-preserving and information is not lost during optimization.\n\nQuestion: What is the purpose of the invertible transformation in the neural projector?\nAnswer: The invertible transformation is designed to guarantee a unit Jacobian determinant, which ensures that the projection is volume-preserving and information is not lost during optimization.\n\nQuestion: What is the dimensionality of the latent embedding space in the neural projector?\nAnswer: The dimensionality of the latent embedding", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "The full catalogue of features, their description, detailed annotation guideline as well as illustrating examples can be found in Appendix .\n\nQuestion: What is the main goal of the framework?\nAnswer: The main goal of the framework is to characterise machine reading comprehension gold standards.\n\nQuestion: What is the framework used for?\nAnswer: The framework is used for comparing different gold standards, considering the design choices for a new gold standard, and performing qualitative error analyses for a proposed approach.\n\nQuestion: What is the framework's potential applications?\nAnswer: The framework's potential", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": "600K sentences with 11.6M words\n\nQuestion: what is the size of the vocabulary?\nAnswer: 82K\n\nQuestion: what is the size of the training set?\nAnswer: 89,042 sentence pairs\n\nQuestion: what is the size of the test set?\nAnswer: 100 pairs\n\nQuestion: what is the size of the synthetic training set?\nAnswer: 60 000 sentences\n\nQuestion: what is the size of the synthetic test set?\nAnswer: 3", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": "The baselines are the methods that are compared to the proposed method.\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: The main contribution of the paper is to propose a method that can re-use every sub-net and keep the role of sub-net consistent between pre-training and fine-tuning.\n\nQuestion: What is the main problem in previous methods?\n\nAnswer: The main problem in previous methods is that there is a huge gap between pre-training and fine-tuning.\n\nQuestion: What is the proposed method?\n\nAnswer: The proposed method is", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": "English\n\nQuestion: What is the main focus of the paper?\nAnswer: To address the problem of class imbalance and lack of similarity between training and test data inherent to the task.\n\nQuestion: What is the main contribution of the paper?\nAnswer: The paper provides a statistical method of establishing the similarity of datasets and incorporates cost-sensitivity into BERT to enable models to adapt to dissimilar datasets.\n\nQuestion: What is the main result of the paper?\nAnswer: The paper shows that common (`easy') methods of data augmentation for dealing with class imbalance do", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": "The models used in the experiment are a linear SVM, a bidirectional LSTM, and a CNN.\n\nQuestion: What is the performance of the CNN model in the experiment?\nAnswer: The performance of the CNN model in the experiment is a macro-F1 score of 0.80.\n\nQuestion: What is the performance of the BiLSTM model in the experiment?\nAnswer: The performance of the BiLSTM model in the experiment is a macro-F1 score of 0.69.\n\nQuestion: What is the performance of the SVM model in", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "The answer is \"unanswerable\".\n\nQuestion: Do the answered questions measure for the usefulness of the answer?\n\nAnswer: The answer is \"unanswerable\".\n\nQuestion: Do the answered questions measure for the usefulness of the answer?\n\nAnswer: The answer is \"unanswerable\".\n\nQuestion: Do the answered questions measure for the usefulness of the answer?\n\nAnswer: The answer is \"unanswerable\".\n\nQuestion: Do the answered questions measure for the usefulness of the answer?\n\nAnswer: The answer is \"unanswerable\".\n\nQuestion:", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "Edinburgh embeddings BIBREF14 are obtained by training skip-gram model on Edinburgh corpus BIBREF15 .\n\nQuestion: what is the name of the system that was used for the experiments?\nAnswer: EmoInt\n\nQuestion: what is the name of the framework that was used for the experiments?\nAnswer: EmoInt\n\nQuestion: what is the name of the tool that was used for preprocessing?\nAnswer: EmoInt\n\nQuestion: what is the name of the tool that was used for feature extraction?\nAnswer: EmoInt\n\nQuestion:", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": "The results of the new dataset are not mentioned in the article.\n\nQuestion: What is the name of the dataset used in the experiment?\nAnswer: The dataset used in the experiment is not mentioned in the article.\n\nQuestion: What is the name of the model used in the experiment?\nAnswer: The model used in the experiment is not mentioned in the article.\n\nQuestion: What is the name of the dataset used in the experiment?\nAnswer: The dataset used in the experiment is not mentioned in the article.\n\nQuestion: What is the name of the model used in the experiment?\nAnswer:", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": "The combination of rewards for reinforcement learning is the harmonic mean of irony accuracy, sentiment and content preservation.\n\nQuestion: What is the main reason for the issue that some models tend to generate sentences which are towards irony but do not preserve content?\n\nAnswer: The main reason for the issue is that rewards for content preservation are too prominent and rewards for style accuracy cannot work well.\n\nQuestion: What is the conclusion of the automatic evaluation results of the transformation from non-ironic sentences to ironic sentences?\n\nAnswer: The conclusion of the automatic evaluation results of the transformation", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": "The authors demonstrate that their model does not work well with Shakespeare style transfer for the given painting \"Starry Night\" with a low average content score. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data.<begin_url>https://www.physicsforums.com/threads/finding-the-area-of-a-triangle-with-given-sides.266669/<end_url># Finding the area of a triangle with given", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": "The Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset.\n\nQuestion: What is the name of the dataset used for development?\nAnswer: Affective development.\n\nQuestion: What is the name of the model used for development?\nAnswer: B-M.\n\nQuestion: What is the name of the model used for evaluation?\nAnswer: B-E.\n\nQuestion: What is the name of the model used for evaluation?\nAnswer: B-M.\n\nQuestion: What is the name of the model used for evaluation?\nAnswer:", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": "The distribution results showed that accounts spreading fake news had a higher proportion of friends/followers than those spreading viral content only.\n\nQuestion: What was the main observation made from the results?\nAnswer: The main observation was that accounts spreading fake news had a higher proportion of friends/followers than those spreading viral content only.\n\nQuestion: What was the main finding regarding the content of viral fake news?\nAnswer: The main finding was that the content of viral fake news was highly polarized.\n\nQuestion: What was the main finding regarding the number of URLs in viral", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": "The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset.\n\nQuestion: What is the goal of the pairwise ranking model?\nAnswer: The goal of the pairwise ranking model is to divide a given hashtag into a sequence of meaningful words.\n\nQuestion: What is the architecture of the pairwise neural ranking model?\nAnswer: The architecture of the pairwise neural ranking model is presented in Figure FIGREF11 (a).\n\nQuestion: What is the architecture of the multi-task learning model?\nAnswer: The architecture of the multi-task", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": "The corpus contains Persian accents from different regions.\n\nQuestion: what is the size of the database?\nAnswer: The database contains 1969 respondents, with 1149 of them being male and 820 female.\n\nQuestion: what is the main goal of the DeepMine project?\nAnswer: The main goal of the DeepMine project was to collect speech from at least a few thousand speakers, enabling research and development of deep learning methods.\n\nQuestion: what is the number of unique phrases in each part of the database?\nAnswer:", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "word subspace can represent sets of word vectors, retaining most of the variability of features.\n\nQuestion: What is the main goal of the experiment in the section \"Evaluation of the word subspace representation\"?\nAnswer: The main goal of the experiment is to visualize how much of the text data can be represented by a lower dimensional subspace.\n\nQuestion: What is the accuracy rate of SA with w2v?\nAnswer: The accuracy rate of SA with w2v is 78.73%.\n\nQuestion: What is the accuracy rate of LSA with BOW features?", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": "SVM\n\nQuestion: What is the overall performance of INLINEFORM0?\nAnswer: P=0.93, R=0.514, F1=0.676\n\nQuestion: What is the overall performance of INLINEFORM0 across different classes?\nAnswer: P=0.844, R=0.885, F1=0.860\n\nQuestion: What is the overall performance of INLINEFORM0 for the years 2009 and 2013?\nAnswer: P=0.66, R", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": "unanswerable\n\nQuestion: What is the number of Transformer blocks in the BERT model?\n\nAnswer: 12\n\nQuestion: What is the number of the hidden layer in the BERT model?\n\nAnswer: 768\n\nQuestion: What is the number of self-attention heads in the BERT model?\n\nAnswer: 12\n\nQuestion: What is the total number of parameters of the pre-trained BERT model?\n\nAnswer: 110M\n\nQuestion: What is the number of epochs in the fine-tuning", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": "11,000 speakers and over 60 accents.\n\nQuestion: What is the total speech duration of CoVoST?\nAnswer: 708 hours.\n\nQuestion: What is the total speech duration of Tatoeba evaluation set?\nAnswer: 9.3 hours.\n\nQuestion: What is the total speech duration of CoVoST?\nAnswer: 708 hours.\n\nQuestion: What is the total speech duration of Tatoeba evaluation set?\nAnswer: 9.3 hours.\n\nQuestion: What is the", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": "The dataset used is the SemEval-2016 “Sentiment Analysis in Twitter” task.\n\nQuestion: What is the primary task?\nAnswer: The primary task is the fine-grained sentiment classification.\n\nQuestion: What is the secondary task?\nAnswer: The secondary task is the ternary sentiment classification.\n\nQuestion: What is the architecture of the neural network used?\nAnswer: The architecture of the neural network used is a biLSTM network with two LSTMs.\n\nQuestion: What is the primary measure used for evaluation?\nAnswer: The primary", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": "unanswerable\n\nQuestion: What is the number of Transformer blocks in the BERT model?\n\nAnswer: 12\n\nQuestion: What is the number of the hidden layer in the BERT model?\n\nAnswer: 768\n\nQuestion: What is the number of self-attention heads in the BERT model?\n\nAnswer: 12\n\nQuestion: What is the total number of parameters of the pre-trained model?\n\nAnswer: 110M\n\nQuestion: What is the name of the pre-trained BERT model?\n", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": "Yes, the automatically constructed datasets are subject to quality control.\n\nQuestion: What is the main appeal of using automatically generate datasets?\nAnswer: The main appeal of using automatically generate datasets is the ability to systematically manipulate and control the complexity of target questions, which allows for more controlled experimentation and new forms of evaluation.\n\nQuestion: What is the cluster-based analysis used for?\nAnswer: The cluster-based analysis is used to evaluate how consistent and robust models are across our different probes, and to get insight into whether errors are concentrated on a small set of concepts or widespread across clusters.", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": "unanswerable\n\nQuestion: What is the main focus of the article?\nAnswer: unanswerable\n\nQuestion: What is the main focus of the article?\nAnswer: unanswerable\n\nQuestion: What is the main focus of the article?\nAnswer: unanswerable\n\nQuestion: What is the main focus of the article?\nAnswer: unanswerable\n\nQuestion: What is the main focus of the article?\nAnswer: unanswerable\n\nQuestion: What is the main focus of the article?\nAnswer: unanswerable\n\nQuestion: What is the main focus of the", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": "The model trained on Facebook reactions achieved competitive results without relying on any handcrafted resource.\n\nQuestion: What is the name of the dataset used for evaluation?\nAnswer: The Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset.\n\nQuestion: What is the name of the model that achieved the best performance on development data?\nAnswer: The best model (B-M) on development data relies entirely on automatically obtained information, both in terms of training data as well as features.\n\nQuestion: What is the name of the model that achieved the best", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": "The tagging scheme consists of two tags: INLINEFORM0 and INLINEFORM1. INLINEFORM0 indicates that the current word is not a pun, while INLINEFORM1 indicates that the current word is a pun. If the tag sequence of a sentence contains a INLINEFORM0 tag, then the text contains a pun and the word corresponding to INLINEFORM1 is the pun.\n\nQuestion: What is the main difference between the INLINEFORM0 and INLINEFORM1 tagging schemes?\nAnswer: The INLINEFORM0 tagging scheme indicates that the current word appears before the pun in the given", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": "Yes, Arabic is one of the 11 languages in CoVost.\n\nQuestion: What is the total speech duration of CoVost in hours?\nAnswer: The total speech duration of CoVost is 708 hours.\n\nQuestion: What is the total speech duration of Tatoeba in hours?\nAnswer: The total speech duration of Tatoeba is 9.3 hours.\n\nQuestion: What is the total number of speakers in CoVost?\nAnswer: The total number of speakers in CoVost is over 11,000", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": "The robustness problem is addressed in the framework of generalized expectation criteria. The model is constrained to be in the preferred direction.\n\nQuestion: What is the main contribution of this work?\nAnswer: The main contributions of this work are as follows: (1) the factors of reducing the sensibility of the prior knowledge; (2) the bias in the prior knowledge; (3) the robustness problem on top of GE-FL; (4) the regularization terms to make the model more robust.\n\nQuestion: What is the difference between GE-FL and GE-FL with regularization terms", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": "tf-idf, average GloVe embeddings, InferSent, and Universal Sentence Encoder.\n\nQuestion: What is the performance of SBERT for STS tasks?\nAnswer: SBERT outperforms InferSent and Universal Sentence Encoder on the STS benchmark dataset.\n\nQuestion: What is the performance of SBERT for SentEval?\nAnswer: SBERT outperforms InferSent and Universal Sentence Encoder on the SentEval toolkit.\n\nQuestion: What is the performance of SBERT for the AFS", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "The proposed method improves the F1 score by +0.97 and +2.36 for English and Chinese datasets, respectively.\n\nQuestion: What are the datasets used for MRC task?\nAnswer: The following five datasets are used for MRC task: SQuAD v1.1, SQuAD v2.0, Quoref, SQuAD v1.1, and SQuAD v2.0.\n\nQuestion: What is the performance of the proposed method on the PI task?\nAnswer: The proposed method improves the F1 score by +0.", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": "They test their conflict method on two tasks: Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask.\n\nQuestion: What is the main objective of any attentive or alignment process?\nAnswer: The main objective of any attentive or alignment process is to look for matching words or phrases between two sequences and assign a high weight to the most similar pairs and vice-versa.\n\nQuestion: What is the main limitation of using only attention?\nAnswer: The main limitation of using only attention is that it assumes that there is some highly matched word pairs", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": "The baselines they compared against were:\n- A feed-forward neural network\n- A matrix-vector multiplication\n- A tensor computation\n- A shift-reduce algorithm\n- A typical LSTM\n- A feed-forward neural network\n- A matrix-vector multiplication\n- A tensor computation\n- A shift-reduce algorithm\n- A typical LSTM\n- A feed-forward neural network\n- A matrix-vector multiplication\n- A tensor computation\n- A shift-reduce algorithm\n- A typical LSTM\n- A feed-forward neural network\n- A matrix-vector multiplication\n- A", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": "The core component for KBQA is the improved relation detection model, HR-BiLSTM, which performs hierarchical matching between questions and KB relations.\n\nQuestion: What is the main focus of this work?\nAnswer: The main focus of this work is to improve the relation detection subtask and further explore how it can contribute to the KBQA system.\n\nQuestion: What is the main difference between general relation detection tasks and KB-specific relation detection?\nAnswer: The main difference is that the number of target relations is limited in KB-specific relation detection, normally smaller", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "The baseline models are the Encoder-Decoder model and the Nearest-Neighbor model.\n\nQuestion: What is the hidden size of the Encoder-Decoder model?\nAnswer: The hidden size of the Encoder-Decoder model is 256.\n\nQuestion: What is the number of unique ingredients in the recipes?\nAnswer: The number of unique ingredients in the recipes is 13K.\n\nQuestion: What is the maximum number of ingredients in a recipe?\nAnswer: The maximum number of ingredients in a", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": "The methods to find examples of biases and unwarranted inferences are manual detection, part-of-speech information, and Louvain clustering.\n\nQuestion: What is the main goal of this paper?\nAnswer: The main goal of this paper is to give an overview of linguistic bias and unwarranted inferences resulting from stereotypes and prejudices.\n\nQuestion: What is the main conclusion of this paper?\nAnswer: The main conclusion of this paper is that the Flickr30K dataset is biased and that it is difficult to automatically detect bias from the", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": "French\n\nQuestion: What is the gender of the pronoun in the sentence \"The girls sang a song and they danced\"?\nAnswer: feminine\n\nQuestion: What is the gender of the pronoun in the sentence \"The girls sang a song and they danced\"?\nAnswer: feminine\n\nQuestion: What is the gender of the pronoun in the sentence \"The girls sang a song and they danced\"?\nAnswer: feminine\n\nQuestion: What is the gender of the pronoun in the sentence \"The girls sang a song and they danced\"?\nAnswer: feminine\n\nQuestion", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": "They experimented with models that use plain stacked LSTMs, models with different INLINEFORM0 , models without INLINEFORM1 , and models that integrate lower contexts via peephole connections.\n\nQuestion: What is the name of the dataset used for evaluating the performance of the proposed method on the NLI task?\nAnswer: The name of the dataset used for evaluating the performance of the proposed method on the NLI task is SNLI.\n\nQuestion: What is the name of the dataset used for evaluating the performance of the proposed method on the PI task?\nAnswer:", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": "unanswerable\n\nQuestion: What is the name of the algorithm used in the proposed method?\n\nAnswer: GloVe\n\nQuestion: What is the name of the lexical resource used in the proposed method?\n\nAnswer: Roget's Thesaurus\n\nQuestion: What is the name of the word embedding algorithm used as a baseline?\n\nAnswer: GloVe\n\nQuestion: What is the name of the word embedding algorithm used as a baseline?\n\nAnswer: GloVe\n\nQuestion: What is the name of the word embedding algorithm used as a baseline", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": "The authors experimented with several summarization algorithms, including those provided by the Sumy package.\n\nQuestion: What is the purpose of the ILP-based summarization algorithm?\n\nAnswer: The ILP-based summarization algorithm is designed to create a summary of peer feedback comments for a given employee, comparing it with manual summaries.\n\nQuestion: What is the significance of the two sample t-test results?\n\nAnswer: The two sample t-test results show that the performance of the ILP-based summarization algorithm is comparable to other algorithms, and that human evaluators preferred the phrase-", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": "The previous state of the art for this task was proposed by BIBREF0 .\n\nQuestion: What is the primary problem that the article is trying to solve?\nAnswer: The primary problem leads to a secondary problem of inferring the appropriate amount of context to intervene.\n\nQuestion: What is the main innovation of the proposed models?\nAnswer: The main innovation of the proposed models is to decompose the intervention prediction problem into a two-stage model that first explicitly tries to discover the proper context to which a potential intervention could be replying to, and then, predict the intervention status.", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": "The answer is \"unanswerable\".\n\nQuestion: What is the main difference between MPAD and the other baselines?\nAnswer: The answer is \"MPAD explicitly captures the hierarchical structure of documents, while the other baselines do not.\"\n\nQuestion: What is the main difference between MPAD and the other baselines?\nAnswer: The answer is \"MPAD explicitly captures the hierarchical structure of documents, while the other baselines do not.\"\n\nQuestion: What is the main difference between MPAD and the other baselines?\nAnswer: The answer is \"MPAD explicitly", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": "The corpus used for the task is DTA corpus BIBREF11.\n\nQuestion: What is the gold standard data set used for the task?\nAnswer: The gold standard data set used for the task is DURel data set BIBREF12.\n\nQuestion: What is the metric used to evaluate the models?\nAnswer: The metric used to evaluate the models is Spearman's $\\rho $.\n\nQuestion: What are the two baselines used for the task?\nAnswer: The two baselines used for the task are log-transformed normalized frequency difference (FD", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "Kannada, Hindi, Telugu, Malayalam, Bengali, and English.\n\nQuestion: What is the name of the model used for feature extraction?\nAnswer: ResNet-34\n\nQuestion: What is the name of the pooling method that outperforms all other pooling methods?\nAnswer: GhostVLAD\n\nQuestion: What is the name of the pooling method that outperforms all other pooling methods?\nAnswer: GhostVLAD\n\nQuestion: What is the name of the pooling method that outperforms all other pooling", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": "The model performance on target language reading comprehension is unanswerable.\n\nQuestion: What is the model performance on target language reading comprehension?\nAnswer: The model performance on target language reading comprehension is unanswerable.\n\nQuestion: What is the model performance on target language reading comprehension?\nAnswer: The model performance on target language reading comprehension is unanswerable.\n\nQuestion: What is the model performance on target language reading comprehension?\nAnswer: The model performance on target language reading comprehension is unanswerable.\n\nQuestion: What is the model performance on target language reading comprehension", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": "The difference in performance between proposed model and baselines is significant.\n\nQuestion: What is the performance of the proposed model compared to the baselines?\nAnswer: The performance of the proposed model is significantly better than the baselines.\n\nQuestion: What is the performance of the proposed model compared to the baselines?\nAnswer: The performance of the proposed model is significantly better than the baselines.\n\nQuestion: What is the performance of the proposed model compared to the baselines?\nAnswer: The performance of the proposed model is significantly better than the baselines.\n\nQuestion: What is the performance of the", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "ARAML performs significantly better than other baselines in all the cases. This result indicates that the samples surrounding true responses provide stable rewards for the generator, and stable RAML training paradigm significantly enhances the performance in both metrics.\n\nQuestion: What is the main difference between ARAML and RAML?\nAnswer: The main difference is that ARAML gets samples from a stationary distribution around real data, while RAML gets samples from a non-parametric distribution constructed based on a specific reward.\n\nQuestion: What is the impact of temperature on the performance of ARAML?\nAnswer", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": "The authors present evidence that the model can capture some biases in data annotation and collection by examining the results of the fine-tuning strategies. The results show that the model misclassifies hate content as offensive in 63% of the cases for the Davidson dataset, which suggests that the model is capturing biases in the data annotation and collection process. Additionally, the authors provide examples of tweets that are misclassified as offensive or neither, which further supports the claim that the model is capturing biases in the data.\n\nQuestion: What is the main reason for the high misclass", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "Yes, the answer is \"yes\".\n\nQuestion: What is the performance of the best performing BERT variant on the answer sentence selection task?\nAnswer: The answer is \"39.8\".\n\nQuestion: What is the percentage of questions that are identified as being incomprehensible?\nAnswer: The answer is \"4.18%\".\n\nQuestion: What is the percentage of questions that are identified as being relevant?\nAnswer: The answer is \"95.7%\".\n\nQuestion: What is the percentage of questions that are identified as being ill-formed?\nAnswer:", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "The size of the dataset is not mentioned in the article, so the answer is \"unanswerable\".\n\nQuestion: What is the size of the ILPRL dataset?\n\nAnswer: The size of the ILPRL dataset is not mentioned in the article, so the answer is \"unanswerable\".\n\nQuestion: What is the size of the OurNepali dataset?\n\nAnswer: The size of the OurNepali dataset is not mentioned in the article, so the answer is \"unanswerable\".\n\nQuestion: What is the size of the dataset used for the experiments?\n\nAnswer", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": "The proposed method improves the F1 score by +0.58 for MRPC and +0.73 for QQP.\n\nQuestion: What is the performance of the proposed method on the SST-2 dataset?\n\nAnswer: The proposed method achieves 94.9 in terms of accuracy.\n\nQuestion: What is the performance of the proposed method on the SST-5 dataset?\n\nAnswer: The proposed method achieves 55.19 in terms of accuracy.\n\nQuestion: What is the performance of the proposed method on the Chinese OntoNotes4", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "The datasets used are the ERP data from BIBREF0 and the eye-tracking and self-paced reading time data from BIBREF1 .\n\nQuestion: What is the purpose of the neural network?\nAnswer: The purpose of the neural network is to predict the ERP components from the behavioral data and the neural data.\n\nQuestion: What is the significance of the results?\nAnswer: The significance of the results is that they show that all six of the ERP components from BIBREF0 can be predicted above chance by a model which has been pretrained using a language model", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": "The subjects were presented with a series of stimuli, including words, letters, and numbers, to elicit event-related responses.\n\nQuestion: What was the primary goal of the study?\n\nAnswer: The primary goal of the study was to investigate the neural mechanisms underlying event-related responses.\n\nQuestion: What was the experimental design of the study?\n\nAnswer: The experimental design involved presenting stimuli to subjects and recording their event-related responses.\n\nQuestion: What was the main finding of the study?\n\nAnswer: The main finding of the study was that event-related responses", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": "Pointer-Gen, Pointer-Gen-RL+ROUGE, Pointer-Gen-RL-SEN, and Pointer-Gen-ARL-SEN.\n\nQuestion: What is the sensationalism score used for?\nAnswer: To measure how sensational a headline is.\n\nQuestion: What is the sensationalism scorer used for?\nAnswer: To classify sensational and non-sensational headlines.\n\nQuestion: What is the reward function used for?\nAnswer: To give dynamic weights to balance between MLE and RL.\n\nQuestion:", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": "The dataset uses traditional machine learning classifiers such as Naïve Bayes, Logistic Regression, Support Vector Machine, and Random Forests. Additionally, it uses neural network models such as Convolutional Neural Networks, Recurrent Neural Networks, and their variants.\n\nQuestion: What is the most accurate learning model on the dataset?\nAnswer: The most accurate learning model on the dataset is the bidirectional GRU network with LTC.\n\nQuestion: What is the highest F1 score for \"spam\" tweets?\nAnswer: The highest F1 score for \"spam\"", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": "The language model architectures used are Big Transformer and base transformer.\n\nQuestion: What is the number of transformer blocks in the encoder and decoder?\nAnswer: The number of transformer blocks in the encoder and decoder is 6 and 190M respectively.\n\nQuestion: What is the number of optimizer steps used for training?\nAnswer: The number of optimizer steps used for training is 10K-200K depending on bitext size.\n\nQuestion: What is the number of GPUs used for training?\nAnswer: The number of", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": "The weights are dynamically adjusted based on the training loss, which encourages learning easier examples first.\n\nQuestion: What is the purpose of the proposed method?\nAnswer: The proposed method aims to address the data imbalance issue in NLP tasks.\n\nQuestion: What is the difference between the proposed method and the standard cross-entropy loss?\nAnswer: The proposed method uses dice loss, which is a hard version of the F1 score, while the standard cross-entropy loss is a soft version of the F1 score.\n\nQuestion: What is the effect of the proposed method on accuracy", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": "The results show that the knowledge graph is critical for passing bottlenecks, and that the policy chaining method is more efficient than the Go-Explore method.\n\nQuestion: What is the main contribution of this work?\nAnswer: The main contribution is the introduction of a method to detect bottlenecks in text-games using the overall reward gained and the knowledge graph state.\n\nQuestion: What is the difference between the two exploration methods?\nAnswer: The two exploration methods are KG-A2C and A2C-Explore. KG-A2C uses", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": "The individual model consists of individual Bayesian models for each language.\n\nQuestion: What is the total number of predicate instances in the EN data?\nAnswer: The total number of predicate instances in the EN data is 3.4M.\n\nQuestion: What is the total number of predicate instances in the DE data?\nAnswer: The total number of predicate instances in the DE data is 2.62M.\n\nQuestion: What is the total number of predicate instances in the EN data?\nAnswer: The total number of predicate instances in the EN data is 3.4M.\n\nQuestion", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": "The non-standard pronunciation is identified by the use of the generated phonetic lexicon also used in the above-mentioned speech synthesis techniques.\n\nQuestion: What is the size of the resource in hours?\nAnswer: The resource is comprised of 142 hours of spoken Mapudungun.\n\nQuestion: What is the main criterion for choosing alphabetic characters?\nAnswer: The main criterion for choosing alphabetic characters was to use the current Spanish keyboard that was available on all computers in Chilean offices and schools.\n\nQuestion: What is the orth", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": "A semicharacter architecture is a type of word recognition model that processes a sentence of words with misspelled characters, predicting the correct words at each step.\n\nQuestion: What is the sensitivity of a word recognition model?\nAnswer: The sensitivity of a word recognition model is the expected number of unique outputs it assigns to a set of adversarial perturbations.\n\nQuestion: What is the main contribution of the paper?\nAnswer: The main contribution of the paper is a task-agnostic defense, attaching a word recognition model before the downstream classifier, which can handle adversarial missp", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": "The 16 languages explored are Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish.\n\nQuestion: what is the main goal of the article?\nAnswer: The main goal of the article is to compare the respective impact of external lexicons and word vector representations on the accuracy of PoS models.\n\nQuestion: what is the main conclusion of the article?\nAnswer: The main conclusion of the article is that feature-based models adequately enriched with external morph", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": "NCEL outperforms all baseline methods in both easy and hard cases.\n\nQuestion: What is the main limitation of the global approach?\nAnswer: The main limitation of the global approach is the high complexity and expensive costs.\n\nQuestion: What is the impact of the prior probability in the \"easy\" case?\nAnswer: The prior probability performs quite well in TAC2010 but poorly in WW.\n\nQuestion: What is the impact of the global module in NCEL?\nAnswer: The global module in NCEL brings more improvements in the \"hard\" case", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": "Yes\n\nQuestion: What is the percentage of times dosage is correct in this case?\n\nAnswer: 71.75%\n\nQuestion: What is the percentage of times the correct frequency was extracted by the model?\n\nAnswer: 73.58%\n\nQuestion: What is the name of the model used for the QA approach?\n\nAnswer: QA PGNet\n\nQuestion: What is the name of the model used for the Multi-decoder approach?\n\nAnswer: Multi-decoder QA PGNet\n\nQuestion: What is the name", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "The baseline used was the error detection system by Rei2016, trained using the same FCE dataset.\n\nQuestion: What was the main evaluation measure used?\nAnswer: The main evaluation measure used was INLINEFORM0, which was established as the preferred measure for error correction and detection by the CoNLL-14 shared task.\n\nQuestion: What was the main improvement observed?\nAnswer: The main improvement observed was that error detection performance was substantially improved by making use of artificially generated data, created by any of the described methods.\n\nQuestion: What was the main limitation of", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": "The synthesized user queries were created by randomly combining terminologies from the dermatology glossary, which, while providing data that helped the model learn entity segmentation, did not reflect the co-occurrence information in real user queries.\n\nQuestion: what is the name of the BiLSTM-CRF model used in the project?\nAnswer: BiLSTM-CRF model\n\nQuestion: what is the name of the fine-tuned ELMo model used in the project?\nAnswer: ELMo model\n\nQuestion: what is the name of the fine-tuned", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": "Masking words in the decoder is helpful because it allows the model to focus on the most important words in the summary, rather than being distracted by less important words.\n\nQuestion: What is the main contribution of this work?\n\nAnswer: The main contribution of this work is the introduction of a two-stage decoding process, which allows the model to generate each word of the summary considering both sides' context information.\n\nQuestion: What is the main problem with previous abstractive methods?\n\nAnswer: The main problem with previous abstractive methods is that they use left-context-only decoder", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": "PPDB\n\nQuestion: What is the objective function of the model?\nAnswer: Predicting adjacent word (within-tweet relationships), adjacent tweet (inter-tweet relationships), the tweet itself (autoencoder), modeling from structured resources like paraphrase databases and weak supervision.\n\nQuestion: What is the main advantage of using unsupervised representation learning methods?\nAnswer: They are cheaper to train, as they work with unlabelled data, reduce the dependence on domain level experts, and are highly effective across multiple applications.\n\nQuestion: What is", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": "TF-IDF features are used.\n\nQuestion: What is the primary objective of the study?\nAnswer: The primary objective of the study is to analyze the efficacy of existing machine learning approaches for the automated classification of pathology reports into different diagnosis categories.\n\nQuestion: What is the distribution of reports across different primary diagnoses and primary sites?\nAnswer: The distribution of reports across different primary diagnoses and primary sites is reported in tab:report-distribution.\n\nQuestion: What is the performance of the XGBoost classifier?\nAnswer: The XGBoost classifier out", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": "The dataset is annotated with 9,473 annotations for 9,300 tweets. Each tweet is annotated as no evidence of depression (e.g., “Citizens fear an economic depression\") or evidence of depression (e.g., “depressed over disappointment\"). If a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms, for example, depressed mood (e.g., “feeling down in the dumps\"), disturbed sleep (e.g., “another rest", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": "The eight NER tasks are:\n- BioBERTv1.0: BioCreative V1.0\n- BioBERTv1.1: BioCreative V1.1\n- BioBERTv2.0: BioCreative V2.0\n- BioBERTv2.1: BioCreative V2.1\n- BioBERTv2.2: BioCreative V2.2\n- BioBERTv2.3: BioCreative V2.3\n- BioBERTv2.4: BioCreative V2.4\n-", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": "The training data was translated from English to Spanish.\n\nQuestion: What was the name of the machine translation platform used for translation?\nAnswer: Apertium\n\nQuestion: What was the name of the AffectiveTweets WEKA package used for creating word embeddings?\nAnswer: AffectiveTweets\n\nQuestion: What was the name of the neural network model used?\nAnswer: LSTM\n\nQuestion: What was the name of the SVM regressor used?\nAnswer: SVM\n\nQuestion: What was the name of the feed-forward network", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": "\"We build a large, industry-annotated dataset that contains over 20,000 blog users. In addition to their posted text, we also link a number of user metadata including their gender, location, occupation, introduction and interests.\"\n\nQuestion: What is the name of the dataset used in this study?\n\nAnswer: \"We compile our industry-annotated dataset by identifying blogger profiles located in the U.S. on the profile finder on http://www.blogger.com, and scraping only those users that had the industry profile element completed.\"\n\nQuestion: What", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "The baseline system for the SLC task is a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence.\n\nQuestion: What was the approach used by team newspeak?\nAnswer: A word could belong to one of the 18 propaganda techniques, to none of them, or to an auxiliary (token-derived) class.\n\nQuestion: What was the approach used by team Stalin?\nAnswer: They performed subsampling in order to deal with class imbalance, and experimented with BERT", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": "The first block of Table TABREF11 shows the results of the baselines that do not adopt joint learning.\n\nQuestion: What is the name of the dataset used in the experiments?\nAnswer: The two benchmark datasets are the homographic dataset and the heterographic dataset.\n\nQuestion: What is the name of the model architecture used in the experiments?\nAnswer: The model architecture is illustrated in Figure FIGREF8 with a running example.\n\nQuestion: What is the name of the tagging scheme used in the experiments?\nAnswer: We propose a novel tagging scheme consisting of three tags, namely", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": "The political bias of different sources is included in the model by training only on left-biased or right-biased articles of both disinformation and mainstream domains and testing on the entire set of sources.\n\nQuestion: What is the main conclusion of the study?\nAnswer: The main conclusion of the study is that the topological features of multi-layer diffusion networks might be effectively exploited to detect online disinformation.\n\nQuestion: What is the main research question of the study?\nAnswer: The main research question of the study is whether the use of a multi-layer, disentangled network yields a significant", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": "The dataset comes from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era.\n\nQuestion: What is the average length of the sentences translated by the SMT model?\nAnswer: The average length of the sentences translated by the SMT model is 15.50.\n\nQuestion: What is the average length of the sentences translated by the Transformer model?\nAnswer: The average length of the sentences translated by the Transformer model is 16.78.\n\nQuestion: What is the", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "English\n\nQuestion: What is the main purpose of the dataset?\nAnswer: To provide a dataset for the shared task SemEval 2019 Task 6: Identifying and Categorizing Offensive Language in Social Media (OffensEval)\n\nQuestion: What is the breakdown of the data into training and testing for the labels from each level?\nAnswer: The training data is 10,000 tweets and the testing data is 4,100 tweets.\n\nQuestion: What is the breakdown of the keywords and their offensive content in the trial", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": "The compound PCFG/neural PCFG are strong models for grammar induction. In particular the compound PCFG outperforms other models by an appreciable margin on both English and Chinese. We again note that we were unable to induce meaningful grammars through a traditional PCFG with the scalar parameterization despite a thorough hyperparameter search. See lab:full for the full results (including corpus-level INLINEFORM1 ) broken down by sentence length.\n\nQuestion: what is the main motivation for the compound PCFG?\n\nAnswer: The compound PCF", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": "The UTCNN model has three layers: a user matrix embedding layer, a user vector embedding layer, and a fully connected layer.\n\nQuestion: What is the purpose of the maximum pooling layer in the UTCNN model?\nAnswer: The maximum pooling layer is used to select the most important feature for comments.\n\nQuestion: What is the difference between the UTCNN model and the SVM model?\nAnswer: The SVM model uses unigram, bigram, and trigram features, while the UTCNN model uses user, topic, and comment information.\n\nQuestion: What is the difference between the", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": "The dataset used in this paper is the set of 70 million Flickr photos with coordinates in Europe, all of which were uploaded to Flickr before the end of September 2015.\n\nQuestion: what is the main hypothesis of this paper?\nAnswer: The main hypothesis of this paper is that by using vector space embeddings instead of bag-of-words representations, the ecological information which is implicitly captured by Flickr tags can be utilized in a more effective way.\n\nQuestion: what is the main motivation for using vector space embeddings in this context?", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": "NUBes-PHI and MEDDOCAN.\n\nQuestion: What is the main objective of the paper?\nAnswer: To evaluate BERT's multilingual model and compare it to other established machine-learning algorithms in a specific task: sensitive data detection and classification in Spanish clinical free text.\n\nQuestion: What is the main result of the paper?\nAnswer: BERT outperforms other systems without requiring any adaptation or domain-specific feature engineering, just by being trained on the provided labelled data.\n\nQuestion: What is the main conclusion of the paper?\nAnswer: The B", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": "Unigrams and Pragmatic features BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 (b) Stylistic patterns BIBREF4 and patterns related to situational disparity BIBREF5 and (c) Hastag interpretations BIBREF6 , BIBREF7 .\n\nQuestion: What is the name of the database used for sarcasm analysis?\nAnswer: Sarcasm Understanding Database\n\nQuestion: What is the name of the eye-tracking experiment used for sarcasm analysis?\nAnswer: Eye-tracking Database", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are: \n\n1. Predictive performance: LiLi outperforms Sep with INLINEFORM12 on Freebase considering MCC.\n2. Strategy formulation ability: LiLi's RL-model can effectively formulate query-specific inference strategies.\n3. User interaction vs. performance: LiLi's performance improves significantly as it acquires more clues from the user.\n\nNote that the answer is a single phrase, and it does not provide any explanation. ", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "Yes\n\nQuestion: What is the name of the dataset that is created by the indexing-based method?\nAnswer: Answer retrieval\n\nQuestion: What is the name of the dataset that is created by the indexing-based method?\nAnswer: Answer retrieval\n\nQuestion: What is the name of the dataset that is created by the indexing-based method?\nAnswer: Answer retrieval\n\nQuestion: What is the name of the dataset that is created by the indexing-based method?\nAnswer: Answer retrieval\n\nQuestion: What is the name of the dataset that is created by the indexing-based", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": "Galatasaray and Fenerbahçe\n\nQuestion: What is the domain of the tweets?\nAnswer: The domain of the tweets comprises two popular football clubs which constitute the targets of the tweets included.\n\nQuestion: What is the size of the data set?\nAnswer: The data set is a balanced one although it is currently limited in size. The corresponding stance annotations are made publicly available at http://ceng.metu.edu.tr/ INLINEFORM0 e120329/ Turkish_Stance_Detection_Tweet", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": "We conduct experiments on the transformation from non-ironic sentences to ironic sentences and the transformation from ironic sentences to non-ironic sentences.\n\nQuestion: What is the main reason for the issue of no change?\nAnswer: The main reason for the issue of no change is that rewards for content preservation are too prominent and rewards for style accuracy cannot work well.\n\nQuestion: What is the conclusion of the transformation from non-ironic sentences to ironic sentences?\nAnswer: The conclusion of the transformation from non-ironic sentences to ironic sentences is that our model outperforms other gener", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": "Gaussian-masked directional multi-head attention is a variant of self-attention which is used to capture the localness and directional information of input. It is similar as scaled dot-product attention but it has a directional mask and gaussian mask.\n\nQuestion: What is the main difference between the Transformer and the standard self-attention?\n\nAnswer: The Transformer is a kind of self-attention network which is proposed in BIBREF38. The standard self-attention is similar as Gaussian-masked direction attention but it does not have directional mask and gaussian", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "Facebook\n\nQuestion: What is the name of the dataset released by the authors?\nAnswer: Causality Facebook dataset\n\nQuestion: What is the name of the model used for causality prediction?\nAnswer: Linear SVM\n\nQuestion: What is the name of the model used for causal explanation identification?\nAnswer: LSTM\n\nQuestion: What is the name of the model used for causality prediction?\nAnswer: Linear SVM\n\nQuestion: What is the name of the model used for causal explanation identification?\nAnswer: LSTM\n\nQuestion: What is the name of", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": "The baseline features are the features extracted from the baseline CNN architecture.\n\nQuestion: What is the purpose of the baseline method?\nAnswer: The purpose of the baseline method is to classify a sentence as sarcastic or non-sarcastic.\n\nQuestion: What is the difference between the baseline features and the pre-trained features?\nAnswer: The baseline features are the features extracted from the baseline CNN architecture, while the pre-trained features are the features extracted from the pre-trained models.\n\nQuestion: What is the purpose of the pre-tra", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": "The number of clusters and the number of classes were varied in the experiments on the four tasks.\n\nQuestion: What is the main focus of this paper?\nAnswer: The main focus of this paper is to explore the effectiveness of incorporating cluster membership features in the feature extraction pipeline of Named-Entity recognition, sentiment classification, and quantification tasks.\n\nQuestion: What is the main contribution of this paper?\nAnswer: The main contribution of this paper is to show empirically the effectiveness of incorporating cluster membership features in the feature extraction pipeline of Named-Entity recognition, sentiment classification, and quant", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": "The scores of their system are 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": "53 documents, 8,275 sentences, 167,739 words\n\nQuestion: What is the average number of sentences per document?\nAnswer: 156.1 sentences\n\nQuestion: What is the average number of tokens per entity?\nAnswer: 19.55 tokens\n\nQuestion: What is the most frequently annotated type of entity?\nAnswer: Findings\n\nQuestion: What is the most frequently annotated relation in the corpus?\nAnswer: has-relation between a case entity and the findings related to that case\n\nQuestion", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": "Yes, it is possible to convert cloze-style questions to naturally-looking questions.\n\nQuestion: What is the main purpose of cloze-style questions?\nAnswer: The main purpose of cloze-style questions is to construct a large number of cloze questions from the unlabeled corpus.\n\nQuestion: What is the difference between cloze-style questions and naturally-looking questions?\nAnswer: Cloze-style questions are constructed by replacing a placeholder with a specific answer, while naturally-looking questions are constructed by using a natural language.\n\nQuestion: What is the main difference between cloze", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": "Sentiment classification, text categorization, and web-page classification.\n\nQuestion: What is the main contribution of this work?\nAnswer: The main contribution is to reveal the factors of reducing the sensibility of the prior knowledge and therefore to make the model more robust and practical.\n\nQuestion: What is the objective function of GE-FL?\nAnswer: The objective function is to minimize the KL divergence between the predicted and reference class distributions.\n\nQuestion: What is the maximum entropy term?\nAnswer: The maximum entropy term is to constrain the predicted class distribution on unlabeled data", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": "The authors compare their model to those developed on open-domain TREC questions BIBREF6 , with smaller recent datasets available in the biomedical BIBREF3 , BIBREF4 and education BIBREF5 domains.\n\nQuestion: What is the size of the dataset used in this work?\nAnswer: The dataset used in this work is the largest challenge dataset for question classification, containing 7,787 science exam questions.\n\nQuestion: What is the classification taxonomy used in this work?\nAnswer: The classification taxonomy used in this work is a hierarchical taxonomy with ", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": "The training sets are larger by a factor of 1000.\n\nQuestion: What is the name of the dataset used for training the ELMo models for Croatian, Estonian, Finnish, Latvian, Lithuanian, Slovenian, and Swedish?\nAnswer: The dataset is called hrWaC 2.1 corpus.\n\nQuestion: What is the name of the dataset used for training the ELMo models for Croatian, Estonian, Finnish, Latvian, Lithuanian, Slovenian, and Swedish?\nAnswer: The dataset is called hrW", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": "16225 unique words extracted from POS-tagged Nepali National Corpus\n\nQuestion: What is the total number of entities in the dataset?\nAnswer: 16225 unique words extracted from POS-tagged Nepali National Corpus\n\nQuestion: What is the total number of entities in the dataset?\nAnswer: 16225 unique words extracted from POS-tagged Nepali National Corpus\n\nQuestion: What is the total number of entities in the dataset?\nAnswer: 16225 unique words extracted from POS-tag", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": "They compare to MLP, Eusboost, and MWMOTE.\n\nQuestion: What is the main intention of the authors for the task of classifying speech and music?\nAnswer: The main intention of the authors for the task of classifying speech and music is not better feature selection, but to demonstrate the effectiveness of their approach, in particular for low data scenarios.\n\nQuestion: What is the main intention of the authors for the task of emotion classification?\nAnswer: The main intention of the authors for the task of emotion classification is to demonstrate the effectiveness of their approach, in particular for", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": "Yes.\n\nQuestion: What is the main contribution of their work?\n\nAnswer: The main contribution of their work is to incorporate visual contexts for named entity recognition tasks.\n\nQuestion: What is the name of the dataset used in their experiments?\n\nAnswer: SnapCaptions.\n\nQuestion: What is the name of the model used in their experiments?\n\nAnswer: Bi-LSTM/CRF + Bi-CharLSTM + Inception.\n\nQuestion: What is the name of the model used in their experiments?\n\nAnswer: Bi-LSTM/", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": "unanswerable\n\nQuestion: What is the name of the neural projector used in the experiments?\n\nAnswer: volume-preserving invertible neural network\n\nQuestion: What is the name of the invertible transformation proposed by BIBREF16?\n\nAnswer: normalizing flow\n\nQuestion: What is the name of the Markov-structured syntax model used in the experiments?\n\nAnswer: Gaussian HMM\n\nQuestion: What is the name of the DMV-structured syntax model used in the experiments?\n\nAnswer: Dependency Model with Valence\n\nQuestion: What is", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": "26%\n\nQuestion: What was their highest MRR score?\nAnswer: 26%\n\nQuestion: What was their highest MRR score?\nAnswer: 26%\n\nQuestion: What was their highest MRR score?\nAnswer: 26%\n\nQuestion: What was their highest MRR score?\nAnswer: 26%\n\nQuestion: What was their highest MRR score?\nAnswer: 26%\n\nQuestion: What was their highest MRR score?\nAnswer: 26%\n\nQuestion: What was their highest MRR", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": "The Wall Street Journal (WSJ) portion of the Penn Treebank.\n\nQuestion: What is the number of words in the WSJ corpus?\nAnswer: 1 million.\n\nQuestion: What is the dimensionality of the word embeddings?\nAnswer: 100.\n\nQuestion: What is the context window size for the word embeddings?\nAnswer: 1.\n\nQuestion: What is the number of tag clusters in the POS tagging experiment?\nAnswer: 45.\n\nQuestion: What is the number of coupling layers in the", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": "The authors evidence this claim by conducting a survey among engineers and identifying a spectrum of three personas. The survey revealed that 87.5% of NLP-related jobs belong to a few common tasks, and more than 90% of the networks are composed of several common components. The authors also analyzed the NLP jobs submitted to a commercial centralized GPU cluster, which showed that about 87.5% NLP related jobs belong to a few common tasks, and more than 90% of the networks were composed of several common components.\n\nQuestion: What is the main idea of", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": "SimpleQuestions and WebQSP\n\nQuestion: What is the name of the entity linker used in the system?\nAnswer: Entity linker\n\nQuestion: What is the name of the KBQA system?\nAnswer: KBQA system\n\nQuestion: What is the name of the KBQA end task?\nAnswer: KBQA end task\n\nQuestion: What is the name of the KBQA system that uses multiple relation detectors?\nAnswer: STAGG\n\nQuestion: What is the name of the KBQA system that uses a single relation det", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
