{"pred": "The ground truth for fake news is established by a single person who manually annotates the tweets in the dataset.\n\nQuestion: What is the main goal of this study?\nAnswer: The main goal of this study is to provide a preliminary characterization of fake news in Twitter by looking into meta-data embedded in tweets.\n\nQuestion: What is the main finding of this study?\nAnswer: The main finding of this study is that there are specific pieces of meta-data about tweets that may allow the identification of fake news.\n\nQuestion: What is the main hypothesis of this study?\n", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": "The GhostVLAD approach was originally proposed for face recognition to improve language identification performance for Indian languages.\n\nQuestion: What is the main idea of any pooling strategy?\nAnswer: The main idea of any pooling strategy is to pool the frame-level representations into a single utterance level representation.\n\nQuestion: What is the NetVLAD pooling strategy?\nAnswer: The NetVLAD pooling strategy was initially developed for place recognition by R. Arandjelovic et al. [11].\n\nQuestion: What is the GhostVLAD pooling approach?\nAnswer:", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": "By 68.8% to 71.8%\n\nQuestion: What is the name of the dataset used in the experiments?\nAnswer: Interactive Emotional Dyadic Motion Capture (IEMOCAP)\n\nQuestion: What is the name of the toolkit used to extract audio features?\nAnswer: OpenSMILE toolkit\n\nQuestion: What is the name of the model that uses high-level text transcription, as well as low-level audio signals, to utilize the information contained within low-resource datasets to a greater degree?\nAnswer: Multimod", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": "The use of context tweets is proposed as an additional feature to improve the accuracy of neural network models.\n\nQuestion: What is the most accurate traditional machine learning model?\nAnswer: The most accurate traditional machine learning model is the LR model.\n\nQuestion: What is the highest F1 score for \"spam\" tweets?\nAnswer: The highest F1 score for \"spam\" tweets is 0.551.\n\nQuestion: What is the highest F1 score for \"hateful\" tweets?\nAnswer: The highest F1 score for \"hateful\" tweets is", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.\n\nQuestion: What is the name of the dataset used for the development set?\nAnswer: Affective Text\n\nQuestion: What is the name of the dataset used for the test set?\nAnswer: Fairy Tales\n\nQuestion: What is the name of the dataset used for the evaluation?\nAnswer: ISEAR\n", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "No, the hashtag dataset contains both English and non-English data, while the SemEval dataset is English only.\n\nQuestion: What is the main goal of the pairwise ranking model?\nAnswer: The pairwise ranking model aims to rank candidate segmentations of a hashtag based on their relative order, rather than directly comparing them.\n\nQuestion: What is the difference between the pairwise ranking model and the pairwise linear ranker?\nAnswer: The pairwise ranking model uses a feedforward network to compare candidate segmentations, while the pairwise linear ranker uses a perceptron class", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": "The evaluation protocol and baseline are provided along with the corpus.\n\nQuestion: What is the size of the document clusters in the corpus?\nAnswer: The document clusters are 15 times larger than typical DUC clusters of ten documents and five times larger than the 25-document-clusters.\n\nQuestion: What is the average number of tokens per document in the corpus?\nAnswer: The average cluster size is 97,880 token.\n\nQuestion: What is the average Jensen-Shannon divergence between the word distribution of one document and the word distribution", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "The CNN/DailyMail, NYT, and XSum datasets are used for evaluation.\n\nQuestion: What is the name of the dataset used for evaluation?\nAnswer: The CNN/DailyMail dataset is used for evaluation.\n\nQuestion: What is the name of the dataset used for evaluation?\nAnswer: The NYT dataset is used for evaluation.\n\nQuestion: What is the name of the dataset used for evaluation?\nAnswer: The XSum dataset is used for evaluation.\n\nQuestion: What is the name of the dataset used for evaluation?\nAnswer: The CNN/DailyMail dataset", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": "The proposed approach compares favorably with other WSD approaches employing word embeddings.\n\nQuestion: What is the main advantage of using KL divergence as the energy function?\nAnswer: The main advantage of using KL divergence as the energy function is that it enables to capture asymmetry in entailment datasets.\n\nQuestion: What is the main advantage of using KL divergence as the energy function?\nAnswer: The main advantage of using KL divergence as the energy function is that it enables to capture asymmetry in entailment datasets.\n\nQuestion: What is the", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": "The algorithm starts with the best performing model according to validation performance. Then in each step, it tries adding the best performing model that has not been previously tried. It keeps it in the ensemble if it improves its validation performance and discards it otherwise. This way, it gradually tries each model once.\n\nQuestion: What is the name of the algorithm used to select the best performing model?\nAnswer: The algorithm is called a greedy ensemble.\n\nQuestion: What is the name of the dataset used to select the best performing model?\nAnswer: The dataset is called the INLINEFORM0 BookTest validation dataset.", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": "The datasets are composed of two subsets, Friends and EmotionPush, according to the source of the dialogues. The former comes from the scripts of the Friends TV sitcom. The other is made up of Facebook messenger chats.\n\nQuestion: What is the objective of the challenge?\nAnswer: The objective of the challenge is to recognize emotions for all utterances in EmotionLines dataset, a dataset consists of dialogues.\n\nQuestion: What is the difference between single utterance and dialogue emotion recognition?\nAnswer: The same utterances in Table TABREF2, even the same", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": "English\n\nQuestion: what is the main limitation of the aforementioned NMT models for text simplification?\nAnswer: The main limitation of the aforementioned NMT models for text simplification depended on the parallel ordinary-simplified sentence pairs.\n\nQuestion: what is the main limitation of the aforementioned NMT models for text simplification?\nAnswer: The main limitation of the aforementioned NMT models for text simplification depended on the parallel ordinary-simplified sentence pairs.\n\nQuestion: what is the main limitation of the aforementioned NMT", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": "IMDb dataset of movie reviews by BIBREF11\n\nQuestion: What is the objective of this work?\nAnswer: To determine the optimal combinations of word2vec hyper-parameters for intrinsic evaluation (semantic and syntactic analogies) and extrinsic evaluation tasks (BIBREF13, BIBREF14), like SA and NER.\n\nQuestion: What is the objective of this work?\nAnswer: To determine the optimal combinations of word2vec hyper-parameters for intrinsic evaluation (semantic and syntactic analogies) and extrinsic evaluation tasks (BIBREF13", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "The proposed system achieves an accuracy of 95.3% on the DL-PS dataset, 94.8% on the EC-MT dataset, and 95.1% on the EC-UQ dataset.\n\nQuestion: What is the purpose of the label Bi-LSTM?\n\nAnswer: The label Bi-LSTM is used for worker discriminator only. It is used to predict the exact worker responsible for the annotation.\n\nQuestion: What is the purpose of the CNN module?\n\nAnswer: The CNN module is used to extract features for the worker", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": "yes\n\nQuestion: What is the main difference between the two reading paradigms?\nAnswer: The participants had to determine whether a certain relation type occurred in the sentence or not.\n\nQuestion: What is the main difference between the two reading paradigms?\nAnswer: The participants had to determine whether a certain relation type occurred in the sentence or not.\n\nQuestion: What is the main difference between the two reading paradigms?\nAnswer: The participants had to determine whether a certain relation type occurred in the sentence or not.\n\nQuestion: What is the main difference between the two reading paradigms", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": "The authors would like to thank Maximilien de Bayser, Ana Paula Appel, Flavio Figueiredo and Marisa Vasconcellos, who contributed with discussions during SABIA and CognIA's implementation.\n\nQuestion: What is the main goal of the architecture?\nAnswer: The main goal of the architecture is to address the challenges of engineering MPCS.\n\nQuestion: What is the main goal of the architecture?\nAnswer: The main goal of the architecture is to address the challenges of engineering MPCS.\n\nQuestion: What is the main goal", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "The HealthCare sector achieved the best performance.\n\nQuestion: What is the main limitation of the current NLP research on volatility forecasting?\n\nAnswer: The main limitation of the current NLP research on volatility forecasting is the lack of a comprehensive dataset of news headlines at the individual stock level.\n\nQuestion: What is the main advantage of using a global model approach?\n\nAnswer: The main advantage of using a global model approach is that it learns a model that takes into account the features of all the 40 stocks universe.\n\n\nQuestion:", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "RNN-based NMT and Transformer-NMT\n\nQuestion: what is the name of the dataset they built?\nAnswer: Ancient-Modern Chinese Dataset\n\nQuestion: what is the name of the method they used to build the dataset?\nAnswer: Clause Alignment\n\nQuestion: what is the name of the method they used to align the clauses?\nAnswer: Dynamic Programming\n\nQuestion: what is the name of the method they used to align the paragraphs?\nAnswer: Paragraph Alignment\n\nQuestion: what is the name of the method they used to align", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": "The three regularization terms are: (1) a regularization term associated with neutral features; (2) the maximum entropy of class distribution regularization term; and (3) the KL divergence between reference and predicted class distribution.\n\nQuestion: What is the purpose of incorporating neutral features?\nAnswer: To prevent the model from biasing to the class that has a dominate number of labeled features.\n\nQuestion: What is the purpose of incorporating the KL divergence of class distribution?\nAnswer: To control the unbalance in labeled features and in the dataset.\n\nQuestion:", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": "The baselines are: 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) two mature deep learning models on text classification, CNN and Recurrent Convolutional Neural Networks (RCNN), 5) the above SVM and deep learning models with comment information, 6) UTCNN without user information, representing a pure-text CNN model where we use the same user matrix and user embeddings for each user, 7) UTCNN without the", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": "The biLSTM network with multitask learning achieves the best performance.\n\nQuestion: What is the name of the winning system of the 2016 edition of the challenge?\nAnswer: Balikas et al.\n\nQuestion: What is the name of the dataset used in the experiment?\nAnswer: SemEval-2016 “Sentiment Analysis in Twitter” task\n\nQuestion: What is the name of the feature set used in the experiment?\nAnswer: nbow\n\nQuestion: What is the name of the feature set used in the experiment?\nAnswer:", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": "The adaptively sparse Transformers with @!START@$\\alpha $@!END@-entmax learn different sparsity patterns at each layer, leading to more variance in individual head behavior, to clearly-identified dense and sparse heads, and overall to different tendencies compared to the fixed case of @!START@$\\alpha =1.5$@!END@.\n\nQuestion: What is the main contribution of this work?\nAnswer: The main contribution of this work is the introduction of adaptively sparse attention into the Transformer architecture, showing that it eases interpretability and leads to slight accuracy g", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "The baseline model, the model used for back-translation, and the DocRepair model are all Transformer base models.\n\nQuestion: what is the main novelty of this work?\nAnswer: The main novelty of this work is that our DocRepair model operates on groups of sentences and is thus able to fix consistency errors caused by the context-agnostic baseline MT system.\n\nQuestion: what is the main limitation of this work?\nAnswer: The main limitation of this work is that it assumes that parallel document-level training data is available.\n\nQuestion: what is the", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": "The metrics used for evaluation are accuracy and LAS.\n\nQuestion: What is the main limitation of the approach?\n\nAnswer: The main limitation of the approach is that it cannot be applied to autoregressive LMs such as XLNet.\n\nQuestion: What is the purpose of the first step in the approach?\n\nAnswer: The purpose of the first step is to initialize target language word-embeddings $_f in the English embedding space such that embeddings of a target word and its English equivalents are close together.\n\nQuestion: What is the purpose of the second step in", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": "The attention module is pretrained on ASR and MT tasks.\n\nQuestion: What is the role of the text encoder in the TCEN model?\nAnswer: The text encoder learns high-level linguistic features into hidden representations.\n\nQuestion: What is the role of the speech encoder in the TCEN model?\nAnswer: The speech encoder reads the input audio to word or subword representations.\n\nQuestion: What is the role of the decoder in the TCEN model?\nAnswer: The decoder defines a distribution probability over target words.\n\nQuestion", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": "BIBREF4 and patterns related to situational disparity\n\nQuestion: What is the name of the database used for sarcasm analysis?\nAnswer: BIBREF8\n\nQuestion: What is the name of the eye-tracking experiment?\nAnswer: Eye-tracking Database for Sarcasm Analysis\n\nQuestion: What is the name of the eye-tracking device used?\nAnswer: SR-Research Eyelink-1000\n\nQuestion: What is the name of the two-tailed t-test used?\nAnswer: Two-tailed t-", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "The encoder has an LSTM.\n\nQuestion: What is the main objective of the baseline system?\nAnswer: The main objective of the baseline system is to generate an inflected word form given its lemma and the context in which it occurs.\n\nQuestion: What is the main objective of the auxiliary objective?\nAnswer: The auxiliary objective is to predict the MSD tag of the target form.\n\nQuestion: What is the effect of encoding the full context with an LSTM?\nAnswer: Encoding the full context with an LSTM increases the variance of the observed results", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Yes, WordNet is useful for taxonomic reasoning for this task.\n\nQuestion: What is the main focus of this paper?\nAnswer: The main focus of this paper is to look at systematically constructing synthetic datasets from expert knowledge sources and using them to probe state-of-the-art transformer models in the domain of science multiple-choice question answering (MCQA).\n\nQuestion: What is the main methodology used in this paper?\nAnswer: The main methodology used in this paper is to construct challenge datasets (Figure FIGREF1, yellow box) from a target", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": "The baselines were the standard components that led to SOTA results on LibriSpeech and competitive results on other benchmarks.\n\nQuestion: what was the architecture of the Jasper model?\nAnswer: The Jasper model was a family of end-to-end ASR models that replaced acoustic and pronunciation models with a convolutional neural network.\n\nQuestion: what was the architecture of the Jasper DR model?\nAnswer: The Jasper DR model followed DenseNet and DenseRNet, but instead of having dense connections within a block, the output of a convolution block was", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "unanswerable\n\nQuestion: What is the name of the platform?\n\nAnswer: unanswerable\n\nQuestion: What is the name of the company?\n\nAnswer: unanswerable\n\nQuestion: What is the name of the dataset?\n\nAnswer: unanswerable\n\nQuestion: What is the name of the study?\n\nAnswer: unanswerable\n\nQuestion: What is the name of the dataset?\n\nAnswer: unanswerable\n\nQuestion: What is the name of the study?\n\nAnswer: unanswerable\n\nQuestion: What is the name of the dataset", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "BLEU-1/4, ROUGE-L, BPE perplexity, Distinct-1/2, UMA, MRR, recipe-level coherence, and step entailment.\n\nQuestion: What is the main contribution of the paper?\nAnswer: The paper explores a new task of generating plausible and personalized recipes from incomplete input specifications by leveraging historical user preferences.\n\nQuestion: What is the main difference between the baseline and personalized models?\nAnswer: The personalized models attend over historical user preferences from previously consumed", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": "The labels are: (1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom.\n\nQuestion: What is the name of the dataset they use to train their model?\nAnswer: The dataset is called \"Simulated Data\".\n\nQuestion: What is the name of the model they use to train their model?\nAnswer: The model is called \"Bi-directional Attention", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": "The answer is not provided in the article, but it is implied that the task-specific encoder is trained on the full training set, which includes both the difficult and easy sentences.\n\nQuestion: What is the correlation between the predicted difficulty scores and the inter-annotator agreement scores?\nAnswer: The answer is not provided in the article, but it is implied that the predicted difficulty scores are not strongly correlated with the inter-annotator agreement scores.\n\nQuestion: What is the correlation between the predicted difficulty scores and the inter-annotator agreement scores?\nAnswer: The answer is not provided in the", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": "The question cannot be answered based on the information in the article.\n\nQuestion: What is the name of the dataset used for evaluation?\nAnswer: The question cannot be answered based on the information in the article.\n\nQuestion: What is the name of the dataset used for evaluation?\nAnswer: The question cannot be answered based on the information in the article.\n\nQuestion: What is the name of the dataset used for evaluation?\nAnswer: The question cannot be answered based on the information in the article.\n\nQuestion: What is the name of the dataset used for evaluation?\nAnswer: The question cannot be", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": "The ELMo embeddings show the largest improvement over fastText embeddings.\n\nQuestion: What is the size of the training dataset used for the Latvian ELMo model?\nAnswer: The Latvian ELMo model was trained on a whole corpus of 270 million tokens.\n\nQuestion: What is the size of the training dataset used for the English ELMo model?\nAnswer: The English ELMo model was trained on a one billion word large English corpus.\n\nQuestion: What is the size of the training dataset used for the Croatian ELMo model?\nAnswer", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": "They are scholars from very different disciplines.\n\nQuestion: What is the main goal of the article?\nAnswer: To describe the research process that involves measuring social or cultural concepts using computational methods.\n\nQuestion: What is the introductory example about?\nAnswer: It is about the operators of the online discussion site Reddit banning several communities under new anti-harassment rules.\n\nQuestion: What is the main question they are trying to answer?\nAnswer: They are trying to answer whether eliminating these “echo chambers” diminishes the amount of hate speech overall.\n", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "No, the paper is not introducing an unsupervised approach to spam detection. The paper is introducing a supervised approach to spam detection, using a combination of local and global information to improve the performance of existing spammer classification methods.\n\nQuestion: What is the main contribution of the paper?\nAnswer: The main contribution of the paper is the introduction of two topic-based features, LOSS and GOSS, which capture the user's interests on different topics and the user's interests on specific topics in comparison with other users', respectively. These features are used to discriminate human-like sp", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": "The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages.\n\nQuestion: What is the purpose of the proposed algorithm?\nAnswer: The proposed algorithm was evaluated on three existing datasets and compared to the implementations of three public LID implementations as well as to reported results of four other algorithms.\n\nQuestion: What is the main focus of this section?\nAnswer: The focus of this section is on recently published datasets and LID research applicable to the South African context.\n\nQuestion: What is the main focus of this section?\nAnswer: The focus of", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "2-layers Shenma model further trained with sMBR achieves about 8.1% relative reduction, compared with 2-layers regular-trained Amap model.\n\nQuestion: what is the name of the dataset used in the experiment?\nAnswer: Shenma voice search\n\nQuestion: what is the name of the dataset used in the experiment?\nAnswer: Shenma voice search\n\nQuestion: what is the name of the dataset used in the experiment?\nAnswer: Shenma voice search\n\nQuestion: what is the name of the dataset used in the experiment?\n", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "29,794 articles\n\nQuestion: What is the name of the dataset used in the experiments?\nAnswer: Wikipedia\n\nQuestion: What is the name of the dataset used in the experiments?\nAnswer: arXiv\n\nQuestion: What is the name of the dataset used in the experiments?\nAnswer: Wikipedia\n\nQuestion: What is the name of the dataset used in the experiments?\nAnswer: arXiv\n\nQuestion: What is the name of the dataset used in the experiments?\nAnswer: Wikipedia\n\nQuestion: What is the name of the dataset used in the experiments?", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "The human judgements were assembled by a group of 50 native people who were well-versed in both English and Tamil languages.\n\nQuestion: What was the main drawback of the RNN models used in the experiment?\nAnswer: The main drawback of the RNN models used in the experiment was the static size of the context vector of the encoder.\n\nQuestion: What was the main advantage of the RNNMorph model over the RNNSearch model?\nAnswer: The main advantage of the RNNMorph model over the RNNSearch model was the reduction in", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "Yes, they test their framework performance on commonly used language pairs, such as English-to-German.\n\nQuestion: What is the name of the framework that they propose?\nAnswer: Universal Encoder and Decoder\n\nQuestion: What is the name of the framework that they propose?\nAnswer: Universal Encoder and Decoder\n\nQuestion: What is the name of the framework that they propose?\nAnswer: Universal Encoder and Decoder\n\nQuestion: What is the name of the framework that they propose?\nAnswer: Universal Encoder and Decoder\n\nQuestion: What is the name of the", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": "The models are evaluated by measuring the accuracy of the reconstruction of the target sentence $x$ from the keywords $z$.\n\nQuestion: What is the main technical contribution of this work?\nAnswer: The main technical contribution is to draw inspiration from the multi-objective optimization literature and view the tradeoff as a sequence of constrained optimization problems, where we minimize the expected cost subject to varying expected reconstruction error constraints $\\epsilon $.\n\nQuestion: What is the objective function used to optimize the communication scheme?\nAnswer: The objective function is to minimize the expected cost subject to varying expected reconstruction error", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": "The evaluation metrics looked at for classification tasks are precision, recall, and F-measure.\n\nQuestion: What is the purpose of the PA process in a modern organization?\n\nAnswer: The PA process in a modern organization is to periodically measure and evaluate every employee's performance.\n\nQuestion: What is the purpose of the PA system in a large multi-national IT company?\n\nAnswer: The PA system in a large multi-national IT company is to track and analyze the interactions that happen in various steps of the PA process.\n\nQuestion: What is the purpose of the PA dataset used in", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": "The source domain is the domain from which the labeled data is available, and the target domain is the domain from which the unlabeled data is available.\n\nQuestion: What is the main limitation of the methods proposed in previous works?\nAnswer: The methods highly depend on the heuristic selection of pivot features, which may be sensitive to different applications.\n\nQuestion: What is the main intuition behind the proposed method?\nAnswer: The proposed method argues that the information from unlabeled target data is beneficial for domain adaptation and proposes a novel Domain Adaptive Semi-supervised learning framework", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": "LSTMs, RAN, QRNN, NAS\n\nQuestion: what is the main difference between the PRU and LSTM?\nAnswer: PRU uses pyramidal and grouped linear transformations while LSTM uses linear transformations\n\nQuestion: what is the main advantage of the PRU over LSTM?\nAnswer: PRU learns representations at higher dimensionality with more generalization power, resulting in performance gains for language modeling\n\nQuestion: what is the main advantage of the PRU over LSTM?\nAnswer: PRU learns representations at higher dimensionality with more", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": "The neural network modules included in NeuronBlocks are word/character embedding, RNN, CNN, QRNN, Transformer, Highway network, Encoder Decoder architecture, attention mechanisms, regularization layers, loss functions, metrics, and knowledge distillation.\n\nQuestion: What is the purpose of the Block Zoo in NeuronBlocks?\n\nAnswer: The Block Zoo is an open framework that provides common layers like RNN, CNN, QRNN, Transformer, attention mechanisms, regularization layers, loss functions, metrics, and knowledge distillation.\n\nQuestion", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": "The multilingual pronunciation corpus collected by deri2016grapheme for all experiments.\n\nQuestion: what is the name of the system that they compared their results to?\nAnswer: wFST\n\nQuestion: what is the name of the system that they compared their results to?\nAnswer: wFST\n\nQuestion: what is the name of the system that they compared their results to?\nAnswer: wFST\n\nQuestion: what is the name of the system that they compared their results to?\nAnswer: wFST\n\nQuestion: what is the name of", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": "The baselines were BERT, XLNet, and RoBERTa.\n\nQuestion: What was the dataset used for the speculation detection and scope resolution task?\nAnswer: The dataset used for the speculation detection and scope resolution task was the BioScope Corpus.\n\nQuestion: What was the dataset used for the negation detection and scope resolution task?\nAnswer: The dataset used for the negation detection and scope resolution task was the Sherlock Corpus.\n\nQuestion: What was the dataset used for the speculation detection and scope resolution task?\nAnswer: The dataset used for the speculation", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": "English, Spanish, Finnish\n\nQuestion: What is the main goal of their work?\nAnswer: To analyze the effect of both human and machine translation in cross-lingual models.\n\nQuestion: What is the main reason for the improvements in their results?\nAnswer: The loss of performance when generalizing from original to translated data.\n\nQuestion: What is the main reason for the improvements in their results?\nAnswer: The loss of performance when generalizing from original to translated data.\n\nQuestion: What is the main reason for the improvements in their results?\nAnswer: The loss of performance when general", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": "They test their method on the task of predicting hashtags for a held-out set of posts.\n\nQuestion: What is the name of the model that they propose?\nAnswer: tweet2vec\n\nQuestion: What is the name of the model that they propose?\nAnswer: tweet2vec\n\nQuestion: What is the name of the model that they propose?\nAnswer: tweet2vec\n\nQuestion: What is the name of the model that they propose?\nAnswer: tweet2vec\n\nQuestion: What is the name of the model that they propose?\nAnswer:", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "Yes, they use pretrained embeddings.\n\nQuestion: What is the vocabulary size?\nAnswer: 400K words.\n\nQuestion: What is the number of unique words in the corpus?\nAnswer: 400K words.\n\nQuestion: What is the number of examples in the French and German datasets?\nAnswer: 170K and 50K.\n\nQuestion: What is the number of unique words in the corpus?\nAnswer: 400K words.\n\nQuestion: What is the number of examples in", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "Yes, it was evaluated against a baseline that used a traditional modular system.\n\nQuestion: What is the main purpose of task-oriented dialogue systems?\nAnswer: The main purpose of task-oriented dialogue systems is to assist the users in accomplishing a well-defined task such as flight booking, tourist information, restaurant search, or booking a taxi.\n\nQuestion: What is the main challenge of task-oriented dialogue systems?\nAnswer: The main challenge of task-oriented dialogue systems is the need to collect domain-specific data labelled with explicit semantic", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": "By using the distribution of the individual words in a category, they can compile distributions for the entire category, and therefore generate maps for these word categories.\n\nQuestion: What is the name of the tool that can be used to generate maps?\nAnswer: The tool is called \"Web Demonstration\".\n\nQuestion: What is the name of the dataset that they used to generate the maps?\nAnswer: The dataset is called \"Blog Collection\".\n\nQuestion: What is the name of the tool that can be used to generate maps?\nAnswer: The tool is called \"Web Demonstration\".\n", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": "The ML methods aim to identify argument components such as premises, backing, and rebuttal.\n\nQuestion: What is the main limitation of the current argumentation mining field?\nAnswer: The main limitation of the current argumentation mining field is the lack of an ultimate argumentation theory.\n\nQuestion: What is the main focus of the current article?\nAnswer: The main focus of the current article is to push the boundaries of the argumentation mining field by focusing on several novel aspects.\n\nQuestion: What is the main research question in the introduction?\nAnswer: What is the main", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "n-grams of order 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": "1,873 Twitter conversation threads, roughly 14k tweets.\n\nQuestion: What is the main conclusion of the analysis?\n\nAnswer: OSG conversations satisfy higher number of conditions approximating therapeutic factors than Twitter conversations.\n\nQuestion: What is the main limitation of the proposed methodology?\n\nAnswer: The proposed methodology is an approximation of the tedious tasks of annotation of conversations by experts versed in the therapeutic factors and their associated theories.\n\nQuestion: What is the main contribution of the proposed methodology?\n\n", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": "The 12 languages covered are Welsh, Kiswahili, Yue Chinese, Finnish, Spanish, Mandarin Chinese, English, Polish, Russian, Estonian, and French.\n\nQuestion: What is the main goal of the Multi-SimLex resource?\n\nAnswer: The main goal of the Multi-SimLex resource is to create a hugely valuable, large-scale semantic resource for multilingual NLP research.\n\nQuestion: What is the main finding of the results in Table TABREF43?\n\nAnswer: The results in Table TABREF4", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": "Wikipedia data and Reddit CMV data\n\nQuestion: What is the main limitation of the current analysis?\nAnswer: It relies on balanced datasets, while derailment is a relatively rare event for which a more restrictive trigger threshold would be appropriate.\n\nQuestion: What is the main motivation behind the design of the model?\nAnswer: The model is designed to capture inter-comment dependencies and process comments as they happen.\n\nQuestion: What is the main finding of the analysis of the early warning provided by the model?\nAnswer: The model provides early warning of derailment, with an", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "No, the pipeline components were not based on deep learning models.\n\nQuestion: What is the main claim of the paper?\nAnswer: The main claim of the paper is that the proposed system can be used as a base tool for information extraction for the Portuguese language.\n\nQuestion: What is the main contribution of the paper?\nAnswer: The main contribution of the paper is the development of an ontology for the criminal law domain, the alignment of the Eurovoc thesaurus and IATE terminology with the ontology created, and the representation of the extracted events from texts in the linked knowledge base defined", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": "The quality of the data is empirically evaluated by measuring the perplexity of the translations using a language model trained on a large amount of clean monolingual data.\n\nQuestion: What is the name of the language model used to evaluate the quality of the translations? \nAnswer: The language model used to evaluate the quality of the translations is not explicitly mentioned in the article, but it is likely a state-of-the-art system.\n\nQuestion: What is the name of the corpus used to create the evaluation set? \nAnswer: The name of the corpus used to", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": "They use two RNNs to encode data from the audio signal and textual inputs independently. The audio-RNN encodes MFCC features from the audio signal using equation EQREF2 . The last hidden state of the audio-RNN is concatenated with the prosodic features to form the final vector representation INLINEFORM0 , and this vector is then passed through a fully connected neural network layer to form the audio encoding vector A. On the other hand, the text-RNN encodes the word sequence of the transcript using equation EQREF2 . The final hidden states of the text-RNN", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": "2.11 BLEU, 1.7 FKGL and 1.07 SARI\n\nQuestion: what is the name of the method used to create the synthetic parallel training set?\nAnswer: back-translation\n\nQuestion: what is the name of the model used in the experiments?\nAnswer: NMT\n\nQuestion: what is the name of the dataset used in the experiments?\nAnswer: WikiLarge\n\nQuestion: what is the name of the dataset used in the experiments?\nAnswer: WikiSmall\n\nQuestion: what is the name of", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": "unanswerable\n\nQuestion: what is the main limitation of the DocRepair model?\nAnswer: it requires monolingual document-level data\n\nQuestion: what is the main novelty of the DocRepair model?\nAnswer: it operates on groups of sentences and fixes consistency errors caused by the context-agnostic baseline MT system\n\nQuestion: what is the main difference between the DocRepair model and previous work on automatic post-editing?\nAnswer: the DocRepair model operates on groups of sentences and is thus able to fix consistency errors caused by the context-", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": "Tweets that were retweeted more than 1000 times by the 8th of November 2016.\n\nQuestion: What is the main goal of this study?\nAnswer: To provide a preliminary characterization of fake news in Twitter by looking into meta-data embedded in tweets.\n\nQuestion: What is the main finding of this study?\nAnswer: There are specific pieces of meta-data about tweets that may allow the identification of fake news.\n\nQuestion: What is the main hypothesis of this study?\nAnswer: There are specific pieces of meta-data", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": "LSTM-CRF\n\nQuestion: Which basic neural architecture perform best by itself?\nAnswer: LSTM-CRF\n\nQuestion: Which basic neural architecture perform best by itself?\nAnswer: LSTM-CRF\n\nQuestion: Which basic neural architecture perform best by itself?\nAnswer: LSTM-CRF\n\nQuestion: Which basic neural architecture perform best by itself?\nAnswer: LSTM-CRF\n\nQuestion: Which basic neural architecture perform best by itself?\nAnswer: LSTM-CRF\n\nQuestion: Which basic neural architecture perform best by itself", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": "The data was collected using crowdsourcing.\n\nQuestion: what is the main goal of the DeepMine project?\nAnswer: The main goal of the DeepMine project was to collect speech from at least a few thousand speakers, enabling research and development of deep learning methods.\n\nQuestion: what is the main goal of the DeepMine project?\nAnswer: The main goal of the DeepMine project was to collect speech from at least a few thousand speakers, enabling research and development of deep learning methods.\n\nQuestion: what is the main goal of the DeepMine project?", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": "Machine learning and deep learning methods are used for RQE, including Logistic Regression, Recurrent Neural Networks (RNNs), Long Short Term Memory cells (LSTMs), and Convolutional Neural Networks (CNNs).\n\nQuestion: What is the definition of RQE?\n\nAnswer: RQE is defined as: a question entails a question if every answer to INLINEFORM2 is also a correct answer to INLINEFORM3 .\n\nQuestion: What is the purpose of RQE?\n\nAnswer: The purpose of RQE is to", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "The benchmark dataset is the Social Honeypot dataset, and its quality is high.\n\nQuestion: What is the difference between the red bars and green bars in Figure 1?\nAnswer: The red bars represent spammers, and the green bars represent legitimate users.\n\nQuestion: What is the difference between the red bars and green bars in Figure 1?\nAnswer: The red bars represent spammers, and the green bars represent legitimate users.\n\nQuestion: What is the difference between the red bars and green bars in Figure 1?\nAnswer: The red bars represent spammers", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": "The decoder has an LSTM encoder.\n\nQuestion: What is the main objective of the baseline system?\nAnswer: The main objective of the baseline system is to generate an inflected word form given its lemma and the context in which it occurs.\n\nQuestion: What is the main objective of the auxiliary objective?\nAnswer: The auxiliary objective is to predict the MSD tag of the target form.\n\nQuestion: What is the effect of encoding the full context with an LSTM?\nAnswer: Encoding the full context with an LSTM increases the variance of the", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "unanswerable\n\nQuestion: What is the name of the dataset used in the experiments?\n\nAnswer: FSD dataset\n\nQuestion: What is the name of the algorithm used in the experiments?\n\nAnswer: K-means\n\nQuestion: What is the name of the algorithm used in the experiments?\n\nAnswer: LEM\n\nQuestion: What is the name of the algorithm used in the experiments?\n\nAnswer: DPEMM\n\nQuestion: What is the name of the algorithm used in the experiments?\n\nAnswer: Twitter dataset\n\nQuestion: What is the name of the algorithm", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": "The best performing model is the one that uses BERT and is ranked 3rd in FLC task.\n\nQuestion: What is the best performing model among author's submissions, what performance it had?\nAnswer: The best performing model is the one that uses BERT and is ranked 3rd in FLC task.\n\nQuestion: What is the best performing model among author's submissions, what performance it had?\nAnswer: The best performing model is the one that uses BERT and is ranked 3rd in FLC task.\n\nQuestion: What is the best performing model among author", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "The baseline was a weak model trained only on in-domain data.\n\nQuestion: what was the best model?\nAnswer: The best model was the M2M Transformer NMT model (b3).\n\nQuestion: what was the best result?\nAnswer: The best result was a BLEU score of 21.5 for Ja INLINEFORM1 Ru.\n\nQuestion: what was the best result for the other translation directions?\nAnswer: The best result for the other translation directions was a BLEU score of 10.\n\nQuestion: what was the best result for", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "0.7033\n\nQuestion: What was their lowest recall score?\n\nAnswer: 0.1119\n\nQuestion: What was their highest F1 score?\n\nAnswer: 0.6103\n\nQuestion: What was their lowest F1 score?\n\nAnswer: 0.4325\n\nQuestion: What was their highest MRR score?\n\nAnswer: 0.6103\n\nQuestion: What was their lowest MRR score?\n\nAnswer: 0.4325\n\nQuestion: What was their", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "Word embedding techniques such as word2vec.\n\nQuestion: What is the goal of the paper's approach?\nAnswer: To reduce the amount of noise in a second–order co–occurrence vector.\n\nQuestion: What is the main hypothesis of the paper?\nAnswer: That integrating pairwise similarity scores into second–order vectors will reduce the amount of noise and improve correlation with human judgments.\n\nQuestion: What is the main difference between the paper's approach and previous work?\nAnswer: The paper's approach restricts the context used by the vector measure to words that exist in", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "They match words by swapping the position of the noun phrase followed by a transitive verb with the transitive verb.\n\nQuestion: What is the main hypothesis of the paper?\nAnswer: The main hypothesis is that word-order divergence can limit the benefits of multilingual translation.\n\nQuestion: What is the main limitation of the BIBREF3 approach?\nAnswer: The limitation of BIBREF3 approach is that they ignore the lexical similarity between languages and also the source language embeddings are randomly initialized.\n\nQuestion: What is the main limitation of the BIBREF10 approach", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": "No, the paper does not explore extraction from electronic health records.\n\nQuestion: What is the main problem in BioIE?\nAnswer: The main problems in BioIE are similar to those in Information Extraction:\n\nQuestion: What is the state of biomedical text mining?\nAnswer: The state of biomedical text mining is reviewed regularly.\n\nQuestion: What is the main challenge in NER in the biomedical domain?\nAnswer: Some of the earliest systems were heavily dependent on hand-crafted features.\n\nQuestion: What is the main challenge in relation extraction", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": "The experts used for annotation were seven experts with legal training.\n\nQuestion: What is the goal of this work?\nAnswer: The goal of this work is to promote question-answering research in the specialized privacy domain, where it can have large real-world impact.\n\nQuestion: What is the main limitation of the PrivacyQA dataset?\nAnswer: The main limitation of the PrivacyQA dataset is that it does not include policies from well-known applications, which are likely to have carefully-constructed privacy policies.\n\nQuestion: What is the main challenge in answering questions", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "The painting embedding model is a CNN-RNN generative model, and the language style transfer model is a sequence-to-sequence model with parallel text corpus.\n\nQuestion: What is the average content score for the Shakespearean prose generated for the input paintings?\nAnswer: The average content score is 3.7.\n\nQuestion: What is the average creativity score for the Shakespearean prose generated for the input paintings?\nAnswer: The average creativity score is 3.9.\n\nQuestion: What is the average style score for the Shakespearean prose generated for the input", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": "ToBERT outperforms RoBERT on pre-trained BERT features and fine-tuned BERT features for all our tasks.\n\nQuestion: What is the name of the dataset used for topic identification?\nAnswer: Fisher Phase 1 US English corpus\n\nQuestion: What is the name of the dataset used for customer satisfaction prediction?\nAnswer: CSAT dataset\n\nQuestion: What is the name of the dataset used for topic identification?\nAnswer: 20 newsgroups\n\nQuestion: What is the name of the dataset used for customer satisfaction prediction?\nAnswer: Fish", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": "No, the authors do not hypothesize that humans' robustness to noise is due to their general knowledge.\n\nQuestion: What is the name of the data enrichment method used in the paper?\nAnswer: The data enrichment method is named as WordNet-based data enrichment method.\n\nQuestion: What is the name of the MRC model proposed in the paper?\nAnswer: The MRC model is named as Knowledge Aided Reader (KAR).\n\nQuestion: What is the name of the hyper-parameter used in the data enrichment method?\nAnswer:", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "They addressed three topics of cyberbullying: personal attack, racism, and sexism.\n\nQuestion: What is the name of the dataset used for detecting cyberbullying in social media?\nAnswer: Formspring\n\nQuestion: What is the name of the dataset used for detecting cyberbullying in social media?\nAnswer: Twitter\n\nQuestion: What is the name of the dataset used for detecting cyberbullying in social media?\nAnswer: Wikipedia\n\nQuestion: What is the name of the dataset used for detecting cyberbullying in social media?\nAnswer:", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "The contexts are split into three disjoint regions based on the two relation arguments: the left context, the middle context and the right context. Since in most cases the middle context contains the most relevant information for the relation, we want to focus on it but not ignore the other regions completely. Hence, we propose to use two contexts: (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. Due to the repetition of the middle context, we force the network to pay special attention to it. The", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "3 (PER, LOC, ORG)\n\nQuestion: What is the name of the dataset used in the experiments?\nAnswer: OurNepali dataset\n\nQuestion: What is the name of the dataset received from ILPRL lab?\nAnswer: ILPRL dataset\n\nQuestion: What is the name of the dataset used in the experiments?\nAnswer: OurNepali dataset\n\nQuestion: What is the name of the dataset received from ILPRL lab?\nAnswer: ILPRL dataset\n\nQuestion: What is the name of the dataset used in the experiments?\nAnswer:", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": "The resulting annotated data is higher quality than the data collected by crowd workers.\n\nQuestion: What is the name of the model used to predict annotation difficulty?\nAnswer: The model used to predict annotation difficulty is the LSTM-CRF-Pattern model.\n\nQuestion: What is the name of the model used to predict annotation difficulty?\nAnswer: The model used to predict annotation difficulty is the LSTM-CRF-Pattern model.\n\nQuestion: What is the name of the model used to predict annotation difficulty?\nAnswer: The model used to predict annotation difficulty is the LSTM-CR", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": "65% of the speakers are men, speaking more than 75% of the time.\n\nQuestion: What is the proportion of men and women in French radio and TV media data?\n\nAnswer: 65% of the speakers are men, speaking more than 75% of the time.\n\nQuestion: What is the impact of the observed disparity on ASR performance?\n\nAnswer: We conclude that gender is clearly a factor of variation in ASR performance, with a WER increase of 24% for women compared to men, exhibiting a clear gender bias.", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "English-German dataset\n\nQuestion: What is the main metric used to evaluate the performance of the models?\nAnswer: Meteor\n\nQuestion: What is the main difference between the deliberation models and the transformer models enriched with image information?\nAnswer: The deliberation models lead to significant improvements over the vanilla deliberation performance (del).\n\nQuestion: What is the main difference between the deliberation models and the transformer models enriched with image information?\nAnswer: The deliberation models lead to significant improvements over the vanilla deliberation performance (del).\n\nQuestion: What is the", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": "The question is answered in the article, but it is not explicitly stated in the question. The answer is \"strong baselines model\".\n\nQuestion: What is the name of the dataset used for evaluation?\nAnswer: The question is answered in the article, but it is not explicitly stated in the question. The answer is \"SIGHAN Bakeoff 2005\".\n\nQuestion: What is the name of the model used for evaluation?\nAnswer: The question is answered in the article, but it is not explicitly stated in the question. The answer is \"strong baselines model\".\n\nQuestion", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "Logistic Regression (LR) and Multilayer Perceptron (MLP)\n\nQuestion: What is the main goal of the experiment?\nAnswer: To demonstrate the effectiveness of our approach as a new model training technique\n\nQuestion: What is the main difference between the metrics used for accuracy and AUC?\nAnswer: Accuracy is dominated by negative examples; AUC, in comparison, better characterizes the discriminative power of the model.\n\nQuestion: What is the main reason for the performance degradation of the models when further trained for more iterations?\nAnswer: The newly", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "NLTK, Stanford CoreNLP, TwitterNLP, and CogComp-NLP.\n\nQuestion: What is the sentiment analysis task?\nAnswer: The task of entity-level sentiment analysis is difficult for existing tools to answer accurately while the recognition of the entity, here, which politician, was easier.\n\nQuestion: What is the sentiment analysis task?\nAnswer: The task of entity-level sentiment analysis is difficult for existing tools to answer accurately while the recognition of the entity, here, which politician, was easier.\n\nQuestion: What is the sentiment analysis task?\nAnswer:", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": "The SQuAD dataset is used for experiments.\n\nQuestion: What is the name of the tool used to extract structured answer-relevant relations?\n\nAnswer: OpenIE toolbox\n\nQuestion: What is the name of the tool used to extract structured answer-relevant relations?\n\nAnswer: OpenIE toolbox\n\nQuestion: What is the name of the tool used to extract structured answer-relevant relations?\n\nAnswer: OpenIE toolbox\n\nQuestion: What is the name of the tool used to extract structured answer-relevant relations?\n\nAnswer: OpenIE", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "The use of Flickr for modelling urban environments has already received considerable attention. For instance, various approaches have been proposed for modelling urban regions BIBREF0 , and for identifying points-of-interest BIBREF1 and itineraries BIBREF2 , BIBREF3 . However, the usefulness of Flickr for characterizing the natural environment, which is the focus of this paper, is less well-understood.\n\nQuestion: what is the main hypothesis?\nAnswer: Our main hypothesis in this paper is that by using vector space embeddings instead of bag-of-", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": "Yes, they use attention in the contextual encoding layer.\n\nQuestion: What is the name of the model?\nAnswer: Joint SAN\n\nQuestion: What is the name of the dataset?\nAnswer: SQuAD 2.0\n\nQuestion: What is the name of the classifier?\nAnswer: Unanswerable classifier\n\nQuestion: What is the name of the objective function?\nAnswer: Objective function\n\nQuestion: What is the name of the evaluation metric?\nAnswer: Exact Match (EM) and Macro-averaged F1 score (F1)", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": "CSAT dataset for CSAT prediction, consisting of spoken transcripts (automatic via ASR). 20 newsgroups for topic identification task, consisting of written text; Fisher Phase 1 corpus for topic identification task, consisting of spoken transcripts (manual);\n\nQuestion: What is the name of the dataset used for topic identification?\nAnswer: Fisher Phase 1 US English corpus\n\nQuestion: What is the name of the dataset used for customer satisfaction prediction?\nAnswer: CSAT dataset for CSAT prediction, consisting of spoken transcripts (automatic via ASR).", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": "IMDb movie review dataset\n\nQuestion: What is the number of units in each layer of the QRNN model for language modeling?\nAnswer: 640 units\n\nQuestion: What is the number of units in each layer of the QRNN model for character-level machine translation?\nAnswer: 320 units\n\nQuestion: What is the number of units in each layer of the QRNN model for document-level sentiment classification?\nAnswer: 256 units\n\nQuestion: What is the number of units in each layer of the QRNN model for language modeling?", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": "Yes, the BERT models are evaluated on the same tasks as in previous work, but with different stimuli and evaluation protocols.\n\nQuestion: What is the main difference between the BERT model and the LSTM model?\nAnswer: The BERT model relies purely on attention mechanisms, while the LSTM model models word order directly.\n\nQuestion: What is the main difference between the BERT model and the LSTM model?\nAnswer: The BERT model relies purely on attention mechanisms, while the LSTM model models word order directly.\n\nQuestion: What is the", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": "No, the dataset is not balanced.\n\nQuestion: What is the CCR for crowdworkers in sentiment analysis?\nAnswer: 74.7%\n\nQuestion: What is the CCR for Google Cloud in sentiment analysis?\nAnswer: 3%\n\nQuestion: What is the CCR for TensiStrength in sentiment analysis?\nAnswer: 10.5%\n\nQuestion: What is the CCR for Rosette Text Analytics in sentiment analysis?\nAnswer: 22.7%\n\nQuestion: What is the CCR for crowdworkers in named", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": "The invertibility condition is that the neural projector must be able to reverse the projection, which means it must be able to calculate the inverse of the projection.\n\nQuestion: What is the Jacobian regularization term?\nAnswer: The Jacobian regularization term is a non-zero and differentiable term that prevents information loss during optimization. It is equivalent to preventing the trivial solution of always projecting data to the same single point to maximize likelihood.\n\nQuestion: What is the purpose of the invertible transformation proposed by BIBREF16?\nAnswer: The invertible transformation proposed by BIBREF", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "The proposed qualitative annotation schema is described in Appendix .\n\nQuestion: What is the main goal of the proposed framework?\nAnswer: The main goal of the proposed framework is to characterise machine reading comprehension gold standards.\n\nQuestion: What is the purpose of the framework?\nAnswer: The purpose of the framework is to compare different gold standards, consider the design choices for a new gold standard, and perform qualitative error analyses for a proposed approach.\n\nQuestion: What are the limitations of the framework?\nAnswer: The limitations of the framework include the lack of a quantitative analysis and the focus", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": "11.6M words\n\nQuestion: what is the size of the vocabulary?\nAnswer: 82K\n\nQuestion: what is the size of the training set?\nAnswer: 89,042 sentence pairs\n\nQuestion: what is the size of the test set?\nAnswer: 100 pairs\n\nQuestion: what is the size of the development set?\nAnswer: 2,000 sentences\n\nQuestion: what is the size of the testing set?\nAnswer: 359 sentences\n\nQuestion: what is the size of", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": "The baselines are the methods that are compared to the proposed method.\n\nQuestion: What is the main contribution of the paper?\nAnswer: The main contribution of the paper is to propose a method that can re-use every sub-net and keep the role of sub-net consistent between pre-training and fine-tuning.\n\nQuestion: What is the role of the text encoder in the proposed method?\nAnswer: The text encoder is used to extract high-level linguistic features from lower level subword representations.\n\nQuestion: What is the role of the speech encoder in the proposed method", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": "English\n\nQuestion: What is the main focus of this paper?\nAnswer: Class imbalance and cost-sensitivity\n\nQuestion: What is the main focus of this paper?\nAnswer: Class imbalance and cost-sensitivity\n\nQuestion: What is the main focus of this paper?\nAnswer: Class imbalance and cost-sensitivity\n\nQuestion: What is the main focus of this paper?\nAnswer: Class imbalance and cost-sensitivity\n\nQuestion: What is the main focus of this paper?\nAnswer: Class imbalance and cost-sensitivity\n\n", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": "The models used in the experiment are a linear SVM trained on word unigrams, a bidirectional Long Short-Term-Memory (BiLSTM) model, and a Convolutional Neural Network (CNN) model.\n\nQuestion: What is the main goal of the dataset?\nAnswer: The main goal of the dataset is to provide a resource for researchers to study the type and target of offensive language in social media.\n\nQuestion: What is the breakdown of the data into training and testing for the labels from each level?\nAnswer: The breakdown of the data into training", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "Yes, the answered questions measure for the usefulness of the answer.\n\nQuestion: What is the purpose of the question?\n\nAnswer: The purpose of the question is to understand the answerability of questions on Quora.\n\nQuestion: What is the main goal of the study?\n\nAnswer: The main goal of the study is to identify the open questions and take measures based on the types - poor quality questions can be removed from Quora and the good quality questions can be promoted so that they get more visibility and are eventually routed to topical experts for better answers.\n\nQuestion: What is the", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "GloVe and Edinburgh embeddings\n\nQuestion: what is the name of the system that was used to analyze the data?\nAnswer: EmoInt\n\nQuestion: what is the name of the system that was used to analyze the data?\nAnswer: EmoInt\n\nQuestion: what is the name of the system that was used to analyze the data?\nAnswer: EmoInt\n\nQuestion: what is the name of the system that was used to analyze the data?\nAnswer: EmoInt\n\nQuestion: what is the name of the system that was used to analyze the data?\n", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": "The results showed that personalized models outperformed the baseline in BPE perplexity, with the Prior Name model performing the best. They also found that personalized models generated more diverse recipes than the baseline, with more key entities present in the recipe.\n\nQuestion: What was the name of the dataset used in the study?\nAnswer: The dataset used in the study was called Food.com: Dataset Details.\n\nQuestion: What was the name of the model that was used as a baseline?\nAnswer: The baseline model was called Enc-Dec.\n\nQuestion: What", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": "A combination of rewards for reinforcement learning is a set of rewards that are used to control the behavior of a reinforcement learning agent.\n\nQuestion: What is the main reason for the issue of no change in the output sentence?\n\nAnswer: The main reason for the issue of no change in the output sentence is that rewards for content preservation are too prominent and rewards for style accuracy cannot work well.\n\nQuestion: What is the conclusion of the automatic evaluation results of the transformation from non-ironic sentences to ironic sentences?\n\nAnswer: The conclusion of the automatic evaluation results of", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": "The authors demonstrate that their model does not work well with Shakespeare style transfer for \"Starry Night\" because the style transfer dataset does not have similar words in the training set of sentences.\n\nQuestion: What is the name of the dataset used for training the Shakespearean prose model?\nAnswer: MultiM-Poem(Ex)BIBREF1\n\nQuestion: What is the name of the dataset used for training the Shakespearean prose model?\nAnswer: MultiM-Poem(Ex)BIBREF1\n\nQuestion: What is the name of the dataset used for training the Shakespearean prose", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": "The Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset.\n\nQuestion: What is the name of the dataset used for development?\nAnswer: Affective development\n\nQuestion: What is the name of the dataset used for evaluation?\nAnswer: Affective test\n\nQuestion: What is the name of the dataset used for evaluation?\nAnswer: Fairy Tales dataset\n\nQuestion: What is the name of the dataset used for evaluation?\nAnswer: ISEAR dataset\n\nQuestion: What is the name of the dataset used for evaluation?\nAnswer:", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": "The distribution results showed that the number of retweets was not statistically different between viral tweets containing fake news and viral tweets not containing fake news.\n\nQuestion: What was the main goal of the study?\nAnswer: The main goal of the study was to characterize fake news in Twitter by looking into meta-data embedded in tweets.\n\nQuestion: What was the main finding of the study?\nAnswer: The main finding of the study was that there are specific pieces of meta-data about tweets that may allow the identification of fake news.\n\nQuestion: What was the main hypothesis", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": "The dataset of hashtags is sourced from the Stanford Sentiment Analysis Dataset BIBREF36 .\n\nQuestion: What is the name of the model used for hashtag segmentation?\nAnswer: The model used for hashtag segmentation is called HashtagMaster.\n\nQuestion: What is the name of the dataset used for sentiment analysis?\nAnswer: The dataset used for sentiment analysis is the Sentiment Analysis in Twitter shared task (subtask A) at SemEval 2017 BIBREF41 .\n\nQuestion: What is the name of the model", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": "The corpus contains accents from different regions of Iran.\n\nQuestion: what is the gender distribution of the speakers?\nAnswer: The corpus contains 1969 respondents, with 1149 of them being male and 820 female.\n\nQuestion: what is the number of unique phrases in each part of the database?\nAnswer: The number of unique phrases in each part of the database is not explicitly stated in the article, but it is mentioned that the text-dependent part contains 190 thousand utterances with full transcription and 10 thousand with", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "Word subspace can represent sets of word vectors, retaining most of the variability of features.\n\nQuestion: What is the primary goal of the experiment in the evaluation of the word subspace representation?\nAnswer: The primary goal is to visualize how much of the text data can be represented by a lower dimensional subspace.\n\nQuestion: What is the accuracy rate of the simplest baseline, SA with w2v?\nAnswer: The accuracy rate of the simplest baseline, SA with w2v, is 78.73%.\n\nQuestion: What is the accuracy rate of LSA with", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": "Random Forests (RF)\n\nQuestion: What is the overall performance of INLINEFORM0 and comparison to baselines?\nAnswer: INLINEFORM0 significantly outperforms the baseline B1.\n\nQuestion: What is the impact of the individual feature groups that contribute to the superior performance in comparison to the baselines?\nAnswer: The performance of INLINEFORM0 with relative entity frequency from the salience feature group is close to that of all the features combined.\n\nQuestion: What is the evaluation setup for ASP task?\nAnswer: We evaluate the generated supervised models for the two tasks", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": "Yes, SemCor3.0 is a large corpus manually annotated with WordNet sense for WSD.\n\nQuestion: What is the number of Transformer blocks in the BERT$_\\mathrm {BASE}$ model?\nAnswer: 12\n\nQuestion: What is the number of the hidden layer in the BERT$_\\mathrm {BASE}$ model?\nAnswer: 768\n\nQuestion: What is the number of self-attention heads in the BERT$_\\mathrm {BASE}$ model?\nAnswer: 12\n\nQuestion: What is the total number of parameters of", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": "11,000 speakers and over 60 accents.\n\nQuestion: How many languages are in CoVoST?\nAnswer: 11 languages.\n\nQuestion: How many hours of speech are in CoVoST?\nAnswer: 708 hours.\n\nQuestion: How many hours of speech are in Tatoeba?\nAnswer: 9.3 hours.\n\nQuestion: How many hours of speech are in CoVoST?\nAnswer: 708 hours.\n\nQuestion: How many hours of speech are in Tatoeb", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": "The dataset for fine-grained classification is split in training, development, development_test and test parts.\n\nQuestion: What is the primary task?\nAnswer: The fine-grained task is the primary task.\n\nQuestion: What is the primary measure?\nAnswer: The macro-averaged Mean Absolute Error (MAE) is the primary measure.\n\nQuestion: What is the secondary measure?\nAnswer: The micro-averaged F1 score is the secondary measure.\n\nQuestion: What is the architecture of the neural network?\nAnswer: The architecture is a biLSTM network", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": "unanswerable\n\nQuestion: Do they use the pre-trained BERT model?\nAnswer: yes\n\nQuestion: Do they use the pre-trained BERT model?\nAnswer: yes\n\nQuestion: Do they use the pre-trained BERT model?\nAnswer: yes\n\nQuestion: Do they use the pre-trained BERT model?\nAnswer: yes\n\nQuestion: Do they use the pre-trained BERT model?\nAnswer: yes\n\nQuestion: Do they use the pre-trained BERT model?\nAnswer: yes\n\nQuestion: Do", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": "Yes, the automatically constructed datasets are subject to quality control.\n\nQuestion: What is the main appeal of using automatically generate datasets?\nAnswer: The main appeal of using automatically generate datasets is the ability to systematically manipulate and control the complexity of target questions, which allows for more controlled experimentation and new forms of evaluation.\n\nQuestion: What is the cluster-based analysis used for?\nAnswer: The cluster-based analysis is used to evaluate how consistent and robust models are across our different probes, and to get insight into whether errors are concentrated on a small set of concepts or widespread across clusters.", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": "Yes, the images are from the ShapeWorld framework.\n\nQuestion: What is the purpose of the GTD evaluation framework?\nAnswer: The GTD evaluation framework is proposed as a supplement to standard image captioning evaluation, focusing on grammaticality, truthfulness, and diversity.\n\nQuestion: What is the primary motivation for developing the ShapeWorldICE evaluation suite?\nAnswer: The primary motivation is to reduce complexity, enable better control over the data, and provide more detailed insights into strengths and limitations of existing models.\n\nQuestion: What is the main difference", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": "The average f-score is reported as micro-average, to better account for the skewed distribution of the classes as well as in accordance to what is usually reported for this task.\n\nQuestion: What is the name of the dataset used for the evaluation of emotion detection?\nAnswer: Affective Text dataset\n\nQuestion: What is the name of the dataset used for the evaluation of emotion detection?\nAnswer: Fairy Tales dataset\n\nQuestion: What is the name of the dataset used for the evaluation of emotion detection?\nAnswer: ISEAR\n\nQuestion: What is the", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": "The INLINEFORM0 tag indicates that the current word appears before the pun in the given context. The INLINEFORM0 tag highlights the current word is a pun. The INLINEFORM0 tag indicates that the current word appears after the pun.\n\nQuestion: What is the INLINEFORM0 tagging scheme?\nAnswer: The INLINEFORM0 tag indicates that the current word appears before the pun in the given context. The INLINEFORM0 tag highlights the current word is a pun. The INLINEFORM0 tag indicates that the current word appears after the pun.\n\nQuestion: What is the IN", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": "No, Arabic is not one of the 11 languages in CoVost.\n\nQuestion: What is the name of the corpus that CoVost is based on?\nAnswer: Common Voice (CoVo)\n\nQuestion: What is the license of CoVost?\nAnswer: CoVost is released under CC0 license and free to use.\n\nQuestion: What is the name of the language that has the worst $\\textrm {CoefVar}_{MS}$?\nAnswer: German and Persian have the worst $\\textrm {CoefVar}_{MS}$ (least stable)", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": "The robustness of a model is defined as the ability of the model to handle bias in the prior knowledge.\n\nQuestion: What is the main contribution of this work?\nAnswer: The main contribution of this work is to reveal the factors of reducing the sensibility of the prior knowledge and therefore to make the model more robust and practical.\n\nQuestion: What is the purpose of the regularization terms?\nAnswer: The regularization terms are introduced to address the problem of robustness in learning models.\n\nQuestion: What is the difference between maximum entropy and neutral features?\nAnswer: Maximum entropy assumes that the", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": "tf-idf, average GloVe embeddings, InferSent, and Universal Sentence Encoder.\n\nQuestion: What is the default pooling strategy for SBERT?\nAnswer: MEAN.\n\nQuestion: What is the purpose of SentEval?\nAnswer: To evaluate the quality of sentence embeddings.\n\nQuestion: What is the average performance of SBERT on SentEval?\nAnswer: 2 percentage points better than InferSent and Universal Sentence Encoder.\n\nQuestion: What is the computational efficiency of SBERT compared to other sentence", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "The proposed method improves the F1 score by +0.29 and +0.96 for English datasets, and +0.97 and +2.36 for Chinese datasets.\n\nQuestion: What is the name of the dataset used for MRC task?\nAnswer: SQuAD v1.1 and SQuAD v2.0 are the most widely used QA benchmarks.\n\nQuestion: What is the name of the dataset used for PI task?\nAnswer: MRPC is a corpus of sentence pairs automatically extracted from online news sources, with human annotations of whether the", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": "They test their conflict method on two tasks: Quora Duplicate Question Pair Detection and Ranking questions in Bing's People Also Ask.\n\nQuestion: What is the main objective of any attentive or alignment process?\nAnswer: The main objective of any attentive or alignment process is to look for matching words or phrases between two sequences and assign a high weight to the most similar pairs and vice-versa.\n\nQuestion: What is the main objective of any attentive or alignment process?\nAnswer: The main objective of any attentive or alignment process is to look for", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": "They compared against the following baselines:\n\n1. TG-RNN: A model employing different composition functions according to POS tags.\n2. TE-RNN/TE-RNTN: Models which leverage tag embeddings as additional inputs for the existing tree-structured models.\n3. DC-TreeLSTM: A model dynamically creating the parameters for compositional functions in a tree-LSTM.\n4. Tree-LSTM: A standard tree-LSTM model.\n5. SPINN: A shift-reduce algorithm for tree-L", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": "The core component for KBQA is the improved relation detection model, HR-BiLSTM.\n\nQuestion: What is the main focus of this work?\nAnswer: The main focus of this work is to improve the relation detection subtask and further explore how it can contribute to the KBQA system.\n\nQuestion: What is the main difference between the proposed model and previous methods?\nAnswer: The main difference is that the proposed model performs hierarchical matching between questions and KB relations, while previous methods do not.\n\nQuestion: What is the main hypothesis about the training of the", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "The baseline models are the Encoder-Decoder model and the Nearest-Neighbor model.\n\nQuestion: What is the purpose of the attention fusion layer?\n\nAnswer: The attention fusion layer is used to fuse all contexts calculated at time $t$, concatenating them with decoder GRU output and previous token embedding.\n\nQuestion: What is the purpose of the attention-score function $\\alpha$?\n\nAnswer: The attention-score function $\\alpha$ is used to calculate the ingredient context $\\mathbf {a}_{t}^{i} \\in \\mathbb {R}^{d_", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": "The methods to find examples of biases and unwarranted inferences are manual detection, part-of-speech information, and Louvain clustering.\n\nQuestion: What is the main goal of this paper??\nAnswer: The main goal of this paper is to give an overview of linguistic bias and unwarranted inferences resulting from stereotypes and prejudices.\n\nQuestion: What is the main difference between linguistic bias and unwarranted inferences??\nAnswer: The main difference between linguistic bias and unwarranted inferences is that linguistic bias is a systematic asym", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": "French, Spanish, Italian, Portuguese, German, and Arabic.\n\nQuestion: What is the name of the challenge?\nAnswer: Winograd Schema Challenge\n\nQuestion: What is the name of the program that is being challenged?\nAnswer: AI programs\n\nQuestion: What is the name of the website that hosts the challenge?\nAnswer: commonsensereasoning.org\n\nQuestion: What is the name of the sponsor of the challenge?\nAnswer: Nuance Inc.\n\nQuestion: What is the name of the first year the challenge was offered?\nAnswer:", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": "They experimented with models that use plain stacked LSTMs, models with different INLINEFORM0 , models without INLINEFORM1 , and models that integrate lower contexts via peephole connections.\n\nQuestion: What is the name of the dataset used for evaluating the performance of the proposed method on the NLI task?\nAnswer: SNLI\n\nQuestion: What is the name of the dataset used for evaluating the performance of the proposed method on the PI task?\nAnswer: Quora Question Pairs\n\nQuestion: What is the name of the dataset used for evaluating the performance", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": "unanswerable\n\nQuestion: What is the name of the lexical resource used in the proposed method?\nAnswer: Roget's Thesaurus\n\nQuestion: What is the name of the word embedding algorithm used in the proposed method?\nAnswer: GloVe\n\nQuestion: What is the name of the objective function used in the proposed method?\nAnswer: ( SECREF4 )\n\nQuestion: What is the name of the objective function used in the proposed method?\nAnswer: ( SECREF4 )\n\nQuestion: What is the name of the objective function used in the proposed", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": "The authors experimented with several summarization algorithms, including Sumy's summarization algorithms.\n\nQuestion: What is the purpose of the PA system in the organization?\nAnswer: The PA system enables an organization to periodically measure and evaluate every employee's performance.\n\nQuestion: What is the purpose of the PA process in the organization?\nAnswer: The PA process in any modern organization is nowadays implemented and tracked through an IT system (the PA system) that records the interactions that happen in various steps.\n\nQuestion: What is the purpose of the PA dataset used in this paper?\nAnswer", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": "The previous state of the art was proposed by BIBREF0 .\n\nQuestion: What is the primary problem in this task?\nAnswer: The primary problem is to predict instructor intervention in MOOC forums.\n\nQuestion: What is the secondary problem in this task?\nAnswer: The secondary problem is to infer the appropriate amount of context to intervene.\n\nQuestion: What is the main innovation of the proposed models?\nAnswer: The main innovation is to decompose the intervention prediction problem into a two-stage model that first explicitly tries to discover the proper context to which a potential inter", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": "\"unanswerable\"\n\nQuestion: Which component is the most impactful?\nAnswer: \"yes\"\n\nQuestion: Which component is the least impactful?\nAnswer: \"unanswerable\"\n\nQuestion: Which component is the most impactful?\nAnswer: \"yes\"\n\nQuestion: Which component is the least impactful?\nAnswer: \"unanswerable\"\n\nQuestion: Which component is the most impactful?\nAnswer: \"yes\"\n\nQuestion: Which component is the least impactful?\nAnswer: \"unanswerable\"\n\nQuestion: Which component is the most", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": "The corpus used for the task is DTA18 and DTA19, which are subparts of DTA corpus, a freely available lemmatized, POS-tagged and spelling-normalized diachronic corpus of German containing texts from the 16th to the 20th century.\n\nQuestion: What is the metric used to evaluate the models' performance?\nAnswer: The metric used to evaluate the models' performance is Spearman's $\\rho $.\n\nQuestion: What is the best-performing model in the shared task?\nAnswer: The best", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "Kannada, Hindi, Telugu, Malayalam, Bengali, and English.\n\nQuestion: What is the name of the model that uses ResNet as the base feature extractor?\nAnswer: ResNet-34\n\nQuestion: What is the name of the pooling method that outperforms all the other pooling methods?\nAnswer: GhostVLAD\n\nQuestion: What is the name of the model that uses a 5sec spectrogram?\nAnswer: 5sec spectrogram\n\nQuestion: What is the name of the model that uses a ", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": "The model performance on target language reading comprehension is not provided in the article.\n\nQuestion: What is the effect of machine translation on the model performance?\nAnswer: The effect of machine translation on the model performance is not provided in the article.\n\nQuestion: What is the effect of other factors on the model performance?\nAnswer: The effect of other factors on the model performance is not provided in the article.\n\nQuestion: What is the effect of typology manipulation on the model performance?\nAnswer: The effect of typology manipulation on the model performance is not provided in the article.\n\nQuestion", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": "ALOHA achieves a significant boost in Hits@n/N accuracy and other metrics for retrieving the correct response of five diverse characters with different identities.\n\nQuestion: What is the main contribution of the proposed model?\nAnswer: The proposed model, ALOHA, is able to effectively use HLAs to improve upon dialogue retrieval performance.\n\nQuestion: What is the main limitation of the proposed model?\nAnswer: The proposed model, ALOHA, is not able to model the dialogue counterpart (e.g. the dialogue of other characters speaking to the target character).", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "ARAML performs significantly better than other baselines in all the cases.\n\nQuestion: What is the main difference between ARAML and RAML?\nAnswer: ARAML gets samples from a stationary distribution around real data, while RAML gets samples from the generator's distribution.\n\nQuestion: What is the main difference between ARAML and MaliGAN?\nAnswer: ARAML gets samples from a stationary distribution around real data, while MaliGAN gets samples from the generator's distribution.\n\nQuestion: What is the main difference between ARAML and MLE?", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": "The authors present evidence that the model can capture some biases in data annotation and collection by examining the results of the model's performance on a subset of the data. They found that the model misclassified some tweets as hate or offensive when they were actually neither or offensive, and vice versa. This suggests that the model is not capturing the nuances of the data correctly, and that there may be biases in the data that are not being accounted for.\n\nQuestion: What is the main reason for the high misclassifications of hate samples as offensive in the Davidson dataset?\n", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "Yes, the Word Count Baseline was tested, which counts the number of question words that also appear in a sentence.\n\nQuestion: What is the F1 score of the No-Answer Baseline?\nAnswer: 28\n\nQuestion: What is the F1 score of the best-performing BERT variant?\nAnswer: 39.8\n\nQuestion: What is the percentage of questions that are incomprehensible?\nAnswer: 4.18%\n\nQuestion: What is the percentage of questions that are not questions?\nAnswer: 20%\n\nQuestion", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "The dataset is divided into three parts with 64%, 16% and 20% of the total dataset into training set, development set and test set respectively.\n\nQuestion: What is the name of the dataset released by the authors?\n\nAnswer: The dataset released in our github repository contains each word in newline with space separated POS-tags and Entity-tags. The sentences are separated by empty newline. A sample sentence from the dataset is presented in table FIGREF13.\n\nQuestion: What is the name of the dataset received from ILPRL lab?\n\nAnswer: After much", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": "The proposed DSC loss obtains significant performance boost for both BERT and XLNet. Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP.\n\nQuestion: What is the name of the dataset used for the NER task?\nAnswer: OntoNotes4.0\n\nQuestion: What is the name of the dataset used for the MRC task?\nAnswer: SQuAD v1.1 and SQuAD v2.0\n\nQuestion: What is the name of the dataset used for the P", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "The datasets used are the ERP data from BIBREF0 , the eye-tracking data from BIBREF0 , and the self-paced reading time data from BIBREF0 .\n\nQuestion: What is the purpose of the multitask learning analysis?\nAnswer: The purpose of the multitask learning analysis is to explore how information is shared between different types of data, and to explore how different types of data can be used to predict each other.\n\nQuestion: What is the main finding of the multitask learning analysis?\nAnswer: The main finding of the multitask learning", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": "The dataset consists of 14 participants, with each prompt presented 11 times to each individual.\n\nQuestion: What is the name of the dataset used in the study?\nAnswer: KARA ONE\n\nQuestion: What is the name of the classification layer used in the model?\nAnswer: Extreme Gradient Boost\n\nQuestion: What is the name of the deep learning model used in the model?\nAnswer: Deep Autoencoder\n\nQuestion: What is the name of the first level of hierarchy in the model?\nAnswer: CNN & LSTM\n\nQuestion: What is", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": "Pointer-Gen, Pointer-Gen-RL+ROUGE, Pointer-Gen-RL-SEN, and Pointer-Gen-ARL-SEN.\n\nQuestion: What is the minimum sensationalism score that a typical summarization model achieves?\nAnswer: 42.6%\n\nQuestion: What is the minimum sensationalism score that a typical summarization model achieves?\nAnswer: 42.6%\n\nQuestion: What is the minimum sensationalism score that a typical summarization model achieves?\nAnswer: 42.6%", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": "The learning models used on the dataset are traditional machine learning classifiers and neural network based models.\n\nQuestion: What is the most accurate learning model on the dataset?\nAnswer: The most accurate learning model on the dataset is the LR model followed by ensemble models such as GBT and RF.\n\nQuestion: What is the highest F1 score for \"spam\" tweets?\nAnswer: The highest F1 score for \"spam\" tweets is from the RNN-LTC model (0.551).\n\nQuestion: What is the highest F1 score for \"hateful\"", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": "The language model architectures used are a bi-directional language model to augment the sequence to sequence encoder and a uni-directional model to augment the decoder.\n\nQuestion: What is the name of the dataset used for abstractive summarization?\nAnswer: The dataset used for abstractive summarization is .\n\nQuestion: What is the name of the vocabulary used for the English side of the bitext?\nAnswer: The vocabulary used for the English side of the bitext is .\n\nQuestion: What is the name of the vocabulary used for", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": "The weights are dynamically adjusted in proportion to $(1-p)$, where $p$ is the probability of a training example.\n\nQuestion: What is the difference between the proposed dice loss and the standard cross-entropy loss?\nAnswer: The proposed dice loss is a hard version of the F1 score, while the standard cross-entropy loss is an accuracy-oriented objective.\n\nQuestion: What is the effect of hyperparameters in Tversky index?\nAnswer: The hyperparameters $\\alpha $ and $\\beta $ in TI control the tradeoff between precision and recall.\n\nQuestion: What", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": "The results show that the knowledge graph is critical, and that the knowledge graph representation isn't sufficient in that the knowledge graph representation without enhanced exploration methods cannot surpass the bottleneck.\n\nQuestion: What is the name of the game that is used as an example in the article?\nAnswer: Zork1\n\nQuestion: What is the name of the algorithm that is used to explore combinatorially sized action spaces?\nAnswer: Go-Explore\n\nQuestion: What is the name of the algorithm that is used to explore combinatorially sized action spaces?\nAnswer: Go", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": "An individual model consists of individual Bayesian models for each language and crosslingual latent variables to incorporate soft role agreement between aligned constituents.\n\nQuestion: What is the first work to incorporate the coupling of aligned arguments directly in a Bayesian SRL model?\nAnswer: The first work to incorporate the coupling of aligned arguments directly in a Bayesian SRL model is the multilingual model proposed in this paper.\n\nQuestion: What is the main result of the multilingual model?\nAnswer: The main result of the multilingual model is that little information can be learned about semantic roles from", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": "The annotations of non-standard pronunciation interspersed in Mapudungun speech could be extremely useful in the study of historical language and orthography change as a language moves from predominantly oral to being written in a standardized orthography.\n\nQuestion: What is the main criterion for choosing alphabetic characters in the Mapudungun alphabet?\nAnswer: The main criterion for choosing alphabetic characters was to use the current Spanish keyboard that was available on all computers in Chilean offices and schools.\n\nQuestion: What is the name of the tool used to synchronize the", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": "A semicharacter architecture is a type of architecture that uses a single-layered BiLSTM over their characters to encode the input words.\n\nQuestion: What is the main goal of the paper?\nAnswer: The main goal of the paper is to study the vulnerability of character-level inputs in modern NLP pipelines and to recommend word recognition as a safeguard against this.\n\nQuestion: What is the main contribution of the paper?\nAnswer: The main contribution of the paper is to propose a task-agnostic defense, attaching a word recognition model that predicts each word in a sentence", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": "16 languages\n\nQuestion: what is the main conclusion of the paper?\nAnswer: feature-based tagging models adequately enriched with external morphosyntactic lexicons perform, on average, as well as bi-LSTMs enriched with word embeddings.\n\nQuestion: what is the main goal of the paper?\nAnswer: to compare the respective impact of external lexicons and word vector representations on the accuracy of PoS models.\n\nQuestion: what is the main advantage of word vectors?\nAnswer: they are built in an unsupervised way, only requiring", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": "NCEL outperforms the state-of-the-art collective methods across five different datasets.\n\nQuestion: What is the main limitation of the global approach in entity linking?\nAnswer: The main limitation is threefold: (1) the lack of sufficient disambiguation clues for the mention from its surrounding words, (2) the inadequate training data, and (3) the inability to fully utilize the power of NN models for collective EL.\n\nQuestion: What is the main advantage of the global approach in entity linking?\nAnswer: The global approach has achieved significant improvements", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": "Yes\n\nQuestion: What is the best-performing model on ASR transcripts?\nAnswer: G-STT's transcripts (343)\n\nQuestion: What is the best-performing model on human written transcripts with human (defined) segmentation?\nAnswer: 725 (human written)\n\nQuestion: What is the best-performing model on human written transcripts with auto segmentation?\nAnswer: 95 (human written)\n\nQuestion: What is the best-performing model on ASR transcripts?\nAnswer", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "The baseline used was the performance of the error detection system by Rei2016, trained using the same FCE dataset.\n\nQuestion: What is the main evaluation measure used?\nAnswer: INLINEFORM0 is the main evaluation measure used.\n\nQuestion: What is the main evaluation measure used?\nAnswer: INLINEFORM0 is the main evaluation measure used.\n\nQuestion: What is the main evaluation measure used?\nAnswer: INLINEFORM0 is the main evaluation measure used.\n\nQuestion: What is the main evaluation measure used?\nAnswer: INLINEFORM0 is the main", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": "The authors would like to thank visualDx, a healthcare informatics company that provides web-based clinical decision support system. The authors would like to thank Roy Robinson, the Vice President of Technology and Medical Informatics at visualDx, for providing the synthesized user queries, as well as preliminary feedback on the performance of our framework.\n\nQuestion: what is the name of the fine-tuned ELMo model used in the project?\nAnswer: ELMo embeddings fine-tuned on PubMed articles\n\nQuestion: what is the name of the fine-t", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": "PPDB\n\nQuestion: What is the objective function of the model?\n\nAnswer: Modeling from structured resources\n\nQuestion: What is the motivation behind the model?\n\nAnswer: To capture rich tweet semantics\n\nQuestion: What is the motivation behind the model?\n\nAnswer: To capture rich tweet semantics\n\nQuestion: What is the motivation behind the model?\n\nAnswer: To capture rich tweet semantics\n\nQuestion: What is the motivation behind the model?\n\nAnswer: To capture rich tweet semantics\n\nQuestion: What is the motivation behind", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": "TF-IDF features\n\nQuestion: What is the primary objective of the study?\nAnswer: To analyze the efficacy of existing machine learning approaches for the automated classification of pathology reports into different diagnosis categories.\n\nQuestion: What is the distribution of reports across different primary diagnoses and primary sites?\nAnswer: The distribution of reports across different primary diagnoses and primary sites is reported in tab:report-distribution.\n\nQuestion: What is the distribution of reports across different primary diagnoses and primary sites?\nAnswer: The distribution of reports across different primary diagnoses and primary sites is reported in tab", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": "The dataset is annotated with 9,473 annotations for 9,300 tweets. Each tweet is annotated as no evidence of depression or evidence of depression. If a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms, for example, depressed mood.\n\nQuestion: What is the purpose of the feature ablation study?\nAnswer: The purpose of the feature ablation study is to assess the contribution of each feature group by quantifying the change in predictive power when comparing the performance of a class", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": "The eight publicly available NER tasks used in BIBREF2.\n\nQuestion: What is the name of the dataset used for the Covid-19 QA experiment?\nAnswer: Deepset-AI Covid-QA\n\nQuestion: What is the name of the dataset used for the Biomedical NER experiment?\nAnswer: PubMed+PMC\n\nQuestion: What is the name of the dataset used for the Covid-19 QA experiment?\nAnswer: Deepset-AI Covid-QA\n\nQuestion: What is the name of the dataset used", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": "The training data was translated from English to Spanish using the machine translation platform Apertium.\n\nQuestion: What was the name of the machine translation platform used for the translation?\nAnswer: Apertium\n\nQuestion: What was the name of the corpus used for semi-supervised learning?\nAnswer: DISC\n\nQuestion: What was the name of the package used for creating word embeddings?\nAnswer: AffectiveTweets\n\nQuestion: What was the name of the library used for creating word embeddings?\nAnswer: gensim\n\nQuestion: What was the name of", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": "\"The model used for the system was a multinomial Naive Bayes classifier.\"\n\nQuestion: What was the name of the dataset used in this study?\nAnswer: \"The dataset used in this study was the industry-annotated dataset by identifying blogger profiles located in the U.S. on the profile finder on http://www.blogger.com, and scraping only those users that had the industry profile element completed.\"\n\nQuestion: What was the name of the feature selection method used in this study?\nAnswer: \"The feature selection method used in this study was the Ag", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "A very simple logistic regression classifier with default parameters, where a word could belong to one of the 18 propaganda techniques, to none of them, or to an auxiliary (token-derived) class.\n\nQuestion: What was the main focus of the shared task?\nAnswer: To create a corpus of news articles annotated with an inventory of 18 propagandist techniques at the fragment level.\n\nQuestion: What was the main focus of the shared task?\nAnswer: To create a corpus of news articles annotated with an inventory of 18 propagandist", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": "The first block of Table TABREF11 .\n\nQuestion: What is the INLINEFORM0 tagging scheme?\nAnswer: The INLINEFORM0 tag means the current word appears before the pun in the given context.\n\nQuestion: What is the INLINEFORM1 tagging scheme?\nAnswer: The INLINEFORM1 tag means the current word is a pun.\n\nQuestion: What is the INLINEFORM2 tagging scheme?\nAnswer: The INLINEFORM2 tag means the current word appears after the pun.\n\nQuestion: What is the INLINEFORM3 tagging scheme?", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": "The political bias of different sources is included in the model by assigning a political bias label to different US outlets following the procedure described in BIBREF2.\n\nQuestion: What is the main goal of the classification experiment?\nAnswer: The main goal of the classification experiment is to investigate whether a multi-layer representation performs better than one layer.\n\nQuestion: What is the main conclusion of the temporal analysis?\nAnswer: The main conclusion of the temporal analysis is that our methodology can accurate classify mainstream and disinformation news after a few hours of propagation on the platform.\n\nQuestion: What is the", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": "The ancient Chinese dataset comes from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era.\n\nQuestion: What is the name of the method used to align the bilingual clauses?\nAnswer: The proposed method combines both lexical-based information and statistical-based information, which achieves 94.2 F1-score on Test set.\n\nQuestion: What is the name of the method used to align the bilingual clauses?\nAnswer: The proposed method combines both lexical", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "English\n\nQuestion: What is the main goal of the dataset?\nAnswer: To create a dataset with annotation of type and target of offensive language.\n\nQuestion: What is the main goal of the dataset?\nAnswer: To create a dataset with annotation of type and target of offensive language.\n\nQuestion: What is the main goal of the dataset?\nAnswer: To create a dataset with annotation of type and target of offensive language.\n\nQuestion: What is the main goal of the dataset?\nAnswer: To create a dataset with annotation of type and target of offensive language.\n\nQuestion", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": "The Chinese datasets used in this work are the Penn Treebank (PTB) and the Penn Chinese Treebank (PCTB).\n\nQuestion: what is the main motivation for the compound PCFG?\n\nAnswer: The main motivation for the compound PCFG is to model richer dependencies through vertical/horizontal Markovization and lexicalization.\n\nQuestion: what is the difference between the compound PCFG and the traditional PCFG?\n\nAnswer: The compound PCFG assumes a prior on local, sentence-level rule probabilities, while", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": "The UTCNN model has three convolutional layers.\n\nQuestion: What is the purpose of the maximum pooling layer in the document composition process?\nAnswer: The maximum pooling layer is used to select the most important feature for comments.\n\nQuestion: What is the difference between the UTCNN shared user embedding setting and the full UTCNN model?\nAnswer: The full UTCNN model separates authors/likers and commenters embeddings, while the shared user embedding setting forces all embeddings from the same user to be identical.\n\nQuestion: What is the main advantage of the UTCNN model over other", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": "The dataset used in this paper is the same as the one used in BIBREF7 .\n\nQuestion: what is the main hypothesis of this paper?\nAnswer: The main hypothesis of this paper is that by using vector space embeddings instead of bag-of-words representations, the ecological information which is implicitly captured by Flickr tags can be utilized in a more effective way.\n\nQuestion: what is the main motivation for using vector space embeddings in this paper?\nAnswer: The main motivation for using vector space embeddings in this paper is that they allow us to integrate", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": "NUBes-PHI and MEDDOCAN\n\nQuestion: What is the main objective of the paper?\nAnswer: To evaluate BERT's multilingual model and compare it to other established machine-learning algorithms in a specific task: sensitive data detection and classification in Spanish clinical free text.\n\nQuestion: What is the main conclusion of the paper?\nAnswer: The BERT-based model outperforms the other systems without requiring any adaptation or domain-specific feature engineering, just by being trained on the provided labelled data.\n\nQuestion: What is the main difference between the BERT-", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": "Unigrams and Pragmatic features BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 (b) Stylistic patterns BIBREF4 and patterns related to situational disparity BIBREF5 and (c) Hastag interpretations BIBREF6 , BIBREF7 .\n\nQuestion: What is the main hypothesis of the current work?\nAnswer: The hypothesis is that distinctive eye-movement patterns may be observed in the case of successful processing of sarcasm in text in contrast to literal texts.\n\nQuestion: What is the main contribution", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are: \n1. The ability to learn new knowledge in the conversation process.\n2. The ability to perform inference using existing knowledge.\n3. The ability to acquire new knowledge from the user in the conversation process.\n4. The ability to use existing knowledge to perform inference.\n5. The ability to use existing knowledge to acquire new knowledge from the user in the conversation process.\n\nQuestion: What is the main weakness of existing chat systems? \nAnswer: The main weakness of existing chat", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "Yes\n\nQuestion: What is the name of the dataset that is created by the answer retrieval task?\nAnswer: Answer retrieval dataset\n\nQuestion: What is the name of the dataset that is created by the answer retrieval task?\nAnswer: Answer retrieval dataset\n\nQuestion: What is the name of the dataset that is created by the answer retrieval task?\nAnswer: Answer retrieval dataset\n\nQuestion: What is the name of the dataset that is created by the answer retrieval task?\nAnswer: Answer retrieval dataset\n\nQuestion: What is the name of the dataset that is created by", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": "Galatasaray and Fenerbahçe\n\nQuestion: What is the domain of the tweets?\nAnswer: Sports clubs\n\nQuestion: What is the name of the data set?\nAnswer: Turkish_Stance_Detection_Tweet_Dataset.csv\n\nQuestion: What is the name of the file format?\nAnswer: Comma Separated Values (CSV)\n\nQuestion: What is the name of the data set?\nAnswer: Turkish_Stance_Detection_Tweet_Dataset.csv\n\nQuestion: What is the name", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": "The experiments are conducted to evaluate the effectiveness of the model in generating ironic sentences.\n\nQuestion: What is the main reason for the issue of no change in the output sentence?\nAnswer: The main reason is that rewards for content preservation are too prominent and rewards for style accuracy cannot work well.\n\nQuestion: What is the conclusion of the automatic evaluation results of the models in the transformation from non-ironic sentences to ironic sentences?\nAnswer: The conclusion is that our model obtains the best result in sentiment delta.\n\nQuestion: What is the conclusion of the human evaluation results of the", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": "Gaussian-masked directional multi-head attention is a variant of self-attention that uses a Gaussian weight matrix to capture the localness relationship between characters. It ensures that the relationship between adjacent characters is stronger than the relationship between characters with long distances.\n\nQuestion: What is the main contribution of this paper?\nAnswer: The main contribution of this paper is to propose a CWS model with only attention structure. The encoder and decoder are both based on attention structure. With a powerful enough encoder, we for the first time show that unigram (character) featues can help yield strong performance", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "Facebook\n\nQuestion: What is the name of the dataset released by the authors?\nAnswer: Causality Facebook dataset\n\nQuestion: What is the name of the model used for causality prediction?\nAnswer: Linear SVM\n\nQuestion: What is the name of the model used for causal explanation identification?\nAnswer: LSTM\n\nQuestion: What is the name of the model used for causality prediction?\nAnswer: Random Forest\n\nQuestion: What is the name of the model used for causal explanation identification?\nAnswer: RBF SVM\n\nQuestion: What is the name of", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": "The baseline features are the features extracted from the baseline CNN architecture.\n\nQuestion: What is the purpose of the pre-trained models in the proposed framework?\nAnswer: The pre-trained models are used to extract features from the sarcasm datasets.\n\nQuestion: What is the purpose of the baseline features in the proposed framework?\nAnswer: The baseline features are used to extract features from the sarcasm datasets.\n\nQuestion: What is the purpose of the pre-trained models in the proposed framework?\nAnswer: The pre-trained models are used to extract features from", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": "The number of clusters and the number of clusters.\n\nQuestion: What is the main goal of the NER task?\nAnswer: The task of NER concerns the classification of textual segments in a predefined set of categories, like persons, organization and locations.\n\nQuestion: What is the evaluation measure used for the NER task?\nAnswer: The evaluation measure for the task is the F measure.\n\nQuestion: What is the evaluation measure used for the sentiment classification task?\nAnswer: The evaluation measure used for the sentiment classification task is the macro-averaged Mean Absolute Error (MAE).\n", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": "The scores of their system are 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": "The corpus consists of 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average. The corpus comprises 8,275 sentences and 167,739 words in total.\n\nQuestion: What is the average number of tokens per entity?\nAnswer: The number of tokens per entity ranges from one token for all types to 5 tokens for cases (average length 3.1), nine tokens for conditions (average length 2.0), 16 tokens for factors (", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": "Yes, it is possible to convert cloze-style questions to naturally-looking questions.\n\nQuestion: What is the main purpose of the cloze-style questions?\nAnswer: The main purpose of the cloze-style questions is to improve the performance of the QA models in a low-resource setting.\n\nQuestion: What is the main difference between the cloze-style questions and the naturally-looking questions?\nAnswer: The main difference between the cloze-style questions and the naturally-looking questions is that the cloze-style questions are constructed using a heuristic, while the naturally-looking", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": "text categorization, sentiment classification, and web-page classification.\n\nQuestion: What is the main contribution of this paper?\nAnswer: The main contributions of this work are as follows: The rest of the paper is structured as follows: In Section 2, we briefly describe the generalized expectation criteria and present the proposed regularization terms. In Section 3, we conduct extensive experiments to justify the proposed methods. We survey related work in Section 4, and summarize our work in Section 5.\n\nQuestion: What is the main problem addressed in this paper?\nAnswer: The main problem addressed in this paper", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": "\"We make use of a number of existing question classification methods, including those developed for TREC questions, and those developed for biomedical and consumer health questions.\"\n\nQuestion: What is the main contribution of this work?\n\nAnswer: \"We make use of a single learned model to achieve state-of-the-art performance across multiple datasets, including TREC, GARD, and MLBioMedLAT.\"\n\nQuestion: What is the main challenge in question classification?\n\nAnswer: \"The lack of targeted solving – using the same methods for inferring answers to spatial questions about planet", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": "The training sets are larger by a factor of 1000.\n\nQuestion: What is the name of the dataset used for the analogy task?\nAnswer: The analogy dataset is based on the English dataset by BIBREF14 .\n\nQuestion: What is the name of the dataset used for the NER task?\nAnswer: The NER dataset is an adapted version of the NER datasets for all processed languages.\n\nQuestion: What is the name of the tool used for deduplication?\nAnswer: The tool is called Onion (ONe Instance ONly) tool for", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": "64% of the total dataset is used for training, 16% for development and 20% for testing.\n\nQuestion: What is the name of the dataset released by the authors?\nAnswer: OurNepali dataset\n\nQuestion: What is the name of the dataset received from ILPRL lab?\nAnswer: ILPRL dataset\n\nQuestion: What is the name of the dataset used in the experiments?\nAnswer: OurNepali dataset\n\nQuestion: What is the name of the dataset used in the experiments?\nAnswer: ILPRL dataset\n\nQuestion: What", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": "MLP, Eusboost, MWMOTE\n\nQuestion: What is the main intention of the proposed s2sL approach?\nAnswer: To demonstrate the effectiveness of the approach, in particular for low data scenarios.\n\nQuestion: What is the main contribution of the proposed s2sL approach?\nAnswer: The proposed approach is also applicable to low resource data suffering with data imbalance.\n\nQuestion: What is the main contribution of the proposed s2sL approach?\nAnswer: The proposed approach is also applicable to low resource data suffering with data imbalance.\n\nQuestion:", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": "Yes\n\nQuestion: What is the main contribution of their proposed model?\nAnswer: The proposed model learns NER from both text and images.\n\nQuestion: What is the main result of their proposed model?\nAnswer: The main result is that the model outperforms the state-of-the-art baselines when text was the only modality available.\n\nQuestion: What is the main result of their proposed model?\nAnswer: The main result is that the model outperforms the state-of-the-art baselines when text was the only modality available.\n\nQuestion: What", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": "unanswerable\n\nQuestion: What is the name of the paper?\nAnswer: Introduction\n\nQuestion: What is the name of the paper?\nAnswer: Introduction\n\nQuestion: What is the name of the paper?\nAnswer: Introduction\n\nQuestion: What is the name of the paper?\nAnswer: Introduction\n\nQuestion: What is the name of the paper?\nAnswer: Introduction\n\nQuestion: What is the name of the paper?\nAnswer: Introduction\n\nQuestion: What is the name of the paper?\nAnswer: Introduction\n\nQuestion: What is the name of the paper?\n", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": "0.6103\n\nQuestion: What was their highest F-measure score?\n\nAnswer: 0.2862\n\nQuestion: What was their highest recall score?\n\nAnswer: 0.7033\n\nQuestion: What was their highest precision score?\n\nAnswer: 0.1119\n\nQuestion: What was their highest F-measure score?\n\nAnswer: 0.2862\n\nQuestion: What was their highest recall score?\n\nAnswer: 0.7033\n\nQuestion: What was their", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": "The Wall Street Journal (WSJ) portion of the Penn Treebank.\n\nQuestion: What is the dimensionality of the skip-gram embeddings?\nAnswer: 100.\n\nQuestion: What is the context window size for the skip-gram embeddings?\nAnswer: 1.\n\nQuestion: What is the number of tag clusters in the POS tagging experiment?\nAnswer: 45.\n\nQuestion: What is the number of coupling layers in the neural projector?\nAnswer: 4, 8, 16.\n\nQuestion:", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": "The authors evidence this claim by conducting a survey among engineers and identifying a spectrum of three typical personas. The survey revealed that about 87.5% NLP related jobs belong to a few common tasks, including sentence classification, text matching, sequence labeling, MRC, etc. It further suggested that more than 90% of the networks were composed of several common components, such as embedding, CNN/RNN, Transformer and so on.\n\nQuestion: What is the main idea behind the NeuronBlocks toolkit?\nAnswer: The main idea behind the NeuronBlocks", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": "SimpleQuestions and WebQSP\n\nQuestion: What is the name of the dataset they use for the KBQA end task?\nAnswer: SimpleQuestions and WebQSP\n\nQuestion: What is the name of the dataset they use for the KBQA end task?\nAnswer: SimpleQuestions and WebQSP\n\nQuestion: What is the name of the dataset they use for the KBQA end task?\nAnswer: SimpleQuestions and WebQSP\n\nQuestion: What is the name of the dataset they use for the KBQA end task?\nAnswer:", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
